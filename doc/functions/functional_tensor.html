<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>tensap.functions.functional_tensor API documentation</title>
<meta name="description" content="Module functional_tensor â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tensap.functions.functional_tensor</code></h1>
</header>
<section id="section-intro">
<p>Module functional_tensor.</p>
<p>Copyright (c) 2020, Anthony Nouy, Erwan Grelier
This file is part of tensap (tensor approximation package).</p>
<p>tensap is free software: you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.</p>
<p>tensap is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
See the
GNU Lesser General Public License for more details.</p>
<p>You should have received a copy of the GNU Lesser General Public License
along with tensap.
If not, see <a href="https://www.gnu.org/licenses/">https://www.gnu.org/licenses/</a>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
Module functional_tensor.

Copyright (c) 2020, Anthony Nouy, Erwan Grelier
This file is part of tensap (tensor approximation package).

tensap is free software: you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

tensap is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Lesser General Public License for more details.

You should have received a copy of the GNU Lesser General Public License
along with tensap.  If not, see &lt;https://www.gnu.org/licenses/&gt;.

&#39;&#39;&#39;

from copy import deepcopy
import numpy as np
import tensap


class FunctionalTensor(tensap.Function):
    &#39;&#39;&#39;
    Class FunctionalTensor.

    Attributes
    ----------
    tensor : Tensor or tensap.FunctionalTensor, optional
        The tensor of the FunctionalTensor. The default is None.
    bases : list or tensap.FunctionalBases, optional
        The functional bases of the FunctionalTensor. The default is None.
    fdims : list or numpy.ndarray, optional
        The dimensions corresponding to the bases. The default is None.

    &#39;&#39;&#39;

    def __init__(self, tensor=None, bases=None, fdims=None):
        &#39;&#39;&#39;
        Constructor for the FunctionalTensor.

        Parameters
        ----------
        tensor : Tensor or tensap.FunctionalTensor, optional
            The tensor of the FunctionalTensor. The default is None.
        bases : list or tensap.FunctionalBases, optional
            The functional bases of the FunctionalTensor. The default is None.
        fdims : list or numpy.ndarray, optional
            The dimensions corresponding to the bases. The default is None.

        Raises
        ------
        ValueError
            If the provided objects are not of the expected types.

        Returns
        -------
        None.

        &#39;&#39;&#39;
        tensap.Function.__init__(self)

        if isinstance(tensor, FunctionalTensor):
            tensor = tensor.tensor

        self.tensor = tensor

        if bases is not None and \
                not isinstance(bases, (tensap.FunctionalBases, list)):
            raise ValueError(&#39;Must provide a FunctionalBases object, or a &#39; +
                             &#39;cell of bases evaluations.&#39;)
        self.bases = bases
        if fdims is None:
            if tensor.order != len(bases):
                raise ValueError(&#39;Bases must contain as many bases as the &#39; +
                                 &#39;order of the tensor, with possible empty &#39; +
                                 &#39;elements.&#39;)
            fdims = range(tensor.order)
        else:
            if len(fdims) != len(bases):
                raise ValueError(&#39;The number of functional dimensions must &#39; +
                                 &#39;correspond to the number of bases in bases.&#39;)
        self.fdims = np.array(fdims)
        if isinstance(bases, list):
            self.evaluated_bases = True
        else:
            self.evaluated_bases = False
            self.measure = self.bases.measure
            self.dim = np.sum(self.bases.ndim())

    def __plus__(self, y):
        if isinstance(y, FunctionalTensor) and self.bases == y.bases:
            tensor = self.tensor + y.tensor
        else:
            raise NotImplementedError(&#39;Method not implemented.&#39;)
        return FunctionalTensor(tensor, self.bases, self.fdims)

    def is_random(self):
        &#39;&#39;&#39;
        Determine if self is random.

        Returns
        -------
        bool
            Boolean indicating if if self is random.

        &#39;&#39;&#39;
        return isinstance(self.bases.measure, tensap.ProbabilityMeasure)

    def mean(self, *measure):
        &#39;&#39;&#39;
        Compute the expectation of the random variable self(measure) if
        measure is provided, and of self(self.bases.measure) otherwise.

        Parameters
        ----------
        *measure : tensap.RandomVector, optional
            The measure used for the computation of the mean. If not provided,
            indicates to use self.bases.measure.

        Returns
        -------
        float or Tensor
            The mean of the function.

        &#39;&#39;&#39;
        bases_eval = self.bases.mean(None, *measure)
        return self.tensor.tensor_vector_product(bases_eval,
                                                 self.fdims).tolist()

    def expectation(self, *measure):
        &#39;&#39;&#39;
        Compute the expectation of the random variable self(measure) if
        measure is provided, and of self(self.bases.measure) otherwise.

        Parameters
        ----------
        *measure : tensap.RandomVector, optional
            The measure used for the computation of the expectation. If not
            provided, indicates to use self.bases.measure.

        Returns
        -------
        float or Tensor
            The expectation of the function.

        &#39;&#39;&#39;
        return self.mean(*measure)

    def variance(self, *measure):
        &#39;&#39;&#39;
        Compute the variance of the random variable self(measure) if
        measure is provided, and of self(self.bases.measure) otherwise.

        Parameters
        ----------
        *measure : tensap.RandomVector, optional
            The measure used for the computation of the variance. If not
            provided, indicates to use self.bases.measure.

        Returns
        -------
        var : float or Tensor
            The variance of the function.

        &#39;&#39;&#39;
        mean = self.expectation(*measure)
        if np.isscalar(mean):
            var = self.dot_product_expectation(self, None, *measure) - mean**2
        else:
            raise NotImplementedError(&#39;Method not implemented.&#39;)
        return var

    def std(self, *measure):
        &#39;&#39;&#39;&#39;
        Compute the standard deviation of the random variable self(measure) if
        measure is provided, and of self(self.bases.measure) otherwise.

        Parameters
        ----------
        *measure : tensap.RandomVector, optional
            The measure used for the computation of the standard deviation. If
            not provided, indicates to use self.bases.measure.

        Returns
        -------
        v : float or Tensor
            The standard deviation of the function.

        &#39;&#39;&#39;
        return np.sqrt(self.variance(*measure))

    def dot_product_expectation(self, f_2, fdims=None, *measure):
        &#39;&#39;&#39;
        Computes the expectation of self(X)f_2(X) where X is the random vector
        associated with self.bases if measure is not provided, and measure
        otherwise.

        For tensor-valued functions of X (len(X)&lt;self.order), fdims specifies
        the dimensions of self and f_2 corresponding to theRandomVector X.

        Parameters
        ----------
        f_2 : tensap.FunctionalTensor
            The second functional tensor of the product.
        fdims : list of numpy.ndarray, optional
            Specifies the dimensions of self and f_2 corresponding to
            theRandomVector X. The default is None, indicating all the
            dimensions
        *measure : tensap.RandomVector, optional
            The measure used for the computation of the product. If not
            provided, indicates to use self.bases.measure.

        Raises
        ------
        ValueError
            If the two tensors do not have the same order and fdims is not
            specified.
        NotImplementedError
            If the bases of self and f_2 are not equal.

        Returns
        -------
        float or Tensor
            The expectation of self(X)f_2(X).

        &#39;&#39;&#39;
        if fdims is None:
            if self.tensor.order == f_2.tensor.order:
                fdims = range(self.tensor.order)
            else:
                raise ValueError(&#39;Tensors u and v do not have the same &#39; +
                                 &#39;order, must specify fdims.&#39;)
        if self.bases == f_2.bases:
            gram_matrix = self.bases.gram_matrix(fdims, *measure)
        else:
            raise NotImplementedError(&#39;Method not implemented.&#39;)
        tensor = self.tensor.tensor_matrix_product(gram_matrix, fdims)
        return tensor.dot(f_2.tensor)

    def norm(self, *measure):
        &#39;&#39;&#39;
        Return the L^2 norm of self(X), with X = measure if provided, and
        X = self.bases.measure otherwise.

        If self.evaluatedBases is true, without additional information, return
        the canonical norm of self.tensor.

        Parameters
        ----------
        *measure : tensap.RandomVector, optional
            The measure used for the computation of the norm. If not provided,
            indicates to use self.bases.measure.

        Returns
        -------
        float or Tensor
            The L^2 norm of self(X).

        &#39;&#39;&#39;
        if not self.evaluated_bases:
            gram_matrix = self.bases.gram_matrix(range(self.tensor.order),
                                                 *measure)
        else:
            gram_matrix = [np.eye(x.shape[1]) for x in self.bases]
        tensor = self.tensor.tensor_matrix_product(gram_matrix,
                                                   range(self.tensor.order))
        return np.sqrt(tensor.dot(self.tensor))

    def conditional_expectation(self, dims, *args):
        &#39;&#39;&#39;
        Compute the conditional expectation of self with respect to the random
        variables dims (a subset of [1, ..., d]).

        The expectation with respect to other variables (in the complementary
        set of dims) is taken with respect to the probability measure given by
        a tensap.RandomVector if provided as an additional argument, or with
        respect to the probability measure associated with the corresponding
        bases of self.

        Parameters
        ----------
        dims : list or numpy.ndarray
            The dimensions of the random variables with respect to which the
            conditional expectation is to be computed.
        *args : tuple
            Tuple containing a tensap.randomVector giving the probability
            measure of the variables other than the ones in dims. If not
            provided, the measure is infered from self.bases.measure.

        Returns
        -------
        tensap.FunctionalTensor
            The conditional expectation of self with respect to the random
            variables dims, as a len(dims)-order tensor.

        &#39;&#39;&#39;
        dims = np.atleast_1d(dims)
        if np.all([isinstance(x, bool) for x in dims]):
            dims = np.nonzero(dims)[0]

        d = self.tensor.order
        if np.size(dims) == 0:
            return self.expectation(*args)

        dims = np.sort(dims)
        assert np.size(self.fdims) == d and \
            np.array_equal(self.fdims, range(d)), \
            (&#39;Method not implemented for self.fdims different from &#39; +
             &#39;range(d).&#39;)

        dims_C = np.setdiff1d(range(len(self.bases)), dims)
        if dims_C.size == 0:
            return deepcopy(self)

        H = self.bases.mean(dims_C, *args)
        t = self.tensor.tensor_vector_product(H, dims_C)

        bases = self.bases.keep_bases(dims)
        # TODO Take into account the mapping when implemented

        out = FunctionalTensor(t, bases)
        if self.measure is not None:
            out.measure = self.measure.marginal(dims)

        return out

    def variance_conditional_expectation(self, alpha):
        &#39;&#39;&#39;
        Compute the variance of the conditional expectation of self in
        dimensions in alpha.

        Parameters
        ----------
        alpha : list or numpy.ndarray
            Array containing the dimensions (either explicitely or using
            booleans) in which the variance of the conditional expectation is
            computed.

        Returns
        -------
        v : numpy.ndarray
            The variance of the conditional expectation of self in
            dimensions in alpha.

        &#39;&#39;&#39;
        alpha = np.atleast_2d(alpha)
        m = self.expectation()
        v = np.zeros(alpha.shape[0])
        for i in range(alpha.shape[0]):
            u = alpha[i, :]
            if np.all([isinstance(x, bool) for x in u]):
                u = np.nonzero(u)[0]
            if u.size == 0:
                v[i] = 0
            else:
                mu = self.conditional_expectation(u)
                v[i] = mu.dot_product_expectation(mu) - m**2
        return v

    def eval(self, x, *dims):
        &#39;&#39;&#39;
        Evaluate self at the points x.

        If dims is provided, compute the partial evaluations of self at points
        x in dimensions in dims.

        Parameters
        ----------
        x : list or numpy.ndarray or None
            The points at which the function is to be evaluated. If x is None
            and self.evaluated_bases, evaluates the function using the
            evaluations of the bases.
        *dims : list or numpy.ndarray, optional
            The dimensions of the partial evaluation. If not provided,
            evaluate the function in all dimensions.

        Returns
        -------
        numpu.ndarray or Tensor
            The evaluations of self at the points x.

        &#39;&#39;&#39;
        if self.evaluated_bases:
            bases_eval = self.bases
        else:
            bases_eval = self.bases.eval(x, *dims)
        return self.eval_with_bases_evals(bases_eval, *dims)

    def __mul__(self, f_2):
        if isinstance(f_2, (FunctionalTensor, tensap.FunctionalTensor)):
            b = self.bases.kron(f_2.bases)
            t = self.tensor.kron(f_2.tensor)
            out = FunctionalTensor(t, b)
            if isinstance(out.tensor, tensap.TreeBasedTensor) and \
                    out.tensor.ranks[out.tensor.tree.root-1] &gt; 1:
                if self.tensor.ranks[self.tensor.tree.root-1] != \
                        f_2.tensor.ranks[f_2.tensor.tree.root-1]:
                    raise ValueError(&#39;Wrong tensor shapes.&#39;)
                else:
                    root = out.tensor.tree.root
                    c = out.tensor.tensors[root-1]
                    n = self.tensor.ranks[root-1]
                    s = [&#39;:&#39;]*c.order
                    s[-1] = np.arange(n**2, step=n)
                    c = c.sub_tensor(*s)
                    out.tensor.tensors[root-1] = c
                    out.tensor.ranks[root-1] = n
        else:
            out = deepcopy(self)
            out.tensor = out.tensor * f_2
        return out

    def parameter_gradient_eval(self, alpha, x=None, *args):
        &#39;&#39;&#39;
        Compute the gradient of the function with respect to its alpha-th
        parameter, evaluated at some points.

        Parameters
        ----------
        alpha : int
            The number of the parameter with respect to which compute the
            gradient of self.
        x : list or numpy.ndarray, optional
            The points at which the gradient is to be evaluated. The default is
            None, indicating to use self.bases if self.evaluated_bases is True.

        Raises
        ------
        ValueError
            If no input points are provided.

        Returns
        -------
        grad : Tensor
            The gradient of the function with respect to its alpha-th
            parameter, evaluated at some points.

        &#39;&#39;&#39;
        if self.evaluated_bases:
            bases_eval = self.bases
        elif x is not None:
            bases_eval = self.bases.eval(x)
        else:
            raise ValueError(&#39;Must provide the evaluation points or the &#39; +
                             &#39;bases evaluations.&#39;)

        dims = np.arange(self.tensor.order)
        if isinstance(self.tensor, tensap.TreeBasedTensor):
            # Compute fH, the TimesMatrixEvalDiag of f with bases_eval in all
            # the dimensions except the ones associated with alpha (if alpha
            # is a leaf node) or with the inactive children of alpha (if
            # alpha is an internal node). The tensor fH is used to compute
            # the gradient of f with respect to f.tensor.tensors[alpha-1].
            tree = self.tensor.tree
            if tree.is_leaf[alpha-1]:
                dims = dims[tree.dim2ind != alpha]
            else:
                children = tree.children(alpha)
                ind = tensap.fast_intersect(
                    tree.dim2ind,
                    children[np.logical_not(
                        self.tensor.is_active_node[children-1])])
                dims = dims[np.logical_not(np.isin(tree.dim2ind, ind))]

            if np.all(self.tensor.is_active_node):
                fH = self.tensor.tensor_matrix_product([bases_eval[x] for x
                                                        in dims], dims)
            else:
                remaining_dims = np.arange(self.tensor.order)
                tensors = np.array(self.tensor.tensors)
                dim2ind = np.array(tree.dim2ind)

                for leaf in tensap.fast_intersect(tree.dim2ind[dims],
                                                  self.tensor.active_nodes):
                    dims = tensap.fast_setdiff(
                        dims, np.nonzero(tree.dim2ind == leaf)[0][0])
                    tensors[leaf-1] = self.tensor.tensors[leaf-1].\
                        tensor_matrix_product(bases_eval[
                            np.nonzero(tree.dim2ind == leaf)[0][0]], 0)

                for pa in np.unique(tree.parent(tensap.fast_setdiff(
                        tree.dim2ind[dims], self.tensor.active_nodes))):
                    ind = tensap.fast_intersect(tree.dim2ind[dims],
                                                tree.children(pa))
                    ind = tensap.fast_setdiff(ind, self.tensor.active_nodes)
                    dims_loc = np.array([np.nonzero(x == tree.dim2ind)[0][0]
                                         for x in ind])
                    if len(ind) &gt; 1:
                        tensors[pa-1] = self.tensor.tensors[pa-1].\
                            tensor_matrix_product_eval_diag([bases_eval[x] for
                                                             x in dims_loc],
                                                            tree.child_number(
                                                                ind)-1)
                        remaining_dims = tensap.fast_setdiff(remaining_dims,
                                                             dims_loc[1:])
                        if np.all(np.logical_not(self.tensor.is_active_node[
                                tree.children(pa)-1])):
                            dim2ind[dims_loc[0]] = tree.parent(
                                tree.dim2ind[dims_loc[0]])
                        else:
                            dims = tensap.fast_setdiff(dims, dims_loc[0])
                        dim2ind[dims_loc[1:]] = 0
                        perm = np.concatenate((
                            [tree.child_number(ind[0])-1],
                            tensap.fast_setdiff(np.arange(tensors[pa-1].order),
                                                tree.child_number(ind[0])-1)))
                        tensors[pa-1] = tensors[pa-1].itranspose(perm)
                    elif len(ind) == 1:
                        dims = dims[dims != dims_loc]
                        tensors[pa-1] = self.tensor.tensors[pa-1].\
                            tensor_matrix_product([bases_eval[x] for
                                                   x in dims_loc],
                                                  tree.child_number(ind)-1)
                        dim2ind[dims_loc] = tree.dim2ind[dims_loc]

                keep_ind = tensap.fast_setdiff(np.arange(tree.nb_nodes),
                                               tree.dim2ind[dims]-1)
                adj_mat = tree.adjacency_matrix[np.ix_(keep_ind, keep_ind)]
                dim2ind = dim2ind[dim2ind != 0]

                ind = np.zeros(tree.nb_nodes)
                ind[tensap.fast_setdiff(np.arange(tree.nb_nodes),
                                        keep_ind)] = 1
                ind = np.cumsum(ind).astype(int)
                dim2ind -= ind[dim2ind-1]
                alpha = alpha - ind[alpha-1]

                tree = tensap.DimensionTree(dim2ind, adj_mat)
                fH = tensap.TreeBasedTensor(tensors[keep_ind], tree)
                fH = fH.remove_unique_children()
                bases_eval = [bases_eval[x] for x in remaining_dims]
        else:
            if alpha &lt;= self.tensor.order:
                dims = np.delete(dims, alpha-1)
            fH = self.tensor.tensor_matrix_product([bases_eval[x] for
                                                    x in dims], dims)

        grad = fH.parameter_gradient_eval_diag(alpha, bases_eval)
        if isinstance(self.tensor, tensap.TreeBasedTensor) and \
                not tree.is_leaf[alpha-1]:
            # If the order of the children has been modified in grad, compute
            # the inverse permutation.
            ch = tree.children(alpha)
            perm_1 = np.argsort(np.concatenate((
                np.atleast_1d(ch[fH.is_active_node[ch-1]]),
                np.atleast_1d(ch[np.logical_not(fH.is_active_node[ch-1])]))))

            if alpha == tree.root:
                perm_2 = []
            else:
                perm_2 = [fH.tensors[alpha-1].order]

            if alpha != tree.root and self.tensor.ranks[tree.root-1] &gt; 1:
                perm_3 = [grad.order-1]
            else:
                perm_3 = []

            grad = grad.transpose(np.concatenate(([0], perm_1+1,
                                                  perm_2, perm_3)).astype(int))

        return grad

    def parameter_gradient_eval_dmrg(self, alpha, x=None, dmrg_type=&#39;dmrg&#39;,
                                     *args):
        if self.evaluated_bases:
            bases_eval = self.bases
        elif x is not None:
            bases_eval = self.bases.eval(x)
        else:
            raise ValueError(&#39;Must provide the evaluation points or the &#39; +
                             &#39;bases evaluations.&#39;)

        dims = np.arange(self.tensor.order)
        if isinstance(self.tensor, tensap.TreeBasedTensor):
            # Compute fH, the TimesMatrixEvalDiag of f with bases_eval in all
            # the dimensions except the ones associated with alpha (if alpha
            # is a leaf node) or with the inactive children of alpha (if
            # alpha is an internal node). The tensor fH is used to compute
            # the gradient of f with respect to f.tensor.tensors[alpha-1].
            tree = self.tensor.tree
            if tree.is_leaf[alpha-1]:
                dims = dims[tree.dim2ind != alpha]
            else:
                children = tree.children(alpha)
                ind = tensap.fast_intersect(
                    tree.dim2ind,
                    children[np.logical_not(
                        self.tensor.is_active_node[children-1])])
                dims = dims[np.logical_not(np.isin(tree.dim2ind, ind))]

            fH = self.tensor.tensor_matrix_product([bases_eval[x] for x
                                                    in dims], dims)
        else:
            if alpha &lt;= self.tensor.order:
                dims = np.delete(dims, alpha-1)
            fH = self.tensor.tensor_matrix_product([bases_eval[x] for
                                                    x in dims], dims)

        grad, g_alpha, g_gamma = \
            fH.parameter_gradient_eval_diag_dmrg(alpha, bases_eval)

        if isinstance(self.tensor, tensap.TreeBasedTensor):
            # If the order of the children has been modified in grad, compute
            # the inverse permutation.
            ch = tree.children(alpha)

            if ch.size == 0:
                perm_1 = np.array([0])
            else:
                perm_1 = np.argsort(np.concatenate((
                    np.atleast_1d(ch[fH.is_active_node[ch-1]]),
                    np.atleast_1d(
                        ch[np.logical_not(fH.is_active_node[ch-1])]))))
            gamma = tree.parent(alpha)
            ch = tensap.fast_setdiff(tree.children(gamma), alpha)
            perm_1b = np.argsort(np.concatenate((
                    np.atleast_1d(ch[fH.is_active_node[ch-1]]),
                    np.atleast_1d(
                        ch[np.logical_not(fH.is_active_node[ch-1])]))))

            if dmrg_type == &#39;dmrg&#39;:
                perm_1 = np.concatenate((perm_1, perm_1.size+perm_1b))
                perm_2 = []
                if alpha != tree.root and gamma != tree.root:
                    perm_2 = [fH.tensors[alpha-1].order +
                              fH.tensors[gamma-1].order-2]
                perm_3 = []
                if alpha != tree.root and self.tensor.ranks[tree.root-1] &gt; 1:
                    perm_3 = [grad.order-1]
                grad = grad.transpose(np.concatenate(
                    ([0], perm_1+1, perm_2, perm_3)).astype(int))
            elif dmrg_type == &#39;dmrg_low_rank&#39;:
                g_alpha = g_alpha.transpose(np.concatenate(([0], perm_1+1)))
                perm_2 = []
                if gamma != tree.root:
                    # TODO Checks
                    perm_2 = [fH.tensors[gamma-1].order-1]
                perm_3 = []
                if alpha != tree.root and self.tensor.ranks[tree.root-1] &gt; 1:
                    perm_3 = [grad.order-1]
                g_gamma = g_gamma.transpose(np.concatenate(
                    ([0], perm_1b+1, perm_2, perm_3)).astype(int))

                grad = [g_alpha, g_gamma]
            else:
                raise ValueError(&#39;Wrong DMRG type.&#39;)
        return grad

    def eval_derivative(self, n, x, *dims):
        &#39;&#39;&#39;
        Evaluate the n-th order derivative of self at the points x.

        If dims is provided, compute the partial evaluations of the n-th order
        derivative of self at points x n dimensions in dims.

        Parameters
        ----------
        n : int
            The order of derivation.
        x : list or numpy.ndarray
            The points at which the function is to be evaluated.
        *dims : list or numpy.ndarray, optional
            The dimensions of the partial evaluation. If not provided,
            evaluate the function in all dimensions.

        Returns
        -------
        numpu.ndarray or Tensor
            The evaluations of the n-th derivative of self at the points x.

        &#39;&#39;&#39;
        bases_eval = self.bases.eval_derivative(n, x, *dims)
        return self.eval_with_bases_evals(bases_eval, *dims)

    def derivative(self, n):
        &#39;&#39;&#39;
        Compute the n-th order derivative of self.

        Parameters
        ----------
        n : int
            The order of derivation.

        Returns
        -------
        df : tensap.FunctionalTensor
            The n-th order derivative of self.

        &#39;&#39;&#39;
        df = deepcopy(self)
        df.bases = self.bases.derivative(n)
        return df

    def eval_on_grid(self, x, dims=None):
        &#39;&#39;&#39;
        Compute evaluations of self at points x.

        Parameters
        ----------
        x : list
            List such that x[k] contains the grid associated with the (k+1)-th
            variable.
        dims : list or numpy.ndarray, optional
            Array indicating the dimensions associated with x. The default is
            None, indicating all the dimensions.

        Returns
        -------
        out : FunctionalTensor or tensap.Tensor
            The evaluations of self at points x.

        &#39;&#39;&#39;
        if dims is None:
            dims = np.arange(self.bases.length())
        H = self.bases.eval(x, dims)

        if np.size(dims) == self.tensor.order:
            out = self.tensor.tensor_matrix_product(H, dims)
        else:
            out = deepcopy(self)
            out.tensor = self.tensor.tensor_matrix_product(H, dims)
            out.bases = out.bases.remove_bases(dims)
            out.fdims = out.fdims[np.setdiff1d(
                np.range(np.size(out.fdims)), dims)]
            if np.size(out.fdims) == 0:
                out = out.tensor
        return out

    def random(self, *args, **kwargs):
        return self.random_dims(range(len(self.fdims)), *args, **kwargs)

    def random_dims(self, dims, *args, nargout=1):
        &#39;&#39;&#39;
        Evaluate the function in dimensions dims using n points drawn randomly
        according to measure if provided, or to
        self.bases.measure.marginal(dims) otherwise.

        Parameters
        ----------
        dims : list or numpy.ndarray
            The dimensions of the bases to be evaluated.
        n : int, optional
            The number of random evaluations. The default is 1.
        measure : tensap.ProbabilityMeasure, optional
            The probability measure used for the generation of the input
            points. The default is None, indicating to use
            self.measure.marginal(dims).

        Returns
        -------
        bases_eval : list or numpy.ndarray
            Random evaluations of the function.
        x : numpy.ndarray
            The input points, grouped by basis.

        &#39;&#39;&#39;
        bases_eval, x = self.bases.random_dims(dims, *args, nargout=2)
        if nargout == 1:
            return self.eval_with_bases_evals(bases_eval, dims)
        return self.eval_with_bases_evals(bases_eval, dims), x

    def get_random_vector(self):
        &#39;&#39;&#39;
        Return the RandomVector associated with self.bases.

        Returns
        -------
        tensap.RandomVector
            The RandomVector associated with self.bases.

        &#39;&#39;&#39;
        return self.bases.get_random_vector()

    def eval_with_bases_evals(self, bases_eval, dims=None):
        &#39;&#39;&#39;
        Evaluate the function self, given evaluations of self.bases.

        Parameters
        ----------
        bases_eval : list or numpy.ndarray
            The evaluations of self.bases.
        dims : list or numpy.ndarray, optional
            The dimensions of the evaluation. The default is None, indicating
            all the dimensions.

        Returns
        -------
        out : numpy.ndarray or Tensor
            The (partially) evaluated function.

        &#39;&#39;&#39;
        if dims is None:
            dims = range(len(self.bases))
        if len(dims) == 1 and not isinstance(bases_eval, list):
            bases_eval = [bases_eval]
        if len(dims) == self.tensor.order:
            out = self.tensor.tensor_matrix_product_eval_diag(
                bases_eval).numpy()
        else:
            out = deepcopy(self)
            fdims_eval = out.fdims[dims]
            out.tensor = out.tensor.tensor_matrix_product_eval_diag(bases_eval,
                                                                    fdims_eval)
            fdims_eval.sort()
            old_dims = np.setdiff1d(range(self.tensor.order), fdims_eval[2:])
            if out.tensor.shape[fdims_eval[0]] == 1:
                out.tensor = out.tensor.squeeze(fdims_eval[0])
                old_dims = np.delete(old_dims, fdims_eval[0])
            out.bases = out.bases.remove_bases(dims)
            out.fdims = np.delete(out.fdims, dims)
            out.fdims = np.nonzero(np.isin(old_dims, out.fdims))[0]
            if out.fdims.size == 0:
                out = out.tensor
                if out.order == 1:
                    out = out.numpy()
        return out

    def storage(self):
        &#39;&#39;&#39;
        Return the storage requirement of the FunctionalTensor.

        Returns
        -------
        int
            The storage requirement of the FunctionalTensor.

        &#39;&#39;&#39;
        return self.tensor.storage()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tensap.functions.functional_tensor.FunctionalTensor"><code class="flex name class">
<span>class <span class="ident">FunctionalTensor</span></span>
<span>(</span><span>tensor=None, bases=None, fdims=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Class FunctionalTensor.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>tensor</code></strong> :&ensp;<code>Tensor</code> or <code>tensap.FunctionalTensor</code>, optional</dt>
<dd>The tensor of the FunctionalTensor. The default is None.</dd>
<dt><strong><code>bases</code></strong> :&ensp;<code>list</code> or <code>tensap.FunctionalBases</code>, optional</dt>
<dd>The functional bases of the FunctionalTensor. The default is None.</dd>
<dt><strong><code>fdims</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code>, optional</dt>
<dd>The dimensions corresponding to the bases. The default is None.</dd>
</dl>
<p>Constructor for the FunctionalTensor.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>tensor</code></strong> :&ensp;<code>Tensor</code> or <code>tensap.FunctionalTensor</code>, optional</dt>
<dd>The tensor of the FunctionalTensor. The default is None.</dd>
<dt><strong><code>bases</code></strong> :&ensp;<code>list</code> or <code>tensap.FunctionalBases</code>, optional</dt>
<dd>The functional bases of the FunctionalTensor. The default is None.</dd>
<dt><strong><code>fdims</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code>, optional</dt>
<dd>The dimensions corresponding to the bases. The default is None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the provided objects are not of the expected types.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FunctionalTensor(tensap.Function):
    &#39;&#39;&#39;
    Class FunctionalTensor.

    Attributes
    ----------
    tensor : Tensor or tensap.FunctionalTensor, optional
        The tensor of the FunctionalTensor. The default is None.
    bases : list or tensap.FunctionalBases, optional
        The functional bases of the FunctionalTensor. The default is None.
    fdims : list or numpy.ndarray, optional
        The dimensions corresponding to the bases. The default is None.

    &#39;&#39;&#39;

    def __init__(self, tensor=None, bases=None, fdims=None):
        &#39;&#39;&#39;
        Constructor for the FunctionalTensor.

        Parameters
        ----------
        tensor : Tensor or tensap.FunctionalTensor, optional
            The tensor of the FunctionalTensor. The default is None.
        bases : list or tensap.FunctionalBases, optional
            The functional bases of the FunctionalTensor. The default is None.
        fdims : list or numpy.ndarray, optional
            The dimensions corresponding to the bases. The default is None.

        Raises
        ------
        ValueError
            If the provided objects are not of the expected types.

        Returns
        -------
        None.

        &#39;&#39;&#39;
        tensap.Function.__init__(self)

        if isinstance(tensor, FunctionalTensor):
            tensor = tensor.tensor

        self.tensor = tensor

        if bases is not None and \
                not isinstance(bases, (tensap.FunctionalBases, list)):
            raise ValueError(&#39;Must provide a FunctionalBases object, or a &#39; +
                             &#39;cell of bases evaluations.&#39;)
        self.bases = bases
        if fdims is None:
            if tensor.order != len(bases):
                raise ValueError(&#39;Bases must contain as many bases as the &#39; +
                                 &#39;order of the tensor, with possible empty &#39; +
                                 &#39;elements.&#39;)
            fdims = range(tensor.order)
        else:
            if len(fdims) != len(bases):
                raise ValueError(&#39;The number of functional dimensions must &#39; +
                                 &#39;correspond to the number of bases in bases.&#39;)
        self.fdims = np.array(fdims)
        if isinstance(bases, list):
            self.evaluated_bases = True
        else:
            self.evaluated_bases = False
            self.measure = self.bases.measure
            self.dim = np.sum(self.bases.ndim())

    def __plus__(self, y):
        if isinstance(y, FunctionalTensor) and self.bases == y.bases:
            tensor = self.tensor + y.tensor
        else:
            raise NotImplementedError(&#39;Method not implemented.&#39;)
        return FunctionalTensor(tensor, self.bases, self.fdims)

    def is_random(self):
        &#39;&#39;&#39;
        Determine if self is random.

        Returns
        -------
        bool
            Boolean indicating if if self is random.

        &#39;&#39;&#39;
        return isinstance(self.bases.measure, tensap.ProbabilityMeasure)

    def mean(self, *measure):
        &#39;&#39;&#39;
        Compute the expectation of the random variable self(measure) if
        measure is provided, and of self(self.bases.measure) otherwise.

        Parameters
        ----------
        *measure : tensap.RandomVector, optional
            The measure used for the computation of the mean. If not provided,
            indicates to use self.bases.measure.

        Returns
        -------
        float or Tensor
            The mean of the function.

        &#39;&#39;&#39;
        bases_eval = self.bases.mean(None, *measure)
        return self.tensor.tensor_vector_product(bases_eval,
                                                 self.fdims).tolist()

    def expectation(self, *measure):
        &#39;&#39;&#39;
        Compute the expectation of the random variable self(measure) if
        measure is provided, and of self(self.bases.measure) otherwise.

        Parameters
        ----------
        *measure : tensap.RandomVector, optional
            The measure used for the computation of the expectation. If not
            provided, indicates to use self.bases.measure.

        Returns
        -------
        float or Tensor
            The expectation of the function.

        &#39;&#39;&#39;
        return self.mean(*measure)

    def variance(self, *measure):
        &#39;&#39;&#39;
        Compute the variance of the random variable self(measure) if
        measure is provided, and of self(self.bases.measure) otherwise.

        Parameters
        ----------
        *measure : tensap.RandomVector, optional
            The measure used for the computation of the variance. If not
            provided, indicates to use self.bases.measure.

        Returns
        -------
        var : float or Tensor
            The variance of the function.

        &#39;&#39;&#39;
        mean = self.expectation(*measure)
        if np.isscalar(mean):
            var = self.dot_product_expectation(self, None, *measure) - mean**2
        else:
            raise NotImplementedError(&#39;Method not implemented.&#39;)
        return var

    def std(self, *measure):
        &#39;&#39;&#39;&#39;
        Compute the standard deviation of the random variable self(measure) if
        measure is provided, and of self(self.bases.measure) otherwise.

        Parameters
        ----------
        *measure : tensap.RandomVector, optional
            The measure used for the computation of the standard deviation. If
            not provided, indicates to use self.bases.measure.

        Returns
        -------
        v : float or Tensor
            The standard deviation of the function.

        &#39;&#39;&#39;
        return np.sqrt(self.variance(*measure))

    def dot_product_expectation(self, f_2, fdims=None, *measure):
        &#39;&#39;&#39;
        Computes the expectation of self(X)f_2(X) where X is the random vector
        associated with self.bases if measure is not provided, and measure
        otherwise.

        For tensor-valued functions of X (len(X)&lt;self.order), fdims specifies
        the dimensions of self and f_2 corresponding to theRandomVector X.

        Parameters
        ----------
        f_2 : tensap.FunctionalTensor
            The second functional tensor of the product.
        fdims : list of numpy.ndarray, optional
            Specifies the dimensions of self and f_2 corresponding to
            theRandomVector X. The default is None, indicating all the
            dimensions
        *measure : tensap.RandomVector, optional
            The measure used for the computation of the product. If not
            provided, indicates to use self.bases.measure.

        Raises
        ------
        ValueError
            If the two tensors do not have the same order and fdims is not
            specified.
        NotImplementedError
            If the bases of self and f_2 are not equal.

        Returns
        -------
        float or Tensor
            The expectation of self(X)f_2(X).

        &#39;&#39;&#39;
        if fdims is None:
            if self.tensor.order == f_2.tensor.order:
                fdims = range(self.tensor.order)
            else:
                raise ValueError(&#39;Tensors u and v do not have the same &#39; +
                                 &#39;order, must specify fdims.&#39;)
        if self.bases == f_2.bases:
            gram_matrix = self.bases.gram_matrix(fdims, *measure)
        else:
            raise NotImplementedError(&#39;Method not implemented.&#39;)
        tensor = self.tensor.tensor_matrix_product(gram_matrix, fdims)
        return tensor.dot(f_2.tensor)

    def norm(self, *measure):
        &#39;&#39;&#39;
        Return the L^2 norm of self(X), with X = measure if provided, and
        X = self.bases.measure otherwise.

        If self.evaluatedBases is true, without additional information, return
        the canonical norm of self.tensor.

        Parameters
        ----------
        *measure : tensap.RandomVector, optional
            The measure used for the computation of the norm. If not provided,
            indicates to use self.bases.measure.

        Returns
        -------
        float or Tensor
            The L^2 norm of self(X).

        &#39;&#39;&#39;
        if not self.evaluated_bases:
            gram_matrix = self.bases.gram_matrix(range(self.tensor.order),
                                                 *measure)
        else:
            gram_matrix = [np.eye(x.shape[1]) for x in self.bases]
        tensor = self.tensor.tensor_matrix_product(gram_matrix,
                                                   range(self.tensor.order))
        return np.sqrt(tensor.dot(self.tensor))

    def conditional_expectation(self, dims, *args):
        &#39;&#39;&#39;
        Compute the conditional expectation of self with respect to the random
        variables dims (a subset of [1, ..., d]).

        The expectation with respect to other variables (in the complementary
        set of dims) is taken with respect to the probability measure given by
        a tensap.RandomVector if provided as an additional argument, or with
        respect to the probability measure associated with the corresponding
        bases of self.

        Parameters
        ----------
        dims : list or numpy.ndarray
            The dimensions of the random variables with respect to which the
            conditional expectation is to be computed.
        *args : tuple
            Tuple containing a tensap.randomVector giving the probability
            measure of the variables other than the ones in dims. If not
            provided, the measure is infered from self.bases.measure.

        Returns
        -------
        tensap.FunctionalTensor
            The conditional expectation of self with respect to the random
            variables dims, as a len(dims)-order tensor.

        &#39;&#39;&#39;
        dims = np.atleast_1d(dims)
        if np.all([isinstance(x, bool) for x in dims]):
            dims = np.nonzero(dims)[0]

        d = self.tensor.order
        if np.size(dims) == 0:
            return self.expectation(*args)

        dims = np.sort(dims)
        assert np.size(self.fdims) == d and \
            np.array_equal(self.fdims, range(d)), \
            (&#39;Method not implemented for self.fdims different from &#39; +
             &#39;range(d).&#39;)

        dims_C = np.setdiff1d(range(len(self.bases)), dims)
        if dims_C.size == 0:
            return deepcopy(self)

        H = self.bases.mean(dims_C, *args)
        t = self.tensor.tensor_vector_product(H, dims_C)

        bases = self.bases.keep_bases(dims)
        # TODO Take into account the mapping when implemented

        out = FunctionalTensor(t, bases)
        if self.measure is not None:
            out.measure = self.measure.marginal(dims)

        return out

    def variance_conditional_expectation(self, alpha):
        &#39;&#39;&#39;
        Compute the variance of the conditional expectation of self in
        dimensions in alpha.

        Parameters
        ----------
        alpha : list or numpy.ndarray
            Array containing the dimensions (either explicitely or using
            booleans) in which the variance of the conditional expectation is
            computed.

        Returns
        -------
        v : numpy.ndarray
            The variance of the conditional expectation of self in
            dimensions in alpha.

        &#39;&#39;&#39;
        alpha = np.atleast_2d(alpha)
        m = self.expectation()
        v = np.zeros(alpha.shape[0])
        for i in range(alpha.shape[0]):
            u = alpha[i, :]
            if np.all([isinstance(x, bool) for x in u]):
                u = np.nonzero(u)[0]
            if u.size == 0:
                v[i] = 0
            else:
                mu = self.conditional_expectation(u)
                v[i] = mu.dot_product_expectation(mu) - m**2
        return v

    def eval(self, x, *dims):
        &#39;&#39;&#39;
        Evaluate self at the points x.

        If dims is provided, compute the partial evaluations of self at points
        x in dimensions in dims.

        Parameters
        ----------
        x : list or numpy.ndarray or None
            The points at which the function is to be evaluated. If x is None
            and self.evaluated_bases, evaluates the function using the
            evaluations of the bases.
        *dims : list or numpy.ndarray, optional
            The dimensions of the partial evaluation. If not provided,
            evaluate the function in all dimensions.

        Returns
        -------
        numpu.ndarray or Tensor
            The evaluations of self at the points x.

        &#39;&#39;&#39;
        if self.evaluated_bases:
            bases_eval = self.bases
        else:
            bases_eval = self.bases.eval(x, *dims)
        return self.eval_with_bases_evals(bases_eval, *dims)

    def __mul__(self, f_2):
        if isinstance(f_2, (FunctionalTensor, tensap.FunctionalTensor)):
            b = self.bases.kron(f_2.bases)
            t = self.tensor.kron(f_2.tensor)
            out = FunctionalTensor(t, b)
            if isinstance(out.tensor, tensap.TreeBasedTensor) and \
                    out.tensor.ranks[out.tensor.tree.root-1] &gt; 1:
                if self.tensor.ranks[self.tensor.tree.root-1] != \
                        f_2.tensor.ranks[f_2.tensor.tree.root-1]:
                    raise ValueError(&#39;Wrong tensor shapes.&#39;)
                else:
                    root = out.tensor.tree.root
                    c = out.tensor.tensors[root-1]
                    n = self.tensor.ranks[root-1]
                    s = [&#39;:&#39;]*c.order
                    s[-1] = np.arange(n**2, step=n)
                    c = c.sub_tensor(*s)
                    out.tensor.tensors[root-1] = c
                    out.tensor.ranks[root-1] = n
        else:
            out = deepcopy(self)
            out.tensor = out.tensor * f_2
        return out

    def parameter_gradient_eval(self, alpha, x=None, *args):
        &#39;&#39;&#39;
        Compute the gradient of the function with respect to its alpha-th
        parameter, evaluated at some points.

        Parameters
        ----------
        alpha : int
            The number of the parameter with respect to which compute the
            gradient of self.
        x : list or numpy.ndarray, optional
            The points at which the gradient is to be evaluated. The default is
            None, indicating to use self.bases if self.evaluated_bases is True.

        Raises
        ------
        ValueError
            If no input points are provided.

        Returns
        -------
        grad : Tensor
            The gradient of the function with respect to its alpha-th
            parameter, evaluated at some points.

        &#39;&#39;&#39;
        if self.evaluated_bases:
            bases_eval = self.bases
        elif x is not None:
            bases_eval = self.bases.eval(x)
        else:
            raise ValueError(&#39;Must provide the evaluation points or the &#39; +
                             &#39;bases evaluations.&#39;)

        dims = np.arange(self.tensor.order)
        if isinstance(self.tensor, tensap.TreeBasedTensor):
            # Compute fH, the TimesMatrixEvalDiag of f with bases_eval in all
            # the dimensions except the ones associated with alpha (if alpha
            # is a leaf node) or with the inactive children of alpha (if
            # alpha is an internal node). The tensor fH is used to compute
            # the gradient of f with respect to f.tensor.tensors[alpha-1].
            tree = self.tensor.tree
            if tree.is_leaf[alpha-1]:
                dims = dims[tree.dim2ind != alpha]
            else:
                children = tree.children(alpha)
                ind = tensap.fast_intersect(
                    tree.dim2ind,
                    children[np.logical_not(
                        self.tensor.is_active_node[children-1])])
                dims = dims[np.logical_not(np.isin(tree.dim2ind, ind))]

            if np.all(self.tensor.is_active_node):
                fH = self.tensor.tensor_matrix_product([bases_eval[x] for x
                                                        in dims], dims)
            else:
                remaining_dims = np.arange(self.tensor.order)
                tensors = np.array(self.tensor.tensors)
                dim2ind = np.array(tree.dim2ind)

                for leaf in tensap.fast_intersect(tree.dim2ind[dims],
                                                  self.tensor.active_nodes):
                    dims = tensap.fast_setdiff(
                        dims, np.nonzero(tree.dim2ind == leaf)[0][0])
                    tensors[leaf-1] = self.tensor.tensors[leaf-1].\
                        tensor_matrix_product(bases_eval[
                            np.nonzero(tree.dim2ind == leaf)[0][0]], 0)

                for pa in np.unique(tree.parent(tensap.fast_setdiff(
                        tree.dim2ind[dims], self.tensor.active_nodes))):
                    ind = tensap.fast_intersect(tree.dim2ind[dims],
                                                tree.children(pa))
                    ind = tensap.fast_setdiff(ind, self.tensor.active_nodes)
                    dims_loc = np.array([np.nonzero(x == tree.dim2ind)[0][0]
                                         for x in ind])
                    if len(ind) &gt; 1:
                        tensors[pa-1] = self.tensor.tensors[pa-1].\
                            tensor_matrix_product_eval_diag([bases_eval[x] for
                                                             x in dims_loc],
                                                            tree.child_number(
                                                                ind)-1)
                        remaining_dims = tensap.fast_setdiff(remaining_dims,
                                                             dims_loc[1:])
                        if np.all(np.logical_not(self.tensor.is_active_node[
                                tree.children(pa)-1])):
                            dim2ind[dims_loc[0]] = tree.parent(
                                tree.dim2ind[dims_loc[0]])
                        else:
                            dims = tensap.fast_setdiff(dims, dims_loc[0])
                        dim2ind[dims_loc[1:]] = 0
                        perm = np.concatenate((
                            [tree.child_number(ind[0])-1],
                            tensap.fast_setdiff(np.arange(tensors[pa-1].order),
                                                tree.child_number(ind[0])-1)))
                        tensors[pa-1] = tensors[pa-1].itranspose(perm)
                    elif len(ind) == 1:
                        dims = dims[dims != dims_loc]
                        tensors[pa-1] = self.tensor.tensors[pa-1].\
                            tensor_matrix_product([bases_eval[x] for
                                                   x in dims_loc],
                                                  tree.child_number(ind)-1)
                        dim2ind[dims_loc] = tree.dim2ind[dims_loc]

                keep_ind = tensap.fast_setdiff(np.arange(tree.nb_nodes),
                                               tree.dim2ind[dims]-1)
                adj_mat = tree.adjacency_matrix[np.ix_(keep_ind, keep_ind)]
                dim2ind = dim2ind[dim2ind != 0]

                ind = np.zeros(tree.nb_nodes)
                ind[tensap.fast_setdiff(np.arange(tree.nb_nodes),
                                        keep_ind)] = 1
                ind = np.cumsum(ind).astype(int)
                dim2ind -= ind[dim2ind-1]
                alpha = alpha - ind[alpha-1]

                tree = tensap.DimensionTree(dim2ind, adj_mat)
                fH = tensap.TreeBasedTensor(tensors[keep_ind], tree)
                fH = fH.remove_unique_children()
                bases_eval = [bases_eval[x] for x in remaining_dims]
        else:
            if alpha &lt;= self.tensor.order:
                dims = np.delete(dims, alpha-1)
            fH = self.tensor.tensor_matrix_product([bases_eval[x] for
                                                    x in dims], dims)

        grad = fH.parameter_gradient_eval_diag(alpha, bases_eval)
        if isinstance(self.tensor, tensap.TreeBasedTensor) and \
                not tree.is_leaf[alpha-1]:
            # If the order of the children has been modified in grad, compute
            # the inverse permutation.
            ch = tree.children(alpha)
            perm_1 = np.argsort(np.concatenate((
                np.atleast_1d(ch[fH.is_active_node[ch-1]]),
                np.atleast_1d(ch[np.logical_not(fH.is_active_node[ch-1])]))))

            if alpha == tree.root:
                perm_2 = []
            else:
                perm_2 = [fH.tensors[alpha-1].order]

            if alpha != tree.root and self.tensor.ranks[tree.root-1] &gt; 1:
                perm_3 = [grad.order-1]
            else:
                perm_3 = []

            grad = grad.transpose(np.concatenate(([0], perm_1+1,
                                                  perm_2, perm_3)).astype(int))

        return grad

    def parameter_gradient_eval_dmrg(self, alpha, x=None, dmrg_type=&#39;dmrg&#39;,
                                     *args):
        if self.evaluated_bases:
            bases_eval = self.bases
        elif x is not None:
            bases_eval = self.bases.eval(x)
        else:
            raise ValueError(&#39;Must provide the evaluation points or the &#39; +
                             &#39;bases evaluations.&#39;)

        dims = np.arange(self.tensor.order)
        if isinstance(self.tensor, tensap.TreeBasedTensor):
            # Compute fH, the TimesMatrixEvalDiag of f with bases_eval in all
            # the dimensions except the ones associated with alpha (if alpha
            # is a leaf node) or with the inactive children of alpha (if
            # alpha is an internal node). The tensor fH is used to compute
            # the gradient of f with respect to f.tensor.tensors[alpha-1].
            tree = self.tensor.tree
            if tree.is_leaf[alpha-1]:
                dims = dims[tree.dim2ind != alpha]
            else:
                children = tree.children(alpha)
                ind = tensap.fast_intersect(
                    tree.dim2ind,
                    children[np.logical_not(
                        self.tensor.is_active_node[children-1])])
                dims = dims[np.logical_not(np.isin(tree.dim2ind, ind))]

            fH = self.tensor.tensor_matrix_product([bases_eval[x] for x
                                                    in dims], dims)
        else:
            if alpha &lt;= self.tensor.order:
                dims = np.delete(dims, alpha-1)
            fH = self.tensor.tensor_matrix_product([bases_eval[x] for
                                                    x in dims], dims)

        grad, g_alpha, g_gamma = \
            fH.parameter_gradient_eval_diag_dmrg(alpha, bases_eval)

        if isinstance(self.tensor, tensap.TreeBasedTensor):
            # If the order of the children has been modified in grad, compute
            # the inverse permutation.
            ch = tree.children(alpha)

            if ch.size == 0:
                perm_1 = np.array([0])
            else:
                perm_1 = np.argsort(np.concatenate((
                    np.atleast_1d(ch[fH.is_active_node[ch-1]]),
                    np.atleast_1d(
                        ch[np.logical_not(fH.is_active_node[ch-1])]))))
            gamma = tree.parent(alpha)
            ch = tensap.fast_setdiff(tree.children(gamma), alpha)
            perm_1b = np.argsort(np.concatenate((
                    np.atleast_1d(ch[fH.is_active_node[ch-1]]),
                    np.atleast_1d(
                        ch[np.logical_not(fH.is_active_node[ch-1])]))))

            if dmrg_type == &#39;dmrg&#39;:
                perm_1 = np.concatenate((perm_1, perm_1.size+perm_1b))
                perm_2 = []
                if alpha != tree.root and gamma != tree.root:
                    perm_2 = [fH.tensors[alpha-1].order +
                              fH.tensors[gamma-1].order-2]
                perm_3 = []
                if alpha != tree.root and self.tensor.ranks[tree.root-1] &gt; 1:
                    perm_3 = [grad.order-1]
                grad = grad.transpose(np.concatenate(
                    ([0], perm_1+1, perm_2, perm_3)).astype(int))
            elif dmrg_type == &#39;dmrg_low_rank&#39;:
                g_alpha = g_alpha.transpose(np.concatenate(([0], perm_1+1)))
                perm_2 = []
                if gamma != tree.root:
                    # TODO Checks
                    perm_2 = [fH.tensors[gamma-1].order-1]
                perm_3 = []
                if alpha != tree.root and self.tensor.ranks[tree.root-1] &gt; 1:
                    perm_3 = [grad.order-1]
                g_gamma = g_gamma.transpose(np.concatenate(
                    ([0], perm_1b+1, perm_2, perm_3)).astype(int))

                grad = [g_alpha, g_gamma]
            else:
                raise ValueError(&#39;Wrong DMRG type.&#39;)
        return grad

    def eval_derivative(self, n, x, *dims):
        &#39;&#39;&#39;
        Evaluate the n-th order derivative of self at the points x.

        If dims is provided, compute the partial evaluations of the n-th order
        derivative of self at points x n dimensions in dims.

        Parameters
        ----------
        n : int
            The order of derivation.
        x : list or numpy.ndarray
            The points at which the function is to be evaluated.
        *dims : list or numpy.ndarray, optional
            The dimensions of the partial evaluation. If not provided,
            evaluate the function in all dimensions.

        Returns
        -------
        numpu.ndarray or Tensor
            The evaluations of the n-th derivative of self at the points x.

        &#39;&#39;&#39;
        bases_eval = self.bases.eval_derivative(n, x, *dims)
        return self.eval_with_bases_evals(bases_eval, *dims)

    def derivative(self, n):
        &#39;&#39;&#39;
        Compute the n-th order derivative of self.

        Parameters
        ----------
        n : int
            The order of derivation.

        Returns
        -------
        df : tensap.FunctionalTensor
            The n-th order derivative of self.

        &#39;&#39;&#39;
        df = deepcopy(self)
        df.bases = self.bases.derivative(n)
        return df

    def eval_on_grid(self, x, dims=None):
        &#39;&#39;&#39;
        Compute evaluations of self at points x.

        Parameters
        ----------
        x : list
            List such that x[k] contains the grid associated with the (k+1)-th
            variable.
        dims : list or numpy.ndarray, optional
            Array indicating the dimensions associated with x. The default is
            None, indicating all the dimensions.

        Returns
        -------
        out : FunctionalTensor or tensap.Tensor
            The evaluations of self at points x.

        &#39;&#39;&#39;
        if dims is None:
            dims = np.arange(self.bases.length())
        H = self.bases.eval(x, dims)

        if np.size(dims) == self.tensor.order:
            out = self.tensor.tensor_matrix_product(H, dims)
        else:
            out = deepcopy(self)
            out.tensor = self.tensor.tensor_matrix_product(H, dims)
            out.bases = out.bases.remove_bases(dims)
            out.fdims = out.fdims[np.setdiff1d(
                np.range(np.size(out.fdims)), dims)]
            if np.size(out.fdims) == 0:
                out = out.tensor
        return out

    def random(self, *args, **kwargs):
        return self.random_dims(range(len(self.fdims)), *args, **kwargs)

    def random_dims(self, dims, *args, nargout=1):
        &#39;&#39;&#39;
        Evaluate the function in dimensions dims using n points drawn randomly
        according to measure if provided, or to
        self.bases.measure.marginal(dims) otherwise.

        Parameters
        ----------
        dims : list or numpy.ndarray
            The dimensions of the bases to be evaluated.
        n : int, optional
            The number of random evaluations. The default is 1.
        measure : tensap.ProbabilityMeasure, optional
            The probability measure used for the generation of the input
            points. The default is None, indicating to use
            self.measure.marginal(dims).

        Returns
        -------
        bases_eval : list or numpy.ndarray
            Random evaluations of the function.
        x : numpy.ndarray
            The input points, grouped by basis.

        &#39;&#39;&#39;
        bases_eval, x = self.bases.random_dims(dims, *args, nargout=2)
        if nargout == 1:
            return self.eval_with_bases_evals(bases_eval, dims)
        return self.eval_with_bases_evals(bases_eval, dims), x

    def get_random_vector(self):
        &#39;&#39;&#39;
        Return the RandomVector associated with self.bases.

        Returns
        -------
        tensap.RandomVector
            The RandomVector associated with self.bases.

        &#39;&#39;&#39;
        return self.bases.get_random_vector()

    def eval_with_bases_evals(self, bases_eval, dims=None):
        &#39;&#39;&#39;
        Evaluate the function self, given evaluations of self.bases.

        Parameters
        ----------
        bases_eval : list or numpy.ndarray
            The evaluations of self.bases.
        dims : list or numpy.ndarray, optional
            The dimensions of the evaluation. The default is None, indicating
            all the dimensions.

        Returns
        -------
        out : numpy.ndarray or Tensor
            The (partially) evaluated function.

        &#39;&#39;&#39;
        if dims is None:
            dims = range(len(self.bases))
        if len(dims) == 1 and not isinstance(bases_eval, list):
            bases_eval = [bases_eval]
        if len(dims) == self.tensor.order:
            out = self.tensor.tensor_matrix_product_eval_diag(
                bases_eval).numpy()
        else:
            out = deepcopy(self)
            fdims_eval = out.fdims[dims]
            out.tensor = out.tensor.tensor_matrix_product_eval_diag(bases_eval,
                                                                    fdims_eval)
            fdims_eval.sort()
            old_dims = np.setdiff1d(range(self.tensor.order), fdims_eval[2:])
            if out.tensor.shape[fdims_eval[0]] == 1:
                out.tensor = out.tensor.squeeze(fdims_eval[0])
                old_dims = np.delete(old_dims, fdims_eval[0])
            out.bases = out.bases.remove_bases(dims)
            out.fdims = np.delete(out.fdims, dims)
            out.fdims = np.nonzero(np.isin(old_dims, out.fdims))[0]
            if out.fdims.size == 0:
                out = out.tensor
                if out.order == 1:
                    out = out.numpy()
        return out

    def storage(self):
        &#39;&#39;&#39;
        Return the storage requirement of the FunctionalTensor.

        Returns
        -------
        int
            The storage requirement of the FunctionalTensor.

        &#39;&#39;&#39;
        return self.tensor.storage()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="tensap.functions.function.Function" href="function.html#tensap.functions.function.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.conditional_expectation"><code class="name flex">
<span>def <span class="ident">conditional_expectation</span></span>(<span>self, dims, *args)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the conditional expectation of self with respect to the random
variables dims (a subset of [1, &hellip;, d]).</p>
<p>The expectation with respect to other variables (in the complementary
set of dims) is taken with respect to the probability measure given by
a tensap.RandomVector if provided as an additional argument, or with
respect to the probability measure associated with the corresponding
bases of self.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dims</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>The dimensions of the random variables with respect to which the
conditional expectation is to be computed.</dd>
<dt><strong><code>*args</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Tuple containing a tensap.randomVector giving the probability
measure of the variables other than the ones in dims. If not
provided, the measure is infered from self.bases.measure.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tensap.FunctionalTensor</code></dt>
<dd>The conditional expectation of self with respect to the random
variables dims, as a len(dims)-order tensor.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conditional_expectation(self, dims, *args):
    &#39;&#39;&#39;
    Compute the conditional expectation of self with respect to the random
    variables dims (a subset of [1, ..., d]).

    The expectation with respect to other variables (in the complementary
    set of dims) is taken with respect to the probability measure given by
    a tensap.RandomVector if provided as an additional argument, or with
    respect to the probability measure associated with the corresponding
    bases of self.

    Parameters
    ----------
    dims : list or numpy.ndarray
        The dimensions of the random variables with respect to which the
        conditional expectation is to be computed.
    *args : tuple
        Tuple containing a tensap.randomVector giving the probability
        measure of the variables other than the ones in dims. If not
        provided, the measure is infered from self.bases.measure.

    Returns
    -------
    tensap.FunctionalTensor
        The conditional expectation of self with respect to the random
        variables dims, as a len(dims)-order tensor.

    &#39;&#39;&#39;
    dims = np.atleast_1d(dims)
    if np.all([isinstance(x, bool) for x in dims]):
        dims = np.nonzero(dims)[0]

    d = self.tensor.order
    if np.size(dims) == 0:
        return self.expectation(*args)

    dims = np.sort(dims)
    assert np.size(self.fdims) == d and \
        np.array_equal(self.fdims, range(d)), \
        (&#39;Method not implemented for self.fdims different from &#39; +
         &#39;range(d).&#39;)

    dims_C = np.setdiff1d(range(len(self.bases)), dims)
    if dims_C.size == 0:
        return deepcopy(self)

    H = self.bases.mean(dims_C, *args)
    t = self.tensor.tensor_vector_product(H, dims_C)

    bases = self.bases.keep_bases(dims)
    # TODO Take into account the mapping when implemented

    out = FunctionalTensor(t, bases)
    if self.measure is not None:
        out.measure = self.measure.marginal(dims)

    return out</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.derivative"><code class="name flex">
<span>def <span class="ident">derivative</span></span>(<span>self, n)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the n-th order derivative of self.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>The order of derivation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>tensap.FunctionalTensor</code></dt>
<dd>The n-th order derivative of self.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def derivative(self, n):
    &#39;&#39;&#39;
    Compute the n-th order derivative of self.

    Parameters
    ----------
    n : int
        The order of derivation.

    Returns
    -------
    df : tensap.FunctionalTensor
        The n-th order derivative of self.

    &#39;&#39;&#39;
    df = deepcopy(self)
    df.bases = self.bases.derivative(n)
    return df</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.dot_product_expectation"><code class="name flex">
<span>def <span class="ident">dot_product_expectation</span></span>(<span>self, f_2, fdims=None, *measure)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the expectation of self(X)f_2(X) where X is the random vector
associated with self.bases if measure is not provided, and measure
otherwise.</p>
<p>For tensor-valued functions of X (len(X)&lt;self.order), fdims specifies
the dimensions of self and f_2 corresponding to theRandomVector X.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f_2</code></strong> :&ensp;<code>tensap.FunctionalTensor</code></dt>
<dd>The second functional tensor of the product.</dd>
<dt><strong><code>fdims</code></strong> :&ensp;<code>list</code> of <code>numpy.ndarray</code>, optional</dt>
<dd>Specifies the dimensions of self and f_2 corresponding to
theRandomVector X. The default is None, indicating all the
dimensions</dd>
<dt><strong><code>*measure</code></strong> :&ensp;<code>tensap.RandomVector</code>, optional</dt>
<dd>The measure used for the computation of the product. If not
provided, indicates to use self.bases.measure.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the two tensors do not have the same order and fdims is not
specified.</dd>
<dt><code>NotImplementedError</code></dt>
<dd>If the bases of self and f_2 are not equal.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code> or <code>Tensor</code></dt>
<dd>The expectation of self(X)f_2(X).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dot_product_expectation(self, f_2, fdims=None, *measure):
    &#39;&#39;&#39;
    Computes the expectation of self(X)f_2(X) where X is the random vector
    associated with self.bases if measure is not provided, and measure
    otherwise.

    For tensor-valued functions of X (len(X)&lt;self.order), fdims specifies
    the dimensions of self and f_2 corresponding to theRandomVector X.

    Parameters
    ----------
    f_2 : tensap.FunctionalTensor
        The second functional tensor of the product.
    fdims : list of numpy.ndarray, optional
        Specifies the dimensions of self and f_2 corresponding to
        theRandomVector X. The default is None, indicating all the
        dimensions
    *measure : tensap.RandomVector, optional
        The measure used for the computation of the product. If not
        provided, indicates to use self.bases.measure.

    Raises
    ------
    ValueError
        If the two tensors do not have the same order and fdims is not
        specified.
    NotImplementedError
        If the bases of self and f_2 are not equal.

    Returns
    -------
    float or Tensor
        The expectation of self(X)f_2(X).

    &#39;&#39;&#39;
    if fdims is None:
        if self.tensor.order == f_2.tensor.order:
            fdims = range(self.tensor.order)
        else:
            raise ValueError(&#39;Tensors u and v do not have the same &#39; +
                             &#39;order, must specify fdims.&#39;)
    if self.bases == f_2.bases:
        gram_matrix = self.bases.gram_matrix(fdims, *measure)
    else:
        raise NotImplementedError(&#39;Method not implemented.&#39;)
    tensor = self.tensor.tensor_matrix_product(gram_matrix, fdims)
    return tensor.dot(f_2.tensor)</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.eval"><code class="name flex">
<span>def <span class="ident">eval</span></span>(<span>self, x, *dims)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate self at the points x.</p>
<p>If dims is provided, compute the partial evaluations of self at points
x in dimensions in dims.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code> or <code>None</code></dt>
<dd>The points at which the function is to be evaluated. If x is None
and self.evaluated_bases, evaluates the function using the
evaluations of the bases.</dd>
<dt><strong><code>*dims</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code>, optional</dt>
<dd>The dimensions of the partial evaluation. If not provided,
evaluate the function in all dimensions.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpu.ndarray</code> or <code>Tensor</code></dt>
<dd>The evaluations of self at the points x.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def eval(self, x, *dims):
    &#39;&#39;&#39;
    Evaluate self at the points x.

    If dims is provided, compute the partial evaluations of self at points
    x in dimensions in dims.

    Parameters
    ----------
    x : list or numpy.ndarray or None
        The points at which the function is to be evaluated. If x is None
        and self.evaluated_bases, evaluates the function using the
        evaluations of the bases.
    *dims : list or numpy.ndarray, optional
        The dimensions of the partial evaluation. If not provided,
        evaluate the function in all dimensions.

    Returns
    -------
    numpu.ndarray or Tensor
        The evaluations of self at the points x.

    &#39;&#39;&#39;
    if self.evaluated_bases:
        bases_eval = self.bases
    else:
        bases_eval = self.bases.eval(x, *dims)
    return self.eval_with_bases_evals(bases_eval, *dims)</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.eval_derivative"><code class="name flex">
<span>def <span class="ident">eval_derivative</span></span>(<span>self, n, x, *dims)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the n-th order derivative of self at the points x.</p>
<p>If dims is provided, compute the partial evaluations of the n-th order
derivative of self at points x n dimensions in dims.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>The order of derivation.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>The points at which the function is to be evaluated.</dd>
<dt><strong><code>*dims</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code>, optional</dt>
<dd>The dimensions of the partial evaluation. If not provided,
evaluate the function in all dimensions.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpu.ndarray</code> or <code>Tensor</code></dt>
<dd>The evaluations of the n-th derivative of self at the points x.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def eval_derivative(self, n, x, *dims):
    &#39;&#39;&#39;
    Evaluate the n-th order derivative of self at the points x.

    If dims is provided, compute the partial evaluations of the n-th order
    derivative of self at points x n dimensions in dims.

    Parameters
    ----------
    n : int
        The order of derivation.
    x : list or numpy.ndarray
        The points at which the function is to be evaluated.
    *dims : list or numpy.ndarray, optional
        The dimensions of the partial evaluation. If not provided,
        evaluate the function in all dimensions.

    Returns
    -------
    numpu.ndarray or Tensor
        The evaluations of the n-th derivative of self at the points x.

    &#39;&#39;&#39;
    bases_eval = self.bases.eval_derivative(n, x, *dims)
    return self.eval_with_bases_evals(bases_eval, *dims)</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.eval_on_grid"><code class="name flex">
<span>def <span class="ident">eval_on_grid</span></span>(<span>self, x, dims=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute evaluations of self at points x.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>list</code></dt>
<dd>List such that x[k] contains the grid associated with the (k+1)-th
variable.</dd>
<dt><strong><code>dims</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code>, optional</dt>
<dd>Array indicating the dimensions associated with x. The default is
None, indicating all the dimensions.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code><a title="tensap.functions.functional_tensor.FunctionalTensor" href="#tensap.functions.functional_tensor.FunctionalTensor">FunctionalTensor</a></code> or <code>tensap.Tensor</code></dt>
<dd>The evaluations of self at points x.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def eval_on_grid(self, x, dims=None):
    &#39;&#39;&#39;
    Compute evaluations of self at points x.

    Parameters
    ----------
    x : list
        List such that x[k] contains the grid associated with the (k+1)-th
        variable.
    dims : list or numpy.ndarray, optional
        Array indicating the dimensions associated with x. The default is
        None, indicating all the dimensions.

    Returns
    -------
    out : FunctionalTensor or tensap.Tensor
        The evaluations of self at points x.

    &#39;&#39;&#39;
    if dims is None:
        dims = np.arange(self.bases.length())
    H = self.bases.eval(x, dims)

    if np.size(dims) == self.tensor.order:
        out = self.tensor.tensor_matrix_product(H, dims)
    else:
        out = deepcopy(self)
        out.tensor = self.tensor.tensor_matrix_product(H, dims)
        out.bases = out.bases.remove_bases(dims)
        out.fdims = out.fdims[np.setdiff1d(
            np.range(np.size(out.fdims)), dims)]
        if np.size(out.fdims) == 0:
            out = out.tensor
    return out</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.eval_with_bases_evals"><code class="name flex">
<span>def <span class="ident">eval_with_bases_evals</span></span>(<span>self, bases_eval, dims=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the function self, given evaluations of self.bases.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>bases_eval</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>The evaluations of self.bases.</dd>
<dt><strong><code>dims</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code>, optional</dt>
<dd>The dimensions of the evaluation. The default is None, indicating
all the dimensions.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>numpy.ndarray</code> or <code>Tensor</code></dt>
<dd>The (partially) evaluated function.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def eval_with_bases_evals(self, bases_eval, dims=None):
    &#39;&#39;&#39;
    Evaluate the function self, given evaluations of self.bases.

    Parameters
    ----------
    bases_eval : list or numpy.ndarray
        The evaluations of self.bases.
    dims : list or numpy.ndarray, optional
        The dimensions of the evaluation. The default is None, indicating
        all the dimensions.

    Returns
    -------
    out : numpy.ndarray or Tensor
        The (partially) evaluated function.

    &#39;&#39;&#39;
    if dims is None:
        dims = range(len(self.bases))
    if len(dims) == 1 and not isinstance(bases_eval, list):
        bases_eval = [bases_eval]
    if len(dims) == self.tensor.order:
        out = self.tensor.tensor_matrix_product_eval_diag(
            bases_eval).numpy()
    else:
        out = deepcopy(self)
        fdims_eval = out.fdims[dims]
        out.tensor = out.tensor.tensor_matrix_product_eval_diag(bases_eval,
                                                                fdims_eval)
        fdims_eval.sort()
        old_dims = np.setdiff1d(range(self.tensor.order), fdims_eval[2:])
        if out.tensor.shape[fdims_eval[0]] == 1:
            out.tensor = out.tensor.squeeze(fdims_eval[0])
            old_dims = np.delete(old_dims, fdims_eval[0])
        out.bases = out.bases.remove_bases(dims)
        out.fdims = np.delete(out.fdims, dims)
        out.fdims = np.nonzero(np.isin(old_dims, out.fdims))[0]
        if out.fdims.size == 0:
            out = out.tensor
            if out.order == 1:
                out = out.numpy()
    return out</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.expectation"><code class="name flex">
<span>def <span class="ident">expectation</span></span>(<span>self, *measure)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the expectation of the random variable self(measure) if
measure is provided, and of self(self.bases.measure) otherwise.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>*measure</code></strong> :&ensp;<code>tensap.RandomVector</code>, optional</dt>
<dd>The measure used for the computation of the expectation. If not
provided, indicates to use self.bases.measure.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code> or <code>Tensor</code></dt>
<dd>The expectation of the function.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expectation(self, *measure):
    &#39;&#39;&#39;
    Compute the expectation of the random variable self(measure) if
    measure is provided, and of self(self.bases.measure) otherwise.

    Parameters
    ----------
    *measure : tensap.RandomVector, optional
        The measure used for the computation of the expectation. If not
        provided, indicates to use self.bases.measure.

    Returns
    -------
    float or Tensor
        The expectation of the function.

    &#39;&#39;&#39;
    return self.mean(*measure)</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.get_random_vector"><code class="name flex">
<span>def <span class="ident">get_random_vector</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the RandomVector associated with self.bases.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tensap.RandomVector</code></dt>
<dd>The RandomVector associated with self.bases.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_random_vector(self):
    &#39;&#39;&#39;
    Return the RandomVector associated with self.bases.

    Returns
    -------
    tensap.RandomVector
        The RandomVector associated with self.bases.

    &#39;&#39;&#39;
    return self.bases.get_random_vector()</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.is_random"><code class="name flex">
<span>def <span class="ident">is_random</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Determine if self is random.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>Boolean indicating if if self is random.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_random(self):
    &#39;&#39;&#39;
    Determine if self is random.

    Returns
    -------
    bool
        Boolean indicating if if self is random.

    &#39;&#39;&#39;
    return isinstance(self.bases.measure, tensap.ProbabilityMeasure)</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.mean"><code class="name flex">
<span>def <span class="ident">mean</span></span>(<span>self, *measure)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the expectation of the random variable self(measure) if
measure is provided, and of self(self.bases.measure) otherwise.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>*measure</code></strong> :&ensp;<code>tensap.RandomVector</code>, optional</dt>
<dd>The measure used for the computation of the mean. If not provided,
indicates to use self.bases.measure.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code> or <code>Tensor</code></dt>
<dd>The mean of the function.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean(self, *measure):
    &#39;&#39;&#39;
    Compute the expectation of the random variable self(measure) if
    measure is provided, and of self(self.bases.measure) otherwise.

    Parameters
    ----------
    *measure : tensap.RandomVector, optional
        The measure used for the computation of the mean. If not provided,
        indicates to use self.bases.measure.

    Returns
    -------
    float or Tensor
        The mean of the function.

    &#39;&#39;&#39;
    bases_eval = self.bases.mean(None, *measure)
    return self.tensor.tensor_vector_product(bases_eval,
                                             self.fdims).tolist()</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.norm"><code class="name flex">
<span>def <span class="ident">norm</span></span>(<span>self, *measure)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the L^2 norm of self(X), with X = measure if provided, and
X = self.bases.measure otherwise.</p>
<p>If self.evaluatedBases is true, without additional information, return
the canonical norm of self.tensor.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>*measure</code></strong> :&ensp;<code>tensap.RandomVector</code>, optional</dt>
<dd>The measure used for the computation of the norm. If not provided,
indicates to use self.bases.measure.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code> or <code>Tensor</code></dt>
<dd>The L^2 norm of self(X).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def norm(self, *measure):
    &#39;&#39;&#39;
    Return the L^2 norm of self(X), with X = measure if provided, and
    X = self.bases.measure otherwise.

    If self.evaluatedBases is true, without additional information, return
    the canonical norm of self.tensor.

    Parameters
    ----------
    *measure : tensap.RandomVector, optional
        The measure used for the computation of the norm. If not provided,
        indicates to use self.bases.measure.

    Returns
    -------
    float or Tensor
        The L^2 norm of self(X).

    &#39;&#39;&#39;
    if not self.evaluated_bases:
        gram_matrix = self.bases.gram_matrix(range(self.tensor.order),
                                             *measure)
    else:
        gram_matrix = [np.eye(x.shape[1]) for x in self.bases]
    tensor = self.tensor.tensor_matrix_product(gram_matrix,
                                               range(self.tensor.order))
    return np.sqrt(tensor.dot(self.tensor))</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.parameter_gradient_eval"><code class="name flex">
<span>def <span class="ident">parameter_gradient_eval</span></span>(<span>self, alpha, x=None, *args)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the gradient of the function with respect to its alpha-th
parameter, evaluated at some points.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of the parameter with respect to which compute the
gradient of self.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code>, optional</dt>
<dd>The points at which the gradient is to be evaluated. The default is
None, indicating to use self.bases if self.evaluated_bases is True.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If no input points are provided.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>grad</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>The gradient of the function with respect to its alpha-th
parameter, evaluated at some points.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parameter_gradient_eval(self, alpha, x=None, *args):
    &#39;&#39;&#39;
    Compute the gradient of the function with respect to its alpha-th
    parameter, evaluated at some points.

    Parameters
    ----------
    alpha : int
        The number of the parameter with respect to which compute the
        gradient of self.
    x : list or numpy.ndarray, optional
        The points at which the gradient is to be evaluated. The default is
        None, indicating to use self.bases if self.evaluated_bases is True.

    Raises
    ------
    ValueError
        If no input points are provided.

    Returns
    -------
    grad : Tensor
        The gradient of the function with respect to its alpha-th
        parameter, evaluated at some points.

    &#39;&#39;&#39;
    if self.evaluated_bases:
        bases_eval = self.bases
    elif x is not None:
        bases_eval = self.bases.eval(x)
    else:
        raise ValueError(&#39;Must provide the evaluation points or the &#39; +
                         &#39;bases evaluations.&#39;)

    dims = np.arange(self.tensor.order)
    if isinstance(self.tensor, tensap.TreeBasedTensor):
        # Compute fH, the TimesMatrixEvalDiag of f with bases_eval in all
        # the dimensions except the ones associated with alpha (if alpha
        # is a leaf node) or with the inactive children of alpha (if
        # alpha is an internal node). The tensor fH is used to compute
        # the gradient of f with respect to f.tensor.tensors[alpha-1].
        tree = self.tensor.tree
        if tree.is_leaf[alpha-1]:
            dims = dims[tree.dim2ind != alpha]
        else:
            children = tree.children(alpha)
            ind = tensap.fast_intersect(
                tree.dim2ind,
                children[np.logical_not(
                    self.tensor.is_active_node[children-1])])
            dims = dims[np.logical_not(np.isin(tree.dim2ind, ind))]

        if np.all(self.tensor.is_active_node):
            fH = self.tensor.tensor_matrix_product([bases_eval[x] for x
                                                    in dims], dims)
        else:
            remaining_dims = np.arange(self.tensor.order)
            tensors = np.array(self.tensor.tensors)
            dim2ind = np.array(tree.dim2ind)

            for leaf in tensap.fast_intersect(tree.dim2ind[dims],
                                              self.tensor.active_nodes):
                dims = tensap.fast_setdiff(
                    dims, np.nonzero(tree.dim2ind == leaf)[0][0])
                tensors[leaf-1] = self.tensor.tensors[leaf-1].\
                    tensor_matrix_product(bases_eval[
                        np.nonzero(tree.dim2ind == leaf)[0][0]], 0)

            for pa in np.unique(tree.parent(tensap.fast_setdiff(
                    tree.dim2ind[dims], self.tensor.active_nodes))):
                ind = tensap.fast_intersect(tree.dim2ind[dims],
                                            tree.children(pa))
                ind = tensap.fast_setdiff(ind, self.tensor.active_nodes)
                dims_loc = np.array([np.nonzero(x == tree.dim2ind)[0][0]
                                     for x in ind])
                if len(ind) &gt; 1:
                    tensors[pa-1] = self.tensor.tensors[pa-1].\
                        tensor_matrix_product_eval_diag([bases_eval[x] for
                                                         x in dims_loc],
                                                        tree.child_number(
                                                            ind)-1)
                    remaining_dims = tensap.fast_setdiff(remaining_dims,
                                                         dims_loc[1:])
                    if np.all(np.logical_not(self.tensor.is_active_node[
                            tree.children(pa)-1])):
                        dim2ind[dims_loc[0]] = tree.parent(
                            tree.dim2ind[dims_loc[0]])
                    else:
                        dims = tensap.fast_setdiff(dims, dims_loc[0])
                    dim2ind[dims_loc[1:]] = 0
                    perm = np.concatenate((
                        [tree.child_number(ind[0])-1],
                        tensap.fast_setdiff(np.arange(tensors[pa-1].order),
                                            tree.child_number(ind[0])-1)))
                    tensors[pa-1] = tensors[pa-1].itranspose(perm)
                elif len(ind) == 1:
                    dims = dims[dims != dims_loc]
                    tensors[pa-1] = self.tensor.tensors[pa-1].\
                        tensor_matrix_product([bases_eval[x] for
                                               x in dims_loc],
                                              tree.child_number(ind)-1)
                    dim2ind[dims_loc] = tree.dim2ind[dims_loc]

            keep_ind = tensap.fast_setdiff(np.arange(tree.nb_nodes),
                                           tree.dim2ind[dims]-1)
            adj_mat = tree.adjacency_matrix[np.ix_(keep_ind, keep_ind)]
            dim2ind = dim2ind[dim2ind != 0]

            ind = np.zeros(tree.nb_nodes)
            ind[tensap.fast_setdiff(np.arange(tree.nb_nodes),
                                    keep_ind)] = 1
            ind = np.cumsum(ind).astype(int)
            dim2ind -= ind[dim2ind-1]
            alpha = alpha - ind[alpha-1]

            tree = tensap.DimensionTree(dim2ind, adj_mat)
            fH = tensap.TreeBasedTensor(tensors[keep_ind], tree)
            fH = fH.remove_unique_children()
            bases_eval = [bases_eval[x] for x in remaining_dims]
    else:
        if alpha &lt;= self.tensor.order:
            dims = np.delete(dims, alpha-1)
        fH = self.tensor.tensor_matrix_product([bases_eval[x] for
                                                x in dims], dims)

    grad = fH.parameter_gradient_eval_diag(alpha, bases_eval)
    if isinstance(self.tensor, tensap.TreeBasedTensor) and \
            not tree.is_leaf[alpha-1]:
        # If the order of the children has been modified in grad, compute
        # the inverse permutation.
        ch = tree.children(alpha)
        perm_1 = np.argsort(np.concatenate((
            np.atleast_1d(ch[fH.is_active_node[ch-1]]),
            np.atleast_1d(ch[np.logical_not(fH.is_active_node[ch-1])]))))

        if alpha == tree.root:
            perm_2 = []
        else:
            perm_2 = [fH.tensors[alpha-1].order]

        if alpha != tree.root and self.tensor.ranks[tree.root-1] &gt; 1:
            perm_3 = [grad.order-1]
        else:
            perm_3 = []

        grad = grad.transpose(np.concatenate(([0], perm_1+1,
                                              perm_2, perm_3)).astype(int))

    return grad</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.parameter_gradient_eval_dmrg"><code class="name flex">
<span>def <span class="ident">parameter_gradient_eval_dmrg</span></span>(<span>self, alpha, x=None, dmrg_type='dmrg', *args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parameter_gradient_eval_dmrg(self, alpha, x=None, dmrg_type=&#39;dmrg&#39;,
                                 *args):
    if self.evaluated_bases:
        bases_eval = self.bases
    elif x is not None:
        bases_eval = self.bases.eval(x)
    else:
        raise ValueError(&#39;Must provide the evaluation points or the &#39; +
                         &#39;bases evaluations.&#39;)

    dims = np.arange(self.tensor.order)
    if isinstance(self.tensor, tensap.TreeBasedTensor):
        # Compute fH, the TimesMatrixEvalDiag of f with bases_eval in all
        # the dimensions except the ones associated with alpha (if alpha
        # is a leaf node) or with the inactive children of alpha (if
        # alpha is an internal node). The tensor fH is used to compute
        # the gradient of f with respect to f.tensor.tensors[alpha-1].
        tree = self.tensor.tree
        if tree.is_leaf[alpha-1]:
            dims = dims[tree.dim2ind != alpha]
        else:
            children = tree.children(alpha)
            ind = tensap.fast_intersect(
                tree.dim2ind,
                children[np.logical_not(
                    self.tensor.is_active_node[children-1])])
            dims = dims[np.logical_not(np.isin(tree.dim2ind, ind))]

        fH = self.tensor.tensor_matrix_product([bases_eval[x] for x
                                                in dims], dims)
    else:
        if alpha &lt;= self.tensor.order:
            dims = np.delete(dims, alpha-1)
        fH = self.tensor.tensor_matrix_product([bases_eval[x] for
                                                x in dims], dims)

    grad, g_alpha, g_gamma = \
        fH.parameter_gradient_eval_diag_dmrg(alpha, bases_eval)

    if isinstance(self.tensor, tensap.TreeBasedTensor):
        # If the order of the children has been modified in grad, compute
        # the inverse permutation.
        ch = tree.children(alpha)

        if ch.size == 0:
            perm_1 = np.array([0])
        else:
            perm_1 = np.argsort(np.concatenate((
                np.atleast_1d(ch[fH.is_active_node[ch-1]]),
                np.atleast_1d(
                    ch[np.logical_not(fH.is_active_node[ch-1])]))))
        gamma = tree.parent(alpha)
        ch = tensap.fast_setdiff(tree.children(gamma), alpha)
        perm_1b = np.argsort(np.concatenate((
                np.atleast_1d(ch[fH.is_active_node[ch-1]]),
                np.atleast_1d(
                    ch[np.logical_not(fH.is_active_node[ch-1])]))))

        if dmrg_type == &#39;dmrg&#39;:
            perm_1 = np.concatenate((perm_1, perm_1.size+perm_1b))
            perm_2 = []
            if alpha != tree.root and gamma != tree.root:
                perm_2 = [fH.tensors[alpha-1].order +
                          fH.tensors[gamma-1].order-2]
            perm_3 = []
            if alpha != tree.root and self.tensor.ranks[tree.root-1] &gt; 1:
                perm_3 = [grad.order-1]
            grad = grad.transpose(np.concatenate(
                ([0], perm_1+1, perm_2, perm_3)).astype(int))
        elif dmrg_type == &#39;dmrg_low_rank&#39;:
            g_alpha = g_alpha.transpose(np.concatenate(([0], perm_1+1)))
            perm_2 = []
            if gamma != tree.root:
                # TODO Checks
                perm_2 = [fH.tensors[gamma-1].order-1]
            perm_3 = []
            if alpha != tree.root and self.tensor.ranks[tree.root-1] &gt; 1:
                perm_3 = [grad.order-1]
            g_gamma = g_gamma.transpose(np.concatenate(
                ([0], perm_1b+1, perm_2, perm_3)).astype(int))

            grad = [g_alpha, g_gamma]
        else:
            raise ValueError(&#39;Wrong DMRG type.&#39;)
    return grad</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.random_dims"><code class="name flex">
<span>def <span class="ident">random_dims</span></span>(<span>self, dims, *args, nargout=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the function in dimensions dims using n points drawn randomly
according to measure if provided, or to
self.bases.measure.marginal(dims) otherwise.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dims</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>The dimensions of the bases to be evaluated.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of random evaluations. The default is 1.</dd>
<dt><strong><code>measure</code></strong> :&ensp;<code>tensap.ProbabilityMeasure</code>, optional</dt>
<dd>The probability measure used for the generation of the input
points. The default is None, indicating to use
self.measure.marginal(dims).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>bases_eval</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>Random evaluations of the function.</dd>
<dt><strong><code>x</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The input points, grouped by basis.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_dims(self, dims, *args, nargout=1):
    &#39;&#39;&#39;
    Evaluate the function in dimensions dims using n points drawn randomly
    according to measure if provided, or to
    self.bases.measure.marginal(dims) otherwise.

    Parameters
    ----------
    dims : list or numpy.ndarray
        The dimensions of the bases to be evaluated.
    n : int, optional
        The number of random evaluations. The default is 1.
    measure : tensap.ProbabilityMeasure, optional
        The probability measure used for the generation of the input
        points. The default is None, indicating to use
        self.measure.marginal(dims).

    Returns
    -------
    bases_eval : list or numpy.ndarray
        Random evaluations of the function.
    x : numpy.ndarray
        The input points, grouped by basis.

    &#39;&#39;&#39;
    bases_eval, x = self.bases.random_dims(dims, *args, nargout=2)
    if nargout == 1:
        return self.eval_with_bases_evals(bases_eval, dims)
    return self.eval_with_bases_evals(bases_eval, dims), x</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.std"><code class="name flex">
<span>def <span class="ident">std</span></span>(<span>self, *measure)</span>
</code></dt>
<dd>
<div class="desc"><p>'
Compute the standard deviation of the random variable self(measure) if
measure is provided, and of self(self.bases.measure) otherwise.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>*measure</code></strong> :&ensp;<code>tensap.RandomVector</code>, optional</dt>
<dd>The measure used for the computation of the standard deviation. If
not provided, indicates to use self.bases.measure.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>v</code></strong> :&ensp;<code>float</code> or <code>Tensor</code></dt>
<dd>The standard deviation of the function.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def std(self, *measure):
    &#39;&#39;&#39;&#39;
    Compute the standard deviation of the random variable self(measure) if
    measure is provided, and of self(self.bases.measure) otherwise.

    Parameters
    ----------
    *measure : tensap.RandomVector, optional
        The measure used for the computation of the standard deviation. If
        not provided, indicates to use self.bases.measure.

    Returns
    -------
    v : float or Tensor
        The standard deviation of the function.

    &#39;&#39;&#39;
    return np.sqrt(self.variance(*measure))</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.storage"><code class="name flex">
<span>def <span class="ident">storage</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the storage requirement of the FunctionalTensor.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>The storage requirement of the FunctionalTensor.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def storage(self):
    &#39;&#39;&#39;
    Return the storage requirement of the FunctionalTensor.

    Returns
    -------
    int
        The storage requirement of the FunctionalTensor.

    &#39;&#39;&#39;
    return self.tensor.storage()</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.variance"><code class="name flex">
<span>def <span class="ident">variance</span></span>(<span>self, *measure)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the variance of the random variable self(measure) if
measure is provided, and of self(self.bases.measure) otherwise.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>*measure</code></strong> :&ensp;<code>tensap.RandomVector</code>, optional</dt>
<dd>The measure used for the computation of the variance. If not
provided, indicates to use self.bases.measure.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>var</code></strong> :&ensp;<code>float</code> or <code>Tensor</code></dt>
<dd>The variance of the function.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def variance(self, *measure):
    &#39;&#39;&#39;
    Compute the variance of the random variable self(measure) if
    measure is provided, and of self(self.bases.measure) otherwise.

    Parameters
    ----------
    *measure : tensap.RandomVector, optional
        The measure used for the computation of the variance. If not
        provided, indicates to use self.bases.measure.

    Returns
    -------
    var : float or Tensor
        The variance of the function.

    &#39;&#39;&#39;
    mean = self.expectation(*measure)
    if np.isscalar(mean):
        var = self.dot_product_expectation(self, None, *measure) - mean**2
    else:
        raise NotImplementedError(&#39;Method not implemented.&#39;)
    return var</code></pre>
</details>
</dd>
<dt id="tensap.functions.functional_tensor.FunctionalTensor.variance_conditional_expectation"><code class="name flex">
<span>def <span class="ident">variance_conditional_expectation</span></span>(<span>self, alpha)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the variance of the conditional expectation of self in
dimensions in alpha.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>Array containing the dimensions (either explicitely or using
booleans) in which the variance of the conditional expectation is
computed.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>v</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The variance of the conditional expectation of self in
dimensions in alpha.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def variance_conditional_expectation(self, alpha):
    &#39;&#39;&#39;
    Compute the variance of the conditional expectation of self in
    dimensions in alpha.

    Parameters
    ----------
    alpha : list or numpy.ndarray
        Array containing the dimensions (either explicitely or using
        booleans) in which the variance of the conditional expectation is
        computed.

    Returns
    -------
    v : numpy.ndarray
        The variance of the conditional expectation of self in
        dimensions in alpha.

    &#39;&#39;&#39;
    alpha = np.atleast_2d(alpha)
    m = self.expectation()
    v = np.zeros(alpha.shape[0])
    for i in range(alpha.shape[0]):
        u = alpha[i, :]
        if np.all([isinstance(x, bool) for x in u]):
            u = np.nonzero(u)[0]
        if u.size == 0:
            v[i] = 0
        else:
            mu = self.conditional_expectation(u)
            v[i] = mu.dot_product_expectation(mu) - m**2
    return v</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="tensap.functions.function.Function" href="function.html#tensap.functions.function.Function">Function</a></b></code>:
<ul class="hlist">
<li><code><a title="tensap.functions.function.Function.eval_on_tensor_grid" href="function.html#tensap.functions.function.Function.eval_on_tensor_grid">eval_on_tensor_grid</a></code></li>
<li><code><a title="tensap.functions.function.Function.fplot" href="function.html#tensap.functions.function.Function.fplot">fplot</a></code></li>
<li><code><a title="tensap.functions.function.Function.partial_evaluation" href="function.html#tensap.functions.function.Function.partial_evaluation">partial_evaluation</a></code></li>
<li><code><a title="tensap.functions.function.Function.random" href="function.html#tensap.functions.function.Function.random">random</a></code></li>
<li><code><a title="tensap.functions.function.Function.store_eval" href="function.html#tensap.functions.function.Function.store_eval">store_eval</a></code></li>
<li><code><a title="tensap.functions.function.Function.surf" href="function.html#tensap.functions.function.Function.surf">surf</a></code></li>
<li><code><a title="tensap.functions.function.Function.test_error" href="function.html#tensap.functions.function.Function.test_error">test_error</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tensap.functions" href="index.html">tensap.functions</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tensap.functions.functional_tensor.FunctionalTensor" href="#tensap.functions.functional_tensor.FunctionalTensor">FunctionalTensor</a></code></h4>
<ul class="">
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.conditional_expectation" href="#tensap.functions.functional_tensor.FunctionalTensor.conditional_expectation">conditional_expectation</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.derivative" href="#tensap.functions.functional_tensor.FunctionalTensor.derivative">derivative</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.dot_product_expectation" href="#tensap.functions.functional_tensor.FunctionalTensor.dot_product_expectation">dot_product_expectation</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.eval" href="#tensap.functions.functional_tensor.FunctionalTensor.eval">eval</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.eval_derivative" href="#tensap.functions.functional_tensor.FunctionalTensor.eval_derivative">eval_derivative</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.eval_on_grid" href="#tensap.functions.functional_tensor.FunctionalTensor.eval_on_grid">eval_on_grid</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.eval_with_bases_evals" href="#tensap.functions.functional_tensor.FunctionalTensor.eval_with_bases_evals">eval_with_bases_evals</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.expectation" href="#tensap.functions.functional_tensor.FunctionalTensor.expectation">expectation</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.get_random_vector" href="#tensap.functions.functional_tensor.FunctionalTensor.get_random_vector">get_random_vector</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.is_random" href="#tensap.functions.functional_tensor.FunctionalTensor.is_random">is_random</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.mean" href="#tensap.functions.functional_tensor.FunctionalTensor.mean">mean</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.norm" href="#tensap.functions.functional_tensor.FunctionalTensor.norm">norm</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.parameter_gradient_eval" href="#tensap.functions.functional_tensor.FunctionalTensor.parameter_gradient_eval">parameter_gradient_eval</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.parameter_gradient_eval_dmrg" href="#tensap.functions.functional_tensor.FunctionalTensor.parameter_gradient_eval_dmrg">parameter_gradient_eval_dmrg</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.random_dims" href="#tensap.functions.functional_tensor.FunctionalTensor.random_dims">random_dims</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.std" href="#tensap.functions.functional_tensor.FunctionalTensor.std">std</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.storage" href="#tensap.functions.functional_tensor.FunctionalTensor.storage">storage</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.variance" href="#tensap.functions.functional_tensor.FunctionalTensor.variance">variance</a></code></li>
<li><code><a title="tensap.functions.functional_tensor.FunctionalTensor.variance_conditional_expectation" href="#tensap.functions.functional_tensor.FunctionalTensor.variance_conditional_expectation">variance_conditional_expectation</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>
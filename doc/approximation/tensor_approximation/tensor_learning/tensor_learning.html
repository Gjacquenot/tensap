<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>tensap.approximation.tensor_approximation.tensor_learning.tensor_learning API documentation</title>
<meta name="description" content="Module tensor_learning â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tensap.approximation.tensor_approximation.tensor_learning.tensor_learning</code></h1>
</header>
<section id="section-intro">
<p>Module tensor_learning.</p>
<p>Copyright (c) 2020, Anthony Nouy, Erwan Grelier
This file is part of tensap (tensor approximation package).</p>
<p>tensap is free software: you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.</p>
<p>tensap is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
See the
GNU Lesser General Public License for more details.</p>
<p>You should have received a copy of the GNU Lesser General Public License
along with tensap.
If not, see <a href="https://www.gnu.org/licenses/">https://www.gnu.org/licenses/</a>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
Module tensor_learning.

Copyright (c) 2020, Anthony Nouy, Erwan Grelier
This file is part of tensap (tensor approximation package).

tensap is free software: you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

tensap is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Lesser General Public License for more details.

You should have received a copy of the GNU Lesser General Public License
along with tensap.  If not, see &lt;https://www.gnu.org/licenses/&gt;.

&#39;&#39;&#39;

from abc import abstractmethod
from copy import deepcopy
import pprint
import numpy as np
import tensap


class TensorLearning(tensap.Learning):
    &#39;&#39;&#39;
    Class TensorLearning.

    See also tensap.Learning.

    Attributes
    ----------
    order : int
        Order of the tensor.
    bases : list or numpy.ndarray
        The functional bases used for by the approximation.
    bases_eval : list or numpy.ndarray
        The evaluations of the bases on the training data.
    bases_eval_test : list or numpy.ndarray
        The evaluations of the bases on the test data.
    algorithm : str
        The choice of algorithm.
    initialization_type : str
        The type of initialization.
    initial_guess : tensap.Tensor
        An initial guess for the solver.
    tree_adaptation : bool
        Boolean indicating if tree adaptation is to be performed.
    tree_adaptation_options : dict
        Options for the tree adaptation.
    rank_adaptation : bool
        Boolean indicating if rank adaptation is to be performed.
    rank_adaptation_options : dict
        Options for the rank adaptation.
    tolerance : dict
        Tolerances for the solver.
    linear_model_learning_parameters : dict
        Parameters for the linear solvers.
    alternating_minimization_parameters : dict
        Parameters for the alternating minimization.
    bases_adaptation_path : list or numpy.ndarray
        Paths for each basis for the basis adaptation.
    store_iterates : bool
        Boolean indicating if the iterates are to be stored.
    rank : int or list or numpy.ndarray
        The rank of the expected approximation.
    output_dimension : int
        The dimension of the outputs of the function to be approximated.
    _number_of_parameters : int
        The number of parameters of the approximation.
    _exploration_strategy : numpy.ndarray
        The exploration strategy.
    _warnings : dict
        Dictionnary containing the state of some warnings to be displayed.

    &#39;&#39;&#39;

    def __init__(self, loss):
        &#39;&#39;&#39;
        Constructor for the class TensorLearning.

        See also tensap.Learning.

        Parameters
        ----------
        loss : tensap.LossFunction
            The loss function associated with the solver.

        Returns
        -------
        None.

        &#39;&#39;&#39;
        super().__init__(loss)
        self.linear_model_learning = tensap.Learning.linear_model(loss)
        self.order = None
        self.bases = None
        self.bases_eval = None
        self.bases_eval_test = None
        self.algorithm = &#39;standard&#39;
        self.initialization_type = None
        self.initial_guess = None
        self.tree_adaptation = False
        self.tree_adaptation_options = {&#39;tolerance&#39;: None,
                                        &#39;max_iterations&#39;: 100,
                                        &#39;force_rank_adaptation&#39;: True}
        self.rank_adaptation = False
        self.rank_adaptation_options = {&#39;max_iterations&#39;: 10,
                                        &#39;early_stopping&#39;: False,
                                        &#39;early_stopping_factor&#39;: 10}
        self.tolerance = {&#39;on_error&#39;: 1e-6, &#39;on_stagnation&#39;: 1e-6}
        self.linear_model_learning_parameters = \
            {&#39;identical_for_all_parameters&#39;: True}
        self.alternating_minimization_parameters = {&#39;display&#39;: False,
                                                    &#39;max_iterations&#39;: 30,
                                                    &#39;stagnation&#39;: 1e-6,
                                                    &#39;random&#39;: False}
        self.bases_adaptation_path = None
        self.store_iterates = True
        self.rank = 1
        self.output_dimension = 1

        self.model_selection = False
        # Possible choices: &#39;test_error&#39;, &#39;cv_error&#39;
        self.model_selection_options[&#39;type&#39;] = &#39;test_error&#39;

        self._number_of_parameters = None
        self._exploration_strategy = None
        self._warnings = {&#39;orthonormality_warning_display&#39;: True,
                          &#39;empty_bases_warning_display&#39;: True}

    def __repr__(self):
        return pprint.pformat(self.__dict__, indent=4, width=1)

    def solve(self, *args):
        &#39;&#39;&#39;
        Solver for the learning problem with tensor formats.

        Parameters
        ----------
        *args : misc
            Additional arguments, if needed.

        Raises
        ------
        NotImplementedError
            If the required solver is not implemented.
        ValueError
            If the required data is not provided.

        Returns
        -------
        f : tensap.FunctionalTensor
            The learned approximation.
        output : dict
            The outputs of the solver.

        &#39;&#39;&#39;
        # If possible, deduce from training_data the output dimension
        if isinstance(self.training_data, list) and \
                len(self.training_data) == 2 and \
                np.ndim(self.training_data[1]) == 2:
            self.output_dimension = self.training_data[1].shape[1]
        if self.output_dimension &gt; 1 and \
            (not isinstance(self, tensap.TreeBasedTensorLearning) or
             not isinstance(self.loss_function, (tensap.SquareLossFunction,
                                                 tensap.CustomLossFunction))):
            raise NotImplementedError(
                &#39;Solver not implemented for vector-valued functions &#39; +
                &#39;approximation, use TreeBasedTensorLearning with a &#39; +
                &#39;SquareLossFunction or a CustomLossFunction instead.&#39;)

        if self.output_dimension &gt; 1 and \
            &#39;type&#39; in self.rank_adaptation_options and \
                self.rank_adaptation_options[&#39;type&#39;] == &#39;inner&#39;:
            print(&#39;Inner rank adaptation not implemented for &#39; +
                  &#39;output_dimension greater than 1, disabling it.&#39;)
            del self.rank_adaptation_options[&#39;type&#39;]

        if self._warnings[&#39;orthonormality_warning_display&#39;] and \
            (self.bases is None or (self.bases is not None and not
                                    np.all([x.is_orthonormal for x in
                                            self.bases.bases]))):
            self._warnings[&#39;orthonormality_warning_display&#39;] = False
            print(&#39;The implemented learning algorithms are designed &#39; +
                  &#39;for orthonormal bases. These algorithms work with &#39; +
                  &#39;non-orthonormal bases, but without some guarantees &#39; +
                  &#39;on their results.&#39;)

        # If no bases are provided, warn that the returned functions are
        # evaluated on the training data
        if self.bases is None and \
                self._warnings[&#39;empty_bases_warning_display&#39;]:
            self._warnings[&#39;empty_bases_warning_display&#39;] = False
            print(&#39;The returned functions are evaluated on the training &#39; +
                  &#39;data. To evaluate them at other points, assign to the &#39; +
                  &#39;FunctionalTensor a nonempty field bases and set the &#39; +
                  &#39;attribute evaluatedBases to False.&#39;)

        # If the test error cannot be computed, it is disabled
        if self.test_error and \
            not isinstance(self.bases, tensap.FunctionalBases) and \
                self.bases_eval_test is None:
            print(&#39;The test error cannot be computed.&#39;)
            self.test_error = False

        # Assert if basis adaptation can be performed
        if self.bases_adaptation_path is None and \
                not hasattr(self.bases, &#39;adaptation_path&#39;):
            if isinstance(self.linear_model_learning, (list, np.ndarray)) and \
                    np.any([x.basis_adaptation for
                            x in self.linear_model_learning]):
                print(&#39;Cannot perform basis adaptation, disabling it.&#39;)
                for linear_solver in self.linear_model_learning:
                    setattr(linear_solver, &#39;basis_adaptation&#39;, False)

            elif not isinstance(self.linear_model_learning,
                                (list, np.ndarray)) and \
                    self.linear_model_learning.basis_adaptation:
                print(&#39;Cannot perform basis adaptation, disabling it.&#39;)
                self.linear_model_learning.basis_adaptation = False

        # Bases evaluation
        if hasattr(self.bases, &#39;eval&#39;):
            if self.training_data is not None and self.bases_eval is None:
                if isinstance(self.training_data, list) and \
                        self.training_data[0] is not None:
                    self.bases_eval = self.bases.eval(self.training_data[0])
                elif not isinstance(self.training_data, list) and \
                        self.training_data is not None:
                    self.bases_eval = self.bases.eval(self.training_data)
                else:
                    raise ValueError(&#39;Must provide input training data.&#39;)

            if self.test_error and self.test_data is not None and \
                    self.bases_eval_test is None:
                if isinstance(self.test_data, list) and \
                        self.test_data[0] is not None:
                    self.bases_eval_test = self.bases.eval(self.test_data[0])
                elif not isinstance(self.test_data, list) and \
                        self.test_data is not None:
                    self.bases_eval_test = self.bases.eval(self.test_data)
                else:
                    raise ValueError(&#39;Must provide input test data.&#39;)

        if self.model_selection:
            self.store_iterates = True
            if self.model_selection_options[&#39;type&#39;] == &#39;test_error&#39; and \
                    not self.test_error:
                print(&#39;Cannot perform test error based model selection, &#39; +
                      &#39;disabling it.&#39;)
                self.model_selection = False
            elif self.model_selection_options[&#39;type&#39;] == &#39;cv_error&#39; and \
                    not self.error_estimation:
                self.error_estimation = True

        if self.rank_adaptation:
            if &#39;type&#39; not in self.rank_adaptation_options or \
                    self.rank_adaptation_options[&#39;type&#39;] == &#39;standard&#39;:
                fun, output = self._solve_adaptation()
            elif isinstance(self.rank_adaptation_options[&#39;type&#39;], str):
                adapt_type = self.rank_adaptation_options[&#39;type&#39;]
                if adapt_type == &#39;dmrg&#39; or adapt_type == &#39;dmrg_low_rank&#39;:
                    adapt_type = &#39;dmrg&#39;

                # Call the method corresponding to the rank adaptation option
                fun, output = eval(&#39;self._solve_&#39; + adapt_type.lower() +
                                   &#39;_rank_adaptation()&#39;)
            else:
                raise ValueError(&#39;The rank_adaptation_options attribute &#39; +
                                 &#39;must be either inexistant or a string.&#39;)
        elif self.algorithm == &#39;standard&#39;:
            fun, output = self._solve_standard()
        else:
            fun, output = eval(&#39;self._solve_&#39; + self.algorithm.lower() + &#39;()&#39;)

        if self.model_selection:
            if self.model_selection_options[&#39;type&#39;] == &#39;test_error&#39;:
                ind = np.argmin(output[&#39;test_error_iterations&#39;])
                fun = output[&#39;iterates&#39;][ind]
                output[&#39;selected_model_number&#39;] = ind
                if &#39;error_iterations&#39; in output and \
                    np.size(output[&#39;error_iterations&#39;]) &gt; 0:
                    output[&#39;error&#39;] = output[&#39;error_iterations&#39;][ind]
                output[&#39;test_error&#39;] = output[&#39;test_error_iterations&#39;][ind]
                if self.display:
                    print(&#39;\nModel selection using the test error: model &#39; +
                          &#39;#%i selected&#39; % ind)
            elif self.model_selection_options[&#39;type&#39;] == &#39;cv_error&#39;:
                ind = np.argmin(output[&#39;error_iterations&#39;])
                fun = output[&#39;iterates&#39;][ind]
                output[&#39;selected_model_number&#39;] = ind
                if &#39;test_error_iterations&#39; in output and \
                    np.size(output[&#39;test_error_iterations&#39;]) &gt; 0:
                    output[&#39;test_error&#39;] = output[&#39;test_error_iterations&#39;][ind]
                output[&#39;error&#39;] = output[&#39;error_iterations&#39;][ind]
                if self.display:
                    print(&#39;\nModel selection using the cross-validation &#39; +
                          &#39; error: model #%i selected&#39; % ind)
            else:
                print(&#39;Wrong model selection type, returning the last &#39; +
                      &#39;iterate.&#39;)

            if self.display:
                if self.alternating_minimization_parameters[&#39;display&#39;]:
                    print(&#39;&#39;)
                self.final_display(fun)
                if &#39;error&#39; in output:
                    print(&#39;, error = %2.5e&#39; % output[&#39;error&#39;], end=&#39;&#39;)
                if &#39;test_error&#39; in output:
                    print(&#39;, test error = %2.5e&#39; % output[&#39;test_error&#39;],
                          end=&#39;&#39;)
                print(&#39;&#39;)

        return fun, output

    def _solve_standard(self):
        &#39;&#39;&#39;
        Solver for the learning problem with tensor formats using the standard
        algorithm (without adaptation).

        Raises
        ------
        ValueError
            If the number of LinearModelLearning objects is not equal to
            _numberOfParameters.

        Returns
        -------
        f : tensap.FunctionalTensor
            The learned approximation.
        output : dict
            The outputs of the solver.

        &#39;&#39;&#39;
        output = {&#39;flag&#39;: 0}

        self, f = self.initialize()
        f = tensap.FunctionalTensor(f, self.bases_eval)

        # Replication of the LinearModelLearning objects
        if self.linear_model_learning_parameters[
                &#39;identical_for_all_parameters&#39;] and \
                not isinstance(self.linear_model_learning, (list, np.ndarray)):
            self.linear_model_learning = list(map(deepcopy, [
                self.linear_model_learning] * self._number_of_parameters))
        elif isinstance(self.linear_model_learning, (list, np.ndarray)) and \
                len(self.linear_model_learning) != self._number_of_parameters:
            raise ValueError(&#39;Must provide self._numberOfParameters &#39; +
                             &#39;LinearModelLearning objects.&#39;)

        if self.error_estimation:
            for x in self.linear_model_learning:
                setattr(x, &#39;error_estimation&#39;, True)

        # Working set paths
        if isinstance(self.linear_model_learning, (list, np.ndarray)) and \
            np.any([x.basis_adaptation for
                    x in self.linear_model_learning]) and \
                self.bases_adaptation_path is None:
            self.bases_adaptation_path = self.bases.adaptation_path()

        if self.alternating_minimization_parameters[&#39;max_iterations&#39;] == 0:
            return f, output

        output[&#39;stagnation_criterion&#39;] = []
        output[&#39;iterates&#39;] = []
        output[&#39;error_iterations&#39;] = []
        output[&#39;test_error_iterations&#39;] = []

        # Alternating minimization loop
        for iteration in range(self.alternating_minimization_parameters
                               [&#39;max_iterations&#39;]):
            self, f = self.pre_processing(f)
            f0 = deepcopy(f)

            if self.alternating_minimization_parameters[&#39;random&#39;]:
                # Randomize the exploration strategy
                alpha_list = self.randomize_exploration_strategy()
            else:
                alpha_list = self._exploration_strategy

            for alpha in alpha_list:
                self, A, b, f = \
                    self.prepare_alternating_minimization_system(f, alpha)
                self.linear_model_learning[alpha-1].training_data = [None, b]
                self.linear_model_learning[alpha-1].basis = None
                self.linear_model_learning[alpha-1].basis_eval = A

                coef, output_tmp = self.linear_model_learning[alpha-1].solve()
                if coef is None or np.count_nonzero(coef) == 0 or \
                        not np.all(np.isfinite(coef)):
                    print(&#39;Empty, zero or NaN solution, returning to the &#39; +
                          &#39;previous iteration.&#39;)
                    output[&#39;flag&#39;] = -2
                    output[&#39;error&#39;] = np.inf
                    break

                f = self.set_parameter(f, alpha, coef)

            stagnation = self.stagnation_criterion(f, f0)
            output[&#39;stagnation_criterion&#39;].append(stagnation)

            if self.store_iterates:
                if isinstance(self.bases, tensap.FunctionalBases):
                    output[&#39;iterates&#39;].append(tensap.FunctionalTensor(
                        f.tensor, self.bases))
                else:
                    output[&#39;iterates&#39;].append(f)

            if &#39;error&#39; in output_tmp:
                output[&#39;error&#39;] = output_tmp[&#39;error&#39;]
                output[&#39;error_iterations&#39;].append(output[&#39;error&#39;])

            if self.test_error:
                f_eval_test = tensap.FunctionalTensor(f, self.bases_eval_test)
                output[&#39;test_error&#39;] = self.loss_function.test_error(
                    f_eval_test, self.test_data)
                output[&#39;test_error_iterations&#39;].append(output[&#39;test_error&#39;])

            if self.alternating_minimization_parameters[&#39;display&#39;]:
                print(&#39;\tAlt. min. iteration %s: stagnation = %2.5e&#39; %
                      (str(iteration).
                       zfill(len(str(self.alternating_minimization_parameters
                                     [&#39;max_iterations&#39;]-1))),
                       stagnation), end=&#39;&#39;)
                if &#39;error&#39; in output:
                    print(&#39;, error = %2.5e&#39; % output[&#39;error&#39;], end=&#39;&#39;)
                if self.test_error:
                    if not np.isscalar(output[&#39;test_error&#39;]):
                        output[&#39;test_error&#39;] = output[&#39;test_error&#39;].numpy()
                    print(&#39;, test error = %2.5e&#39; % output[&#39;test_error&#39;],
                          end=&#39;&#39;)
                print(&#39;&#39;)

            if iteration &gt; 0 and stagnation &lt; \
                    self.alternating_minimization_parameters[&#39;stagnation&#39;]:
                output[&#39;flag&#39;] = 1
                break

            if self.test_error and \
                    output[&#39;test_error&#39;] &lt; self.tolerance[&#39;on_error&#39;]:
                output[&#39;flag&#39;] = 1
                break

        if isinstance(self.bases, tensap.FunctionalBases):
            f = tensap.FunctionalTensor(f.tensor, self.bases)
        output[&#39;iter&#39;] = iteration

        if self.display and not self.model_selection:
            if self.alternating_minimization_parameters[&#39;display&#39;]:
                print(&#39;&#39;)
            self.final_display(f)
            if &#39;error&#39; in output:
                print(&#39;, error = %2.5e&#39; % output[&#39;error&#39;], end=&#39;&#39;)
            if &#39;test_error&#39; in output:
                print(&#39;, test error = %2.5e&#39; % output[&#39;test_error&#39;], end=&#39;&#39;)
            print(&#39;&#39;)

        return f, output

    def _solve_adaptation(self):
        &#39;&#39;&#39;
        Solver for the learning problem with tensor formats using the adaptive
        algorithm.

        Returns
        -------
        f : tensap.FunctionalTensor
            The learned approximation.
        output : dict
            The outputs of the solver.

        &#39;&#39;&#39;

        flag = 0
        output = {&#39;enriched_nodes_iterations&#39;:
                  np.empty(self.rank_adaptation_options[&#39;max_iterations&#39;],
                           dtype=object)}
        tree_adapt = False

        f = None
        errors = np.zeros(self.rank_adaptation_options[&#39;max_iterations&#39;])
        test_errors = np.zeros(self.rank_adaptation_options[&#39;max_iterations&#39;])
        iterates = np.empty(self.rank_adaptation_options[&#39;max_iterations&#39;],
                            dtype=object)

        # new_rank = s_local.rank
        new_rank = self.rank
        s_local = self.local_solver()
        s_local.model_selection = False

        enriched_nodes = np.array([])
        for iteration in range(self.rank_adaptation_options[&#39;max_iterations&#39;]):
            s_local.bases = self.bases
            s_local.bases_eval = self.bases_eval
            s_local.bases_eval_test = self.bases_eval_test
            s_local.training_data = self.training_data
            s_local.test_data = self.test_data
            s_local.rank = new_rank

            f_old = deepcopy(f)
            f, output_local = s_local.solve()
            s_local = self.local_solver()

            if &#39;error&#39; in output_local:
                errors[iteration] = output_local[&#39;error&#39;]
                if np.isinf(errors[iteration]):
                    print(&#39;Infinite error, returning to the previous iterate.&#39;)
                    f = f_old
                    iteration -= 1
                    flag = -2
                    break

            if self.test_error:
                f_eval_test = tensap.FunctionalTensor(
                    f, self.bases_eval_test)
                test_errors[iteration] = self.loss_function.test_error(
                    f_eval_test, self.test_data)

            if self.store_iterates:
                if isinstance(self.bases, tensap.FunctionalBases):
                    iterates[iteration] = tensap.FunctionalTensor(f.tensor,
                                                                  self.bases)
                else:
                    iterates[iteration] = f

            if self.display:
                if self.alternating_minimization_parameters[&#39;display&#39;]:
                    print(&#39;&#39;)
                print(&#39;\nRank adaptation, iteration %i:&#39; % (iteration))
                self.adaptation_display(f, enriched_nodes)
                print(&#39;\tStorage complexity = %i&#39; % f.tensor.storage())

                if errors[iteration] != 0:
                    print(&#39;\tError      = %2.5e&#39; % errors[iteration])
                if test_errors[iteration] != 0:
                    print(&#39;\tTest error = %2.5e&#39; % test_errors[iteration])
                if self.alternating_minimization_parameters[&#39;display&#39;]:
                    print(&#39;&#39;)

            if iteration == self.rank_adaptation_options[&#39;max_iterations&#39;]-1:
                break

            if (self.test_error and test_errors[iteration] &lt;
                    self.tolerance[&#39;on_error&#39;]) or \
                (&#39;error&#39; in output_local and errors[iteration] &lt;
                 self.tolerance[&#39;on_error&#39;]):
                flag = 1
                break

            fac = self.rank_adaptation_options[&#39;early_stopping_factor&#39;]
            cond = iteration &gt; 0 and (self.test_error and
                                      (np.isnan(test_errors[iteration]) or
                                       fac * np.min(test_errors[:iteration]) &lt;
                                       test_errors[iteration]) or
                                      (&#39;error&#39; in output_local and
                                       (np.isnan(errors[iteration]) or
                                        fac * np.min(errors[:iteration]) &lt;
                                        errors[iteration])))
            if self.rank_adaptation_options[&#39;early_stopping&#39;] and cond:
                print(&#39;Early stopping&#39;, end=&#39;&#39;)
                if &#39;error&#39; in output_local:
                    print(&#39;, error = %2.5e&#39; % errors[iteration], end=&#39;&#39;)
                if self.test_error:
                    print(&#39;, test error = %2.5e&#39; % test_errors[iteration],
                          end=&#39;&#39;)
                print(&#39;\n&#39;)
                iteration -= 1
                f = f_old
                flag = -1
                break

            adapted_tree = False
            if s_local.tree_adaptation and iteration &gt; 0 and \
                    (not self.tree_adaptation_options[&#39;force_rank_adaptation&#39;]
                     or not tree_adapt):
                C_old = f.tensor.storage()
                self, f, output = self.adapt_tree(f, errors[iteration],
                                                  None, output, iteration)
                adapted_tree = output[&#39;adapted_tree&#39;]
                if adapted_tree:
                    if self.display:
                        print(&#39;\t\tStorage complexity before permutation &#39; +
                              &#39;= %i&#39; % C_old)
                        print(&#39;\t\tStorage complexity after permutation &#39; +
                              &#39;= %i&#39; % f.tensor.storage())
                    if self.test_error:
                        f_eval_test = tensap.FunctionalTensor(
                            f, self.bases_eval_test)
                        test_errors[iteration] = self.loss_function.test_error(
                            f_eval_test, self.test_data)
                        if self.display:
                            print(&#39;\t\tTest error after permutation &#39; +
                                  &#39;= %2.5e&#39; % test_errors[iteration])

                    if self.alternating_minimization_parameters[&#39;display&#39;]:
                        print(&#39;&#39;)

            if not self.tree_adaptation or not adapted_tree:
                if iteration &gt; 0 and not tree_adapt:
                    stagnation = self.stagnation_criterion(
                        tensap.FunctionalTensor(f.tensor, self.bases_eval),
                        tensap.FunctionalTensor(f_old.tensor, self.bases_eval))
                    if stagnation &lt; self.tolerance[&#39;on_stagnation&#39;] or \
                            np.isnan(stagnation):
                        break
                tree_adapt = False
                new_rank, enriched_nodes, tensor_for_initialization = \
                    self.new_rank_selection(f)
                output[&#39;enriched_nodes_iterations&#39;][iteration] = enriched_nodes
                s_local = self.initial_guess_new_rank(
                    s_local, tensor_for_initialization, new_rank)
            else:
                tree_adapt = True
                enriched_nodes = []
                new_rank = f.tensor.ranks
                s_local.initialization_type = &#39;initial_guess&#39;
                s_local.initial_guess = f.tensor

        if isinstance(self.bases, tensap.FunctionalBases):
            f = tensap.FunctionalTensor(f.tensor, self.bases)

        if self.store_iterates:
            output[&#39;iterates&#39;] = iterates[:iteration+1]

        output[&#39;flag&#39;] = flag
        output[&#39;enriched_nodes_iterations&#39;] = \
            output[&#39;enriched_nodes_iterations&#39;][:iteration+1]
        if &#39;error&#39; in output_local:
            output[&#39;error_iterations&#39;] = errors[:iteration+1]
            output[&#39;error&#39;] = errors[iteration]

        if self.test_error:
            output[&#39;test_error_iterations&#39;] = test_errors[:iteration+1]
            output[&#39;test_error&#39;] = test_errors[iteration]

        if &#39;adapted_tree&#39; in output:
            del output[&#39;adapted_tree&#39;]

        return f, output

    def adapt_tree(self, f, cv_error, test_error, output, *args):
        &#39;&#39;&#39;
        Tree adaptation algorithm.

        Parameters
        ----------
        f : tensap.Tensor
            The current tensor approximation.
        cv_error : float
            The current cross-validation error.
        test_error : float
            The current test error.
        output : dict
            The outputs of the solver.
        args : tuple
            Additional parameters.

        Returns
        -------
        tensap.TensorLearning
            The TensorLearning object.
        f : tensap.Tensor
            The current tensor approximation, perhaps with an adapted tree.
        output : dict
            The outputs of the solver.

        &#39;&#39;&#39;
        return self, f, output

    @abstractmethod
    def initialize(self):
        &#39;&#39;&#39;
        Initialization of the learning algorithm.

        Returns
        -------
        tensap.TensorLearning
            The TensorLearning object.
        f : tensap.FunctionalTensor
            The initialization of the solver.

        &#39;&#39;&#39;

    @abstractmethod
    def pre_processing(self, f):
        &#39;&#39;&#39;
        Initialization of the alternating minimization algorithm.

        Parameters
        ----------
        f : tensap.Tensor
            The current tensor approximation.

        Returns
        -------
        tensap.TensorLearning
            The TensorLearning object.
        f : tensap.Tensor
            The pre-processed tensor approximation.

        &#39;&#39;&#39;

    @abstractmethod
    def randomize_exploration_strategy(self):
        &#39;&#39;&#39;
        Randomization of the exploration strategy.

        Returns
        -------
        alpha_list : numpy.ndarray
            The randomized exploration strategy.

        &#39;&#39;&#39;

    @abstractmethod
    def prepare_alternating_minimization_system(self, f, mu):
        &#39;&#39;&#39;
        Preparation of the alternating minimization algorithm.

        Parameters
        ----------
        f : tensap.FunctionalTensor
            The current approximation.
        mu : int
            The number of the parameter to be optimized.

        Returns
        -------
        tensap.TensorLearning
            The TensorLearning object.
        A : numpy.ndarray
            The matricized partial evaluation of f.
        b : numpy.ndarray
            The target training data.
        f : tensap.FunctionalTensor
            The current approximation.

        &#39;&#39;&#39;

    @abstractmethod
    def set_parameter(self, f, mu, coef):
        &#39;&#39;&#39;
        Update of the parameter mu of the tensor f.

        Parameters
        ----------
        f : tensap.FunctionalTensor
            The current approximation.
        mu : int
            The number of the optimized parameter.
        coef : numpy.ndarray
            The new value of the parameter mu of f.

        Returns
        -------
        f : tensap.FunctionalTensor
            The updated approximation.

        &#39;&#39;&#39;

    @abstractmethod
    def stagnation_criterion(self, f, f0):
        &#39;&#39;&#39;
        Computation of the stagnation criterion.

        Parameters
        ----------
        f : tensap.FunctionalTensor
            The current approximation.
        f0 : tensap.FunctionalTensor
            The previous approximation.

        Returns
        -------
        stagnation : float
            The value of the stagnation criterion.

        &#39;&#39;&#39;

    @abstractmethod
    def final_display(self, f):
        &#39;&#39;&#39;
        Display at the end of the computation.

        Parameters
        ----------
        f : tensap.FunctionalTensor
            The current approximation.

        Returns
        -------
        None.

        &#39;&#39;&#39;

    @abstractmethod
    def local_solver(self):
        &#39;&#39;&#39;
         Extraction of the solver for the adaptive algorithm

        Returns
        -------
        s_local : tensap.TensorLearning
            The local solver.

        &#39;&#39;&#39;

    @abstractmethod
    def new_rank_selection(self, f):
        &#39;&#39;&#39;
        Selection of a new rank in the adaptive algorithm.

        Parameters
        ----------
        f : tensap.FunctionalTensor
            The current approximation.

        Returns
        -------
        new_rank : numpy.ndarray
            The new tensor rank.
        enriched_nodes : numpy.ndarray
            The enriched parameters.
        tensor_for_initialization : tensap.Tensor
            A tensor used for the initialization of the next iteration.

        &#39;&#39;&#39;

    @abstractmethod
    def initial_guess_new_rank(self, s_local, f, new_rank):
        &#39;&#39;&#39;
        Computation of the initial guess with the new selected rank.

        Parameters
        ----------
        s_local : tensap.TensorLearning
            The local solver.
        f : tensap.FunctionalTensor
            The current approximation.
        new_rank : numpy.ndarray
            The new tensor rank.

        Returns
        -------
        s_local : tensap.TensorLearning
            The local solver with the initial guess in s_local.initial_guess.

        &#39;&#39;&#39;

    @abstractmethod
    def adaptation_display(self, f, enriched_nodes):
        &#39;&#39;&#39;
        Display during the adaptation.

        Parameters
        ----------
        f : tensap.FunctionalTensor
            The current approximation.
        enriched_nodes : numpy.ndarray
            The enriched parameters.

        Returns
        -------
        None.

        &#39;&#39;&#39;
        return</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning"><code class="flex name class">
<span>class <span class="ident">TensorLearning</span></span>
<span>(</span><span>loss)</span>
</code></dt>
<dd>
<div class="desc"><p>Class TensorLearning.</p>
<p>See also tensap.Learning.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>order</code></strong> :&ensp;<code>int</code></dt>
<dd>Order of the tensor.</dd>
<dt><strong><code>bases</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>The functional bases used for by the approximation.</dd>
<dt><strong><code>bases_eval</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>The evaluations of the bases on the training data.</dd>
<dt><strong><code>bases_eval_test</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>The evaluations of the bases on the test data.</dd>
<dt><strong><code>algorithm</code></strong> :&ensp;<code>str</code></dt>
<dd>The choice of algorithm.</dd>
<dt><strong><code>initialization_type</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of initialization.</dd>
<dt><strong><code>initial_guess</code></strong> :&ensp;<code>tensap.Tensor</code></dt>
<dd>An initial guess for the solver.</dd>
<dt><strong><code>tree_adaptation</code></strong> :&ensp;<code>bool</code></dt>
<dd>Boolean indicating if tree adaptation is to be performed.</dd>
<dt><strong><code>tree_adaptation_options</code></strong> :&ensp;<code>dict</code></dt>
<dd>Options for the tree adaptation.</dd>
<dt><strong><code>rank_adaptation</code></strong> :&ensp;<code>bool</code></dt>
<dd>Boolean indicating if rank adaptation is to be performed.</dd>
<dt><strong><code>rank_adaptation_options</code></strong> :&ensp;<code>dict</code></dt>
<dd>Options for the rank adaptation.</dd>
<dt><strong><code>tolerance</code></strong> :&ensp;<code>dict</code></dt>
<dd>Tolerances for the solver.</dd>
<dt><strong><code>linear_model_learning_parameters</code></strong> :&ensp;<code>dict</code></dt>
<dd>Parameters for the linear solvers.</dd>
<dt><strong><code>alternating_minimization_parameters</code></strong> :&ensp;<code>dict</code></dt>
<dd>Parameters for the alternating minimization.</dd>
<dt><strong><code>bases_adaptation_path</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>Paths for each basis for the basis adaptation.</dd>
<dt><strong><code>store_iterates</code></strong> :&ensp;<code>bool</code></dt>
<dd>Boolean indicating if the iterates are to be stored.</dd>
<dt><strong><code>rank</code></strong> :&ensp;<code>int</code> or <code>list</code> or <code>numpy.ndarray</code></dt>
<dd>The rank of the expected approximation.</dd>
<dt><strong><code>output_dimension</code></strong> :&ensp;<code>int</code></dt>
<dd>The dimension of the outputs of the function to be approximated.</dd>
<dt><strong><code>_number_of_parameters</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of parameters of the approximation.</dd>
<dt><strong><code>_exploration_strategy</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The exploration strategy.</dd>
<dt><strong><code>_warnings</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionnary containing the state of some warnings to be displayed.</dd>
</dl>
<p>Constructor for the class TensorLearning.</p>
<p>See also tensap.Learning.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>loss</code></strong> :&ensp;<code>tensap.LossFunction</code></dt>
<dd>The loss function associated with the solver.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TensorLearning(tensap.Learning):
    &#39;&#39;&#39;
    Class TensorLearning.

    See also tensap.Learning.

    Attributes
    ----------
    order : int
        Order of the tensor.
    bases : list or numpy.ndarray
        The functional bases used for by the approximation.
    bases_eval : list or numpy.ndarray
        The evaluations of the bases on the training data.
    bases_eval_test : list or numpy.ndarray
        The evaluations of the bases on the test data.
    algorithm : str
        The choice of algorithm.
    initialization_type : str
        The type of initialization.
    initial_guess : tensap.Tensor
        An initial guess for the solver.
    tree_adaptation : bool
        Boolean indicating if tree adaptation is to be performed.
    tree_adaptation_options : dict
        Options for the tree adaptation.
    rank_adaptation : bool
        Boolean indicating if rank adaptation is to be performed.
    rank_adaptation_options : dict
        Options for the rank adaptation.
    tolerance : dict
        Tolerances for the solver.
    linear_model_learning_parameters : dict
        Parameters for the linear solvers.
    alternating_minimization_parameters : dict
        Parameters for the alternating minimization.
    bases_adaptation_path : list or numpy.ndarray
        Paths for each basis for the basis adaptation.
    store_iterates : bool
        Boolean indicating if the iterates are to be stored.
    rank : int or list or numpy.ndarray
        The rank of the expected approximation.
    output_dimension : int
        The dimension of the outputs of the function to be approximated.
    _number_of_parameters : int
        The number of parameters of the approximation.
    _exploration_strategy : numpy.ndarray
        The exploration strategy.
    _warnings : dict
        Dictionnary containing the state of some warnings to be displayed.

    &#39;&#39;&#39;

    def __init__(self, loss):
        &#39;&#39;&#39;
        Constructor for the class TensorLearning.

        See also tensap.Learning.

        Parameters
        ----------
        loss : tensap.LossFunction
            The loss function associated with the solver.

        Returns
        -------
        None.

        &#39;&#39;&#39;
        super().__init__(loss)
        self.linear_model_learning = tensap.Learning.linear_model(loss)
        self.order = None
        self.bases = None
        self.bases_eval = None
        self.bases_eval_test = None
        self.algorithm = &#39;standard&#39;
        self.initialization_type = None
        self.initial_guess = None
        self.tree_adaptation = False
        self.tree_adaptation_options = {&#39;tolerance&#39;: None,
                                        &#39;max_iterations&#39;: 100,
                                        &#39;force_rank_adaptation&#39;: True}
        self.rank_adaptation = False
        self.rank_adaptation_options = {&#39;max_iterations&#39;: 10,
                                        &#39;early_stopping&#39;: False,
                                        &#39;early_stopping_factor&#39;: 10}
        self.tolerance = {&#39;on_error&#39;: 1e-6, &#39;on_stagnation&#39;: 1e-6}
        self.linear_model_learning_parameters = \
            {&#39;identical_for_all_parameters&#39;: True}
        self.alternating_minimization_parameters = {&#39;display&#39;: False,
                                                    &#39;max_iterations&#39;: 30,
                                                    &#39;stagnation&#39;: 1e-6,
                                                    &#39;random&#39;: False}
        self.bases_adaptation_path = None
        self.store_iterates = True
        self.rank = 1
        self.output_dimension = 1

        self.model_selection = False
        # Possible choices: &#39;test_error&#39;, &#39;cv_error&#39;
        self.model_selection_options[&#39;type&#39;] = &#39;test_error&#39;

        self._number_of_parameters = None
        self._exploration_strategy = None
        self._warnings = {&#39;orthonormality_warning_display&#39;: True,
                          &#39;empty_bases_warning_display&#39;: True}

    def __repr__(self):
        return pprint.pformat(self.__dict__, indent=4, width=1)

    def solve(self, *args):
        &#39;&#39;&#39;
        Solver for the learning problem with tensor formats.

        Parameters
        ----------
        *args : misc
            Additional arguments, if needed.

        Raises
        ------
        NotImplementedError
            If the required solver is not implemented.
        ValueError
            If the required data is not provided.

        Returns
        -------
        f : tensap.FunctionalTensor
            The learned approximation.
        output : dict
            The outputs of the solver.

        &#39;&#39;&#39;
        # If possible, deduce from training_data the output dimension
        if isinstance(self.training_data, list) and \
                len(self.training_data) == 2 and \
                np.ndim(self.training_data[1]) == 2:
            self.output_dimension = self.training_data[1].shape[1]
        if self.output_dimension &gt; 1 and \
            (not isinstance(self, tensap.TreeBasedTensorLearning) or
             not isinstance(self.loss_function, (tensap.SquareLossFunction,
                                                 tensap.CustomLossFunction))):
            raise NotImplementedError(
                &#39;Solver not implemented for vector-valued functions &#39; +
                &#39;approximation, use TreeBasedTensorLearning with a &#39; +
                &#39;SquareLossFunction or a CustomLossFunction instead.&#39;)

        if self.output_dimension &gt; 1 and \
            &#39;type&#39; in self.rank_adaptation_options and \
                self.rank_adaptation_options[&#39;type&#39;] == &#39;inner&#39;:
            print(&#39;Inner rank adaptation not implemented for &#39; +
                  &#39;output_dimension greater than 1, disabling it.&#39;)
            del self.rank_adaptation_options[&#39;type&#39;]

        if self._warnings[&#39;orthonormality_warning_display&#39;] and \
            (self.bases is None or (self.bases is not None and not
                                    np.all([x.is_orthonormal for x in
                                            self.bases.bases]))):
            self._warnings[&#39;orthonormality_warning_display&#39;] = False
            print(&#39;The implemented learning algorithms are designed &#39; +
                  &#39;for orthonormal bases. These algorithms work with &#39; +
                  &#39;non-orthonormal bases, but without some guarantees &#39; +
                  &#39;on their results.&#39;)

        # If no bases are provided, warn that the returned functions are
        # evaluated on the training data
        if self.bases is None and \
                self._warnings[&#39;empty_bases_warning_display&#39;]:
            self._warnings[&#39;empty_bases_warning_display&#39;] = False
            print(&#39;The returned functions are evaluated on the training &#39; +
                  &#39;data. To evaluate them at other points, assign to the &#39; +
                  &#39;FunctionalTensor a nonempty field bases and set the &#39; +
                  &#39;attribute evaluatedBases to False.&#39;)

        # If the test error cannot be computed, it is disabled
        if self.test_error and \
            not isinstance(self.bases, tensap.FunctionalBases) and \
                self.bases_eval_test is None:
            print(&#39;The test error cannot be computed.&#39;)
            self.test_error = False

        # Assert if basis adaptation can be performed
        if self.bases_adaptation_path is None and \
                not hasattr(self.bases, &#39;adaptation_path&#39;):
            if isinstance(self.linear_model_learning, (list, np.ndarray)) and \
                    np.any([x.basis_adaptation for
                            x in self.linear_model_learning]):
                print(&#39;Cannot perform basis adaptation, disabling it.&#39;)
                for linear_solver in self.linear_model_learning:
                    setattr(linear_solver, &#39;basis_adaptation&#39;, False)

            elif not isinstance(self.linear_model_learning,
                                (list, np.ndarray)) and \
                    self.linear_model_learning.basis_adaptation:
                print(&#39;Cannot perform basis adaptation, disabling it.&#39;)
                self.linear_model_learning.basis_adaptation = False

        # Bases evaluation
        if hasattr(self.bases, &#39;eval&#39;):
            if self.training_data is not None and self.bases_eval is None:
                if isinstance(self.training_data, list) and \
                        self.training_data[0] is not None:
                    self.bases_eval = self.bases.eval(self.training_data[0])
                elif not isinstance(self.training_data, list) and \
                        self.training_data is not None:
                    self.bases_eval = self.bases.eval(self.training_data)
                else:
                    raise ValueError(&#39;Must provide input training data.&#39;)

            if self.test_error and self.test_data is not None and \
                    self.bases_eval_test is None:
                if isinstance(self.test_data, list) and \
                        self.test_data[0] is not None:
                    self.bases_eval_test = self.bases.eval(self.test_data[0])
                elif not isinstance(self.test_data, list) and \
                        self.test_data is not None:
                    self.bases_eval_test = self.bases.eval(self.test_data)
                else:
                    raise ValueError(&#39;Must provide input test data.&#39;)

        if self.model_selection:
            self.store_iterates = True
            if self.model_selection_options[&#39;type&#39;] == &#39;test_error&#39; and \
                    not self.test_error:
                print(&#39;Cannot perform test error based model selection, &#39; +
                      &#39;disabling it.&#39;)
                self.model_selection = False
            elif self.model_selection_options[&#39;type&#39;] == &#39;cv_error&#39; and \
                    not self.error_estimation:
                self.error_estimation = True

        if self.rank_adaptation:
            if &#39;type&#39; not in self.rank_adaptation_options or \
                    self.rank_adaptation_options[&#39;type&#39;] == &#39;standard&#39;:
                fun, output = self._solve_adaptation()
            elif isinstance(self.rank_adaptation_options[&#39;type&#39;], str):
                adapt_type = self.rank_adaptation_options[&#39;type&#39;]
                if adapt_type == &#39;dmrg&#39; or adapt_type == &#39;dmrg_low_rank&#39;:
                    adapt_type = &#39;dmrg&#39;

                # Call the method corresponding to the rank adaptation option
                fun, output = eval(&#39;self._solve_&#39; + adapt_type.lower() +
                                   &#39;_rank_adaptation()&#39;)
            else:
                raise ValueError(&#39;The rank_adaptation_options attribute &#39; +
                                 &#39;must be either inexistant or a string.&#39;)
        elif self.algorithm == &#39;standard&#39;:
            fun, output = self._solve_standard()
        else:
            fun, output = eval(&#39;self._solve_&#39; + self.algorithm.lower() + &#39;()&#39;)

        if self.model_selection:
            if self.model_selection_options[&#39;type&#39;] == &#39;test_error&#39;:
                ind = np.argmin(output[&#39;test_error_iterations&#39;])
                fun = output[&#39;iterates&#39;][ind]
                output[&#39;selected_model_number&#39;] = ind
                if &#39;error_iterations&#39; in output and \
                    np.size(output[&#39;error_iterations&#39;]) &gt; 0:
                    output[&#39;error&#39;] = output[&#39;error_iterations&#39;][ind]
                output[&#39;test_error&#39;] = output[&#39;test_error_iterations&#39;][ind]
                if self.display:
                    print(&#39;\nModel selection using the test error: model &#39; +
                          &#39;#%i selected&#39; % ind)
            elif self.model_selection_options[&#39;type&#39;] == &#39;cv_error&#39;:
                ind = np.argmin(output[&#39;error_iterations&#39;])
                fun = output[&#39;iterates&#39;][ind]
                output[&#39;selected_model_number&#39;] = ind
                if &#39;test_error_iterations&#39; in output and \
                    np.size(output[&#39;test_error_iterations&#39;]) &gt; 0:
                    output[&#39;test_error&#39;] = output[&#39;test_error_iterations&#39;][ind]
                output[&#39;error&#39;] = output[&#39;error_iterations&#39;][ind]
                if self.display:
                    print(&#39;\nModel selection using the cross-validation &#39; +
                          &#39; error: model #%i selected&#39; % ind)
            else:
                print(&#39;Wrong model selection type, returning the last &#39; +
                      &#39;iterate.&#39;)

            if self.display:
                if self.alternating_minimization_parameters[&#39;display&#39;]:
                    print(&#39;&#39;)
                self.final_display(fun)
                if &#39;error&#39; in output:
                    print(&#39;, error = %2.5e&#39; % output[&#39;error&#39;], end=&#39;&#39;)
                if &#39;test_error&#39; in output:
                    print(&#39;, test error = %2.5e&#39; % output[&#39;test_error&#39;],
                          end=&#39;&#39;)
                print(&#39;&#39;)

        return fun, output

    def _solve_standard(self):
        &#39;&#39;&#39;
        Solver for the learning problem with tensor formats using the standard
        algorithm (without adaptation).

        Raises
        ------
        ValueError
            If the number of LinearModelLearning objects is not equal to
            _numberOfParameters.

        Returns
        -------
        f : tensap.FunctionalTensor
            The learned approximation.
        output : dict
            The outputs of the solver.

        &#39;&#39;&#39;
        output = {&#39;flag&#39;: 0}

        self, f = self.initialize()
        f = tensap.FunctionalTensor(f, self.bases_eval)

        # Replication of the LinearModelLearning objects
        if self.linear_model_learning_parameters[
                &#39;identical_for_all_parameters&#39;] and \
                not isinstance(self.linear_model_learning, (list, np.ndarray)):
            self.linear_model_learning = list(map(deepcopy, [
                self.linear_model_learning] * self._number_of_parameters))
        elif isinstance(self.linear_model_learning, (list, np.ndarray)) and \
                len(self.linear_model_learning) != self._number_of_parameters:
            raise ValueError(&#39;Must provide self._numberOfParameters &#39; +
                             &#39;LinearModelLearning objects.&#39;)

        if self.error_estimation:
            for x in self.linear_model_learning:
                setattr(x, &#39;error_estimation&#39;, True)

        # Working set paths
        if isinstance(self.linear_model_learning, (list, np.ndarray)) and \
            np.any([x.basis_adaptation for
                    x in self.linear_model_learning]) and \
                self.bases_adaptation_path is None:
            self.bases_adaptation_path = self.bases.adaptation_path()

        if self.alternating_minimization_parameters[&#39;max_iterations&#39;] == 0:
            return f, output

        output[&#39;stagnation_criterion&#39;] = []
        output[&#39;iterates&#39;] = []
        output[&#39;error_iterations&#39;] = []
        output[&#39;test_error_iterations&#39;] = []

        # Alternating minimization loop
        for iteration in range(self.alternating_minimization_parameters
                               [&#39;max_iterations&#39;]):
            self, f = self.pre_processing(f)
            f0 = deepcopy(f)

            if self.alternating_minimization_parameters[&#39;random&#39;]:
                # Randomize the exploration strategy
                alpha_list = self.randomize_exploration_strategy()
            else:
                alpha_list = self._exploration_strategy

            for alpha in alpha_list:
                self, A, b, f = \
                    self.prepare_alternating_minimization_system(f, alpha)
                self.linear_model_learning[alpha-1].training_data = [None, b]
                self.linear_model_learning[alpha-1].basis = None
                self.linear_model_learning[alpha-1].basis_eval = A

                coef, output_tmp = self.linear_model_learning[alpha-1].solve()
                if coef is None or np.count_nonzero(coef) == 0 or \
                        not np.all(np.isfinite(coef)):
                    print(&#39;Empty, zero or NaN solution, returning to the &#39; +
                          &#39;previous iteration.&#39;)
                    output[&#39;flag&#39;] = -2
                    output[&#39;error&#39;] = np.inf
                    break

                f = self.set_parameter(f, alpha, coef)

            stagnation = self.stagnation_criterion(f, f0)
            output[&#39;stagnation_criterion&#39;].append(stagnation)

            if self.store_iterates:
                if isinstance(self.bases, tensap.FunctionalBases):
                    output[&#39;iterates&#39;].append(tensap.FunctionalTensor(
                        f.tensor, self.bases))
                else:
                    output[&#39;iterates&#39;].append(f)

            if &#39;error&#39; in output_tmp:
                output[&#39;error&#39;] = output_tmp[&#39;error&#39;]
                output[&#39;error_iterations&#39;].append(output[&#39;error&#39;])

            if self.test_error:
                f_eval_test = tensap.FunctionalTensor(f, self.bases_eval_test)
                output[&#39;test_error&#39;] = self.loss_function.test_error(
                    f_eval_test, self.test_data)
                output[&#39;test_error_iterations&#39;].append(output[&#39;test_error&#39;])

            if self.alternating_minimization_parameters[&#39;display&#39;]:
                print(&#39;\tAlt. min. iteration %s: stagnation = %2.5e&#39; %
                      (str(iteration).
                       zfill(len(str(self.alternating_minimization_parameters
                                     [&#39;max_iterations&#39;]-1))),
                       stagnation), end=&#39;&#39;)
                if &#39;error&#39; in output:
                    print(&#39;, error = %2.5e&#39; % output[&#39;error&#39;], end=&#39;&#39;)
                if self.test_error:
                    if not np.isscalar(output[&#39;test_error&#39;]):
                        output[&#39;test_error&#39;] = output[&#39;test_error&#39;].numpy()
                    print(&#39;, test error = %2.5e&#39; % output[&#39;test_error&#39;],
                          end=&#39;&#39;)
                print(&#39;&#39;)

            if iteration &gt; 0 and stagnation &lt; \
                    self.alternating_minimization_parameters[&#39;stagnation&#39;]:
                output[&#39;flag&#39;] = 1
                break

            if self.test_error and \
                    output[&#39;test_error&#39;] &lt; self.tolerance[&#39;on_error&#39;]:
                output[&#39;flag&#39;] = 1
                break

        if isinstance(self.bases, tensap.FunctionalBases):
            f = tensap.FunctionalTensor(f.tensor, self.bases)
        output[&#39;iter&#39;] = iteration

        if self.display and not self.model_selection:
            if self.alternating_minimization_parameters[&#39;display&#39;]:
                print(&#39;&#39;)
            self.final_display(f)
            if &#39;error&#39; in output:
                print(&#39;, error = %2.5e&#39; % output[&#39;error&#39;], end=&#39;&#39;)
            if &#39;test_error&#39; in output:
                print(&#39;, test error = %2.5e&#39; % output[&#39;test_error&#39;], end=&#39;&#39;)
            print(&#39;&#39;)

        return f, output

    def _solve_adaptation(self):
        &#39;&#39;&#39;
        Solver for the learning problem with tensor formats using the adaptive
        algorithm.

        Returns
        -------
        f : tensap.FunctionalTensor
            The learned approximation.
        output : dict
            The outputs of the solver.

        &#39;&#39;&#39;

        flag = 0
        output = {&#39;enriched_nodes_iterations&#39;:
                  np.empty(self.rank_adaptation_options[&#39;max_iterations&#39;],
                           dtype=object)}
        tree_adapt = False

        f = None
        errors = np.zeros(self.rank_adaptation_options[&#39;max_iterations&#39;])
        test_errors = np.zeros(self.rank_adaptation_options[&#39;max_iterations&#39;])
        iterates = np.empty(self.rank_adaptation_options[&#39;max_iterations&#39;],
                            dtype=object)

        # new_rank = s_local.rank
        new_rank = self.rank
        s_local = self.local_solver()
        s_local.model_selection = False

        enriched_nodes = np.array([])
        for iteration in range(self.rank_adaptation_options[&#39;max_iterations&#39;]):
            s_local.bases = self.bases
            s_local.bases_eval = self.bases_eval
            s_local.bases_eval_test = self.bases_eval_test
            s_local.training_data = self.training_data
            s_local.test_data = self.test_data
            s_local.rank = new_rank

            f_old = deepcopy(f)
            f, output_local = s_local.solve()
            s_local = self.local_solver()

            if &#39;error&#39; in output_local:
                errors[iteration] = output_local[&#39;error&#39;]
                if np.isinf(errors[iteration]):
                    print(&#39;Infinite error, returning to the previous iterate.&#39;)
                    f = f_old
                    iteration -= 1
                    flag = -2
                    break

            if self.test_error:
                f_eval_test = tensap.FunctionalTensor(
                    f, self.bases_eval_test)
                test_errors[iteration] = self.loss_function.test_error(
                    f_eval_test, self.test_data)

            if self.store_iterates:
                if isinstance(self.bases, tensap.FunctionalBases):
                    iterates[iteration] = tensap.FunctionalTensor(f.tensor,
                                                                  self.bases)
                else:
                    iterates[iteration] = f

            if self.display:
                if self.alternating_minimization_parameters[&#39;display&#39;]:
                    print(&#39;&#39;)
                print(&#39;\nRank adaptation, iteration %i:&#39; % (iteration))
                self.adaptation_display(f, enriched_nodes)
                print(&#39;\tStorage complexity = %i&#39; % f.tensor.storage())

                if errors[iteration] != 0:
                    print(&#39;\tError      = %2.5e&#39; % errors[iteration])
                if test_errors[iteration] != 0:
                    print(&#39;\tTest error = %2.5e&#39; % test_errors[iteration])
                if self.alternating_minimization_parameters[&#39;display&#39;]:
                    print(&#39;&#39;)

            if iteration == self.rank_adaptation_options[&#39;max_iterations&#39;]-1:
                break

            if (self.test_error and test_errors[iteration] &lt;
                    self.tolerance[&#39;on_error&#39;]) or \
                (&#39;error&#39; in output_local and errors[iteration] &lt;
                 self.tolerance[&#39;on_error&#39;]):
                flag = 1
                break

            fac = self.rank_adaptation_options[&#39;early_stopping_factor&#39;]
            cond = iteration &gt; 0 and (self.test_error and
                                      (np.isnan(test_errors[iteration]) or
                                       fac * np.min(test_errors[:iteration]) &lt;
                                       test_errors[iteration]) or
                                      (&#39;error&#39; in output_local and
                                       (np.isnan(errors[iteration]) or
                                        fac * np.min(errors[:iteration]) &lt;
                                        errors[iteration])))
            if self.rank_adaptation_options[&#39;early_stopping&#39;] and cond:
                print(&#39;Early stopping&#39;, end=&#39;&#39;)
                if &#39;error&#39; in output_local:
                    print(&#39;, error = %2.5e&#39; % errors[iteration], end=&#39;&#39;)
                if self.test_error:
                    print(&#39;, test error = %2.5e&#39; % test_errors[iteration],
                          end=&#39;&#39;)
                print(&#39;\n&#39;)
                iteration -= 1
                f = f_old
                flag = -1
                break

            adapted_tree = False
            if s_local.tree_adaptation and iteration &gt; 0 and \
                    (not self.tree_adaptation_options[&#39;force_rank_adaptation&#39;]
                     or not tree_adapt):
                C_old = f.tensor.storage()
                self, f, output = self.adapt_tree(f, errors[iteration],
                                                  None, output, iteration)
                adapted_tree = output[&#39;adapted_tree&#39;]
                if adapted_tree:
                    if self.display:
                        print(&#39;\t\tStorage complexity before permutation &#39; +
                              &#39;= %i&#39; % C_old)
                        print(&#39;\t\tStorage complexity after permutation &#39; +
                              &#39;= %i&#39; % f.tensor.storage())
                    if self.test_error:
                        f_eval_test = tensap.FunctionalTensor(
                            f, self.bases_eval_test)
                        test_errors[iteration] = self.loss_function.test_error(
                            f_eval_test, self.test_data)
                        if self.display:
                            print(&#39;\t\tTest error after permutation &#39; +
                                  &#39;= %2.5e&#39; % test_errors[iteration])

                    if self.alternating_minimization_parameters[&#39;display&#39;]:
                        print(&#39;&#39;)

            if not self.tree_adaptation or not adapted_tree:
                if iteration &gt; 0 and not tree_adapt:
                    stagnation = self.stagnation_criterion(
                        tensap.FunctionalTensor(f.tensor, self.bases_eval),
                        tensap.FunctionalTensor(f_old.tensor, self.bases_eval))
                    if stagnation &lt; self.tolerance[&#39;on_stagnation&#39;] or \
                            np.isnan(stagnation):
                        break
                tree_adapt = False
                new_rank, enriched_nodes, tensor_for_initialization = \
                    self.new_rank_selection(f)
                output[&#39;enriched_nodes_iterations&#39;][iteration] = enriched_nodes
                s_local = self.initial_guess_new_rank(
                    s_local, tensor_for_initialization, new_rank)
            else:
                tree_adapt = True
                enriched_nodes = []
                new_rank = f.tensor.ranks
                s_local.initialization_type = &#39;initial_guess&#39;
                s_local.initial_guess = f.tensor

        if isinstance(self.bases, tensap.FunctionalBases):
            f = tensap.FunctionalTensor(f.tensor, self.bases)

        if self.store_iterates:
            output[&#39;iterates&#39;] = iterates[:iteration+1]

        output[&#39;flag&#39;] = flag
        output[&#39;enriched_nodes_iterations&#39;] = \
            output[&#39;enriched_nodes_iterations&#39;][:iteration+1]
        if &#39;error&#39; in output_local:
            output[&#39;error_iterations&#39;] = errors[:iteration+1]
            output[&#39;error&#39;] = errors[iteration]

        if self.test_error:
            output[&#39;test_error_iterations&#39;] = test_errors[:iteration+1]
            output[&#39;test_error&#39;] = test_errors[iteration]

        if &#39;adapted_tree&#39; in output:
            del output[&#39;adapted_tree&#39;]

        return f, output

    def adapt_tree(self, f, cv_error, test_error, output, *args):
        &#39;&#39;&#39;
        Tree adaptation algorithm.

        Parameters
        ----------
        f : tensap.Tensor
            The current tensor approximation.
        cv_error : float
            The current cross-validation error.
        test_error : float
            The current test error.
        output : dict
            The outputs of the solver.
        args : tuple
            Additional parameters.

        Returns
        -------
        tensap.TensorLearning
            The TensorLearning object.
        f : tensap.Tensor
            The current tensor approximation, perhaps with an adapted tree.
        output : dict
            The outputs of the solver.

        &#39;&#39;&#39;
        return self, f, output

    @abstractmethod
    def initialize(self):
        &#39;&#39;&#39;
        Initialization of the learning algorithm.

        Returns
        -------
        tensap.TensorLearning
            The TensorLearning object.
        f : tensap.FunctionalTensor
            The initialization of the solver.

        &#39;&#39;&#39;

    @abstractmethod
    def pre_processing(self, f):
        &#39;&#39;&#39;
        Initialization of the alternating minimization algorithm.

        Parameters
        ----------
        f : tensap.Tensor
            The current tensor approximation.

        Returns
        -------
        tensap.TensorLearning
            The TensorLearning object.
        f : tensap.Tensor
            The pre-processed tensor approximation.

        &#39;&#39;&#39;

    @abstractmethod
    def randomize_exploration_strategy(self):
        &#39;&#39;&#39;
        Randomization of the exploration strategy.

        Returns
        -------
        alpha_list : numpy.ndarray
            The randomized exploration strategy.

        &#39;&#39;&#39;

    @abstractmethod
    def prepare_alternating_minimization_system(self, f, mu):
        &#39;&#39;&#39;
        Preparation of the alternating minimization algorithm.

        Parameters
        ----------
        f : tensap.FunctionalTensor
            The current approximation.
        mu : int
            The number of the parameter to be optimized.

        Returns
        -------
        tensap.TensorLearning
            The TensorLearning object.
        A : numpy.ndarray
            The matricized partial evaluation of f.
        b : numpy.ndarray
            The target training data.
        f : tensap.FunctionalTensor
            The current approximation.

        &#39;&#39;&#39;

    @abstractmethod
    def set_parameter(self, f, mu, coef):
        &#39;&#39;&#39;
        Update of the parameter mu of the tensor f.

        Parameters
        ----------
        f : tensap.FunctionalTensor
            The current approximation.
        mu : int
            The number of the optimized parameter.
        coef : numpy.ndarray
            The new value of the parameter mu of f.

        Returns
        -------
        f : tensap.FunctionalTensor
            The updated approximation.

        &#39;&#39;&#39;

    @abstractmethod
    def stagnation_criterion(self, f, f0):
        &#39;&#39;&#39;
        Computation of the stagnation criterion.

        Parameters
        ----------
        f : tensap.FunctionalTensor
            The current approximation.
        f0 : tensap.FunctionalTensor
            The previous approximation.

        Returns
        -------
        stagnation : float
            The value of the stagnation criterion.

        &#39;&#39;&#39;

    @abstractmethod
    def final_display(self, f):
        &#39;&#39;&#39;
        Display at the end of the computation.

        Parameters
        ----------
        f : tensap.FunctionalTensor
            The current approximation.

        Returns
        -------
        None.

        &#39;&#39;&#39;

    @abstractmethod
    def local_solver(self):
        &#39;&#39;&#39;
         Extraction of the solver for the adaptive algorithm

        Returns
        -------
        s_local : tensap.TensorLearning
            The local solver.

        &#39;&#39;&#39;

    @abstractmethod
    def new_rank_selection(self, f):
        &#39;&#39;&#39;
        Selection of a new rank in the adaptive algorithm.

        Parameters
        ----------
        f : tensap.FunctionalTensor
            The current approximation.

        Returns
        -------
        new_rank : numpy.ndarray
            The new tensor rank.
        enriched_nodes : numpy.ndarray
            The enriched parameters.
        tensor_for_initialization : tensap.Tensor
            A tensor used for the initialization of the next iteration.

        &#39;&#39;&#39;

    @abstractmethod
    def initial_guess_new_rank(self, s_local, f, new_rank):
        &#39;&#39;&#39;
        Computation of the initial guess with the new selected rank.

        Parameters
        ----------
        s_local : tensap.TensorLearning
            The local solver.
        f : tensap.FunctionalTensor
            The current approximation.
        new_rank : numpy.ndarray
            The new tensor rank.

        Returns
        -------
        s_local : tensap.TensorLearning
            The local solver with the initial guess in s_local.initial_guess.

        &#39;&#39;&#39;

    @abstractmethod
    def adaptation_display(self, f, enriched_nodes):
        &#39;&#39;&#39;
        Display during the adaptation.

        Parameters
        ----------
        f : tensap.FunctionalTensor
            The current approximation.
        enriched_nodes : numpy.ndarray
            The enriched parameters.

        Returns
        -------
        None.

        &#39;&#39;&#39;
        return</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="tensap.approximation.learning.learning.Learning" href="../../learning/learning.html#tensap.approximation.learning.learning.Learning">Learning</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="tensap.approximation.tensor_approximation.tensor_learning.canonical_tensor_learning.CanonicalTensorLearning" href="canonical_tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.canonical_tensor_learning.CanonicalTensorLearning">CanonicalTensorLearning</a></li>
<li><a title="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning" href="tree_based_tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning">TreeBasedTensorLearning</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.adapt_tree"><code class="name flex">
<span>def <span class="ident">adapt_tree</span></span>(<span>self, f, cv_error, test_error, output, *args)</span>
</code></dt>
<dd>
<div class="desc"><p>Tree adaptation algorithm.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.Tensor</code></dt>
<dd>The current tensor approximation.</dd>
<dt><strong><code>cv_error</code></strong> :&ensp;<code>float</code></dt>
<dd>The current cross-validation error.</dd>
<dt><strong><code>test_error</code></strong> :&ensp;<code>float</code></dt>
<dd>The current test error.</dd>
<dt><strong><code>output</code></strong> :&ensp;<code>dict</code></dt>
<dd>The outputs of the solver.</dd>
<dt><strong><code>args</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Additional parameters.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tensap.TensorLearning</code></dt>
<dd>The TensorLearning object.</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.Tensor</code></dt>
<dd>The current tensor approximation, perhaps with an adapted tree.</dd>
<dt><strong><code>output</code></strong> :&ensp;<code>dict</code></dt>
<dd>The outputs of the solver.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adapt_tree(self, f, cv_error, test_error, output, *args):
    &#39;&#39;&#39;
    Tree adaptation algorithm.

    Parameters
    ----------
    f : tensap.Tensor
        The current tensor approximation.
    cv_error : float
        The current cross-validation error.
    test_error : float
        The current test error.
    output : dict
        The outputs of the solver.
    args : tuple
        Additional parameters.

    Returns
    -------
    tensap.TensorLearning
        The TensorLearning object.
    f : tensap.Tensor
        The current tensor approximation, perhaps with an adapted tree.
    output : dict
        The outputs of the solver.

    &#39;&#39;&#39;
    return self, f, output</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.adaptation_display"><code class="name flex">
<span>def <span class="ident">adaptation_display</span></span>(<span>self, f, enriched_nodes)</span>
</code></dt>
<dd>
<div class="desc"><p>Display during the adaptation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.FunctionalTensor</code></dt>
<dd>The current approximation.</dd>
<dt><strong><code>enriched_nodes</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The enriched parameters.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def adaptation_display(self, f, enriched_nodes):
    &#39;&#39;&#39;
    Display during the adaptation.

    Parameters
    ----------
    f : tensap.FunctionalTensor
        The current approximation.
    enriched_nodes : numpy.ndarray
        The enriched parameters.

    Returns
    -------
    None.

    &#39;&#39;&#39;
    return</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.final_display"><code class="name flex">
<span>def <span class="ident">final_display</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"><p>Display at the end of the computation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.FunctionalTensor</code></dt>
<dd>The current approximation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def final_display(self, f):
    &#39;&#39;&#39;
    Display at the end of the computation.

    Parameters
    ----------
    f : tensap.FunctionalTensor
        The current approximation.

    Returns
    -------
    None.

    &#39;&#39;&#39;</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.initial_guess_new_rank"><code class="name flex">
<span>def <span class="ident">initial_guess_new_rank</span></span>(<span>self, s_local, f, new_rank)</span>
</code></dt>
<dd>
<div class="desc"><p>Computation of the initial guess with the new selected rank.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>s_local</code></strong> :&ensp;<code>tensap.TensorLearning</code></dt>
<dd>The local solver.</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.FunctionalTensor</code></dt>
<dd>The current approximation.</dd>
<dt><strong><code>new_rank</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The new tensor rank.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>s_local</code></strong> :&ensp;<code>tensap.TensorLearning</code></dt>
<dd>The local solver with the initial guess in s_local.initial_guess.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def initial_guess_new_rank(self, s_local, f, new_rank):
    &#39;&#39;&#39;
    Computation of the initial guess with the new selected rank.

    Parameters
    ----------
    s_local : tensap.TensorLearning
        The local solver.
    f : tensap.FunctionalTensor
        The current approximation.
    new_rank : numpy.ndarray
        The new tensor rank.

    Returns
    -------
    s_local : tensap.TensorLearning
        The local solver with the initial guess in s_local.initial_guess.

    &#39;&#39;&#39;</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialization of the learning algorithm.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tensap.TensorLearning</code></dt>
<dd>The TensorLearning object.</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.FunctionalTensor</code></dt>
<dd>The initialization of the solver.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def initialize(self):
    &#39;&#39;&#39;
    Initialization of the learning algorithm.

    Returns
    -------
    tensap.TensorLearning
        The TensorLearning object.
    f : tensap.FunctionalTensor
        The initialization of the solver.

    &#39;&#39;&#39;</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.local_solver"><code class="name flex">
<span>def <span class="ident">local_solver</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Extraction of the solver for the adaptive algorithm</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>s_local</code></strong> :&ensp;<code>tensap.TensorLearning</code></dt>
<dd>The local solver.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def local_solver(self):
    &#39;&#39;&#39;
     Extraction of the solver for the adaptive algorithm

    Returns
    -------
    s_local : tensap.TensorLearning
        The local solver.

    &#39;&#39;&#39;</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.new_rank_selection"><code class="name flex">
<span>def <span class="ident">new_rank_selection</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"><p>Selection of a new rank in the adaptive algorithm.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.FunctionalTensor</code></dt>
<dd>The current approximation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>new_rank</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The new tensor rank.</dd>
<dt><strong><code>enriched_nodes</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The enriched parameters.</dd>
<dt><strong><code>tensor_for_initialization</code></strong> :&ensp;<code>tensap.Tensor</code></dt>
<dd>A tensor used for the initialization of the next iteration.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def new_rank_selection(self, f):
    &#39;&#39;&#39;
    Selection of a new rank in the adaptive algorithm.

    Parameters
    ----------
    f : tensap.FunctionalTensor
        The current approximation.

    Returns
    -------
    new_rank : numpy.ndarray
        The new tensor rank.
    enriched_nodes : numpy.ndarray
        The enriched parameters.
    tensor_for_initialization : tensap.Tensor
        A tensor used for the initialization of the next iteration.

    &#39;&#39;&#39;</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.pre_processing"><code class="name flex">
<span>def <span class="ident">pre_processing</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialization of the alternating minimization algorithm.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.Tensor</code></dt>
<dd>The current tensor approximation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tensap.TensorLearning</code></dt>
<dd>The TensorLearning object.</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.Tensor</code></dt>
<dd>The pre-processed tensor approximation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def pre_processing(self, f):
    &#39;&#39;&#39;
    Initialization of the alternating minimization algorithm.

    Parameters
    ----------
    f : tensap.Tensor
        The current tensor approximation.

    Returns
    -------
    tensap.TensorLearning
        The TensorLearning object.
    f : tensap.Tensor
        The pre-processed tensor approximation.

    &#39;&#39;&#39;</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.prepare_alternating_minimization_system"><code class="name flex">
<span>def <span class="ident">prepare_alternating_minimization_system</span></span>(<span>self, f, mu)</span>
</code></dt>
<dd>
<div class="desc"><p>Preparation of the alternating minimization algorithm.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.FunctionalTensor</code></dt>
<dd>The current approximation.</dd>
<dt><strong><code>mu</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of the parameter to be optimized.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tensap.TensorLearning</code></dt>
<dd>The TensorLearning object.</dd>
<dt><strong><code>A</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The matricized partial evaluation of f.</dd>
<dt><strong><code>b</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The target training data.</dd>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.FunctionalTensor</code></dt>
<dd>The current approximation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def prepare_alternating_minimization_system(self, f, mu):
    &#39;&#39;&#39;
    Preparation of the alternating minimization algorithm.

    Parameters
    ----------
    f : tensap.FunctionalTensor
        The current approximation.
    mu : int
        The number of the parameter to be optimized.

    Returns
    -------
    tensap.TensorLearning
        The TensorLearning object.
    A : numpy.ndarray
        The matricized partial evaluation of f.
    b : numpy.ndarray
        The target training data.
    f : tensap.FunctionalTensor
        The current approximation.

    &#39;&#39;&#39;</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.randomize_exploration_strategy"><code class="name flex">
<span>def <span class="ident">randomize_exploration_strategy</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Randomization of the exploration strategy.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>alpha_list</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The randomized exploration strategy.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def randomize_exploration_strategy(self):
    &#39;&#39;&#39;
    Randomization of the exploration strategy.

    Returns
    -------
    alpha_list : numpy.ndarray
        The randomized exploration strategy.

    &#39;&#39;&#39;</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.set_parameter"><code class="name flex">
<span>def <span class="ident">set_parameter</span></span>(<span>self, f, mu, coef)</span>
</code></dt>
<dd>
<div class="desc"><p>Update of the parameter mu of the tensor f.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.FunctionalTensor</code></dt>
<dd>The current approximation.</dd>
<dt><strong><code>mu</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of the optimized parameter.</dd>
<dt><strong><code>coef</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The new value of the parameter mu of f.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.FunctionalTensor</code></dt>
<dd>The updated approximation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def set_parameter(self, f, mu, coef):
    &#39;&#39;&#39;
    Update of the parameter mu of the tensor f.

    Parameters
    ----------
    f : tensap.FunctionalTensor
        The current approximation.
    mu : int
        The number of the optimized parameter.
    coef : numpy.ndarray
        The new value of the parameter mu of f.

    Returns
    -------
    f : tensap.FunctionalTensor
        The updated approximation.

    &#39;&#39;&#39;</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.solve"><code class="name flex">
<span>def <span class="ident">solve</span></span>(<span>self, *args)</span>
</code></dt>
<dd>
<div class="desc"><p>Solver for the learning problem with tensor formats.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>*args</code></strong> :&ensp;<code>misc</code></dt>
<dd>Additional arguments, if needed.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>NotImplementedError</code></dt>
<dd>If the required solver is not implemented.</dd>
<dt><code>ValueError</code></dt>
<dd>If the required data is not provided.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.FunctionalTensor</code></dt>
<dd>The learned approximation.</dd>
<dt><strong><code>output</code></strong> :&ensp;<code>dict</code></dt>
<dd>The outputs of the solver.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def solve(self, *args):
    &#39;&#39;&#39;
    Solver for the learning problem with tensor formats.

    Parameters
    ----------
    *args : misc
        Additional arguments, if needed.

    Raises
    ------
    NotImplementedError
        If the required solver is not implemented.
    ValueError
        If the required data is not provided.

    Returns
    -------
    f : tensap.FunctionalTensor
        The learned approximation.
    output : dict
        The outputs of the solver.

    &#39;&#39;&#39;
    # If possible, deduce from training_data the output dimension
    if isinstance(self.training_data, list) and \
            len(self.training_data) == 2 and \
            np.ndim(self.training_data[1]) == 2:
        self.output_dimension = self.training_data[1].shape[1]
    if self.output_dimension &gt; 1 and \
        (not isinstance(self, tensap.TreeBasedTensorLearning) or
         not isinstance(self.loss_function, (tensap.SquareLossFunction,
                                             tensap.CustomLossFunction))):
        raise NotImplementedError(
            &#39;Solver not implemented for vector-valued functions &#39; +
            &#39;approximation, use TreeBasedTensorLearning with a &#39; +
            &#39;SquareLossFunction or a CustomLossFunction instead.&#39;)

    if self.output_dimension &gt; 1 and \
        &#39;type&#39; in self.rank_adaptation_options and \
            self.rank_adaptation_options[&#39;type&#39;] == &#39;inner&#39;:
        print(&#39;Inner rank adaptation not implemented for &#39; +
              &#39;output_dimension greater than 1, disabling it.&#39;)
        del self.rank_adaptation_options[&#39;type&#39;]

    if self._warnings[&#39;orthonormality_warning_display&#39;] and \
        (self.bases is None or (self.bases is not None and not
                                np.all([x.is_orthonormal for x in
                                        self.bases.bases]))):
        self._warnings[&#39;orthonormality_warning_display&#39;] = False
        print(&#39;The implemented learning algorithms are designed &#39; +
              &#39;for orthonormal bases. These algorithms work with &#39; +
              &#39;non-orthonormal bases, but without some guarantees &#39; +
              &#39;on their results.&#39;)

    # If no bases are provided, warn that the returned functions are
    # evaluated on the training data
    if self.bases is None and \
            self._warnings[&#39;empty_bases_warning_display&#39;]:
        self._warnings[&#39;empty_bases_warning_display&#39;] = False
        print(&#39;The returned functions are evaluated on the training &#39; +
              &#39;data. To evaluate them at other points, assign to the &#39; +
              &#39;FunctionalTensor a nonempty field bases and set the &#39; +
              &#39;attribute evaluatedBases to False.&#39;)

    # If the test error cannot be computed, it is disabled
    if self.test_error and \
        not isinstance(self.bases, tensap.FunctionalBases) and \
            self.bases_eval_test is None:
        print(&#39;The test error cannot be computed.&#39;)
        self.test_error = False

    # Assert if basis adaptation can be performed
    if self.bases_adaptation_path is None and \
            not hasattr(self.bases, &#39;adaptation_path&#39;):
        if isinstance(self.linear_model_learning, (list, np.ndarray)) and \
                np.any([x.basis_adaptation for
                        x in self.linear_model_learning]):
            print(&#39;Cannot perform basis adaptation, disabling it.&#39;)
            for linear_solver in self.linear_model_learning:
                setattr(linear_solver, &#39;basis_adaptation&#39;, False)

        elif not isinstance(self.linear_model_learning,
                            (list, np.ndarray)) and \
                self.linear_model_learning.basis_adaptation:
            print(&#39;Cannot perform basis adaptation, disabling it.&#39;)
            self.linear_model_learning.basis_adaptation = False

    # Bases evaluation
    if hasattr(self.bases, &#39;eval&#39;):
        if self.training_data is not None and self.bases_eval is None:
            if isinstance(self.training_data, list) and \
                    self.training_data[0] is not None:
                self.bases_eval = self.bases.eval(self.training_data[0])
            elif not isinstance(self.training_data, list) and \
                    self.training_data is not None:
                self.bases_eval = self.bases.eval(self.training_data)
            else:
                raise ValueError(&#39;Must provide input training data.&#39;)

        if self.test_error and self.test_data is not None and \
                self.bases_eval_test is None:
            if isinstance(self.test_data, list) and \
                    self.test_data[0] is not None:
                self.bases_eval_test = self.bases.eval(self.test_data[0])
            elif not isinstance(self.test_data, list) and \
                    self.test_data is not None:
                self.bases_eval_test = self.bases.eval(self.test_data)
            else:
                raise ValueError(&#39;Must provide input test data.&#39;)

    if self.model_selection:
        self.store_iterates = True
        if self.model_selection_options[&#39;type&#39;] == &#39;test_error&#39; and \
                not self.test_error:
            print(&#39;Cannot perform test error based model selection, &#39; +
                  &#39;disabling it.&#39;)
            self.model_selection = False
        elif self.model_selection_options[&#39;type&#39;] == &#39;cv_error&#39; and \
                not self.error_estimation:
            self.error_estimation = True

    if self.rank_adaptation:
        if &#39;type&#39; not in self.rank_adaptation_options or \
                self.rank_adaptation_options[&#39;type&#39;] == &#39;standard&#39;:
            fun, output = self._solve_adaptation()
        elif isinstance(self.rank_adaptation_options[&#39;type&#39;], str):
            adapt_type = self.rank_adaptation_options[&#39;type&#39;]
            if adapt_type == &#39;dmrg&#39; or adapt_type == &#39;dmrg_low_rank&#39;:
                adapt_type = &#39;dmrg&#39;

            # Call the method corresponding to the rank adaptation option
            fun, output = eval(&#39;self._solve_&#39; + adapt_type.lower() +
                               &#39;_rank_adaptation()&#39;)
        else:
            raise ValueError(&#39;The rank_adaptation_options attribute &#39; +
                             &#39;must be either inexistant or a string.&#39;)
    elif self.algorithm == &#39;standard&#39;:
        fun, output = self._solve_standard()
    else:
        fun, output = eval(&#39;self._solve_&#39; + self.algorithm.lower() + &#39;()&#39;)

    if self.model_selection:
        if self.model_selection_options[&#39;type&#39;] == &#39;test_error&#39;:
            ind = np.argmin(output[&#39;test_error_iterations&#39;])
            fun = output[&#39;iterates&#39;][ind]
            output[&#39;selected_model_number&#39;] = ind
            if &#39;error_iterations&#39; in output and \
                np.size(output[&#39;error_iterations&#39;]) &gt; 0:
                output[&#39;error&#39;] = output[&#39;error_iterations&#39;][ind]
            output[&#39;test_error&#39;] = output[&#39;test_error_iterations&#39;][ind]
            if self.display:
                print(&#39;\nModel selection using the test error: model &#39; +
                      &#39;#%i selected&#39; % ind)
        elif self.model_selection_options[&#39;type&#39;] == &#39;cv_error&#39;:
            ind = np.argmin(output[&#39;error_iterations&#39;])
            fun = output[&#39;iterates&#39;][ind]
            output[&#39;selected_model_number&#39;] = ind
            if &#39;test_error_iterations&#39; in output and \
                np.size(output[&#39;test_error_iterations&#39;]) &gt; 0:
                output[&#39;test_error&#39;] = output[&#39;test_error_iterations&#39;][ind]
            output[&#39;error&#39;] = output[&#39;error_iterations&#39;][ind]
            if self.display:
                print(&#39;\nModel selection using the cross-validation &#39; +
                      &#39; error: model #%i selected&#39; % ind)
        else:
            print(&#39;Wrong model selection type, returning the last &#39; +
                  &#39;iterate.&#39;)

        if self.display:
            if self.alternating_minimization_parameters[&#39;display&#39;]:
                print(&#39;&#39;)
            self.final_display(fun)
            if &#39;error&#39; in output:
                print(&#39;, error = %2.5e&#39; % output[&#39;error&#39;], end=&#39;&#39;)
            if &#39;test_error&#39; in output:
                print(&#39;, test error = %2.5e&#39; % output[&#39;test_error&#39;],
                      end=&#39;&#39;)
            print(&#39;&#39;)

    return fun, output</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.stagnation_criterion"><code class="name flex">
<span>def <span class="ident">stagnation_criterion</span></span>(<span>self, f, f0)</span>
</code></dt>
<dd>
<div class="desc"><p>Computation of the stagnation criterion.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.FunctionalTensor</code></dt>
<dd>The current approximation.</dd>
<dt><strong><code>f0</code></strong> :&ensp;<code>tensap.FunctionalTensor</code></dt>
<dd>The previous approximation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>stagnation</code></strong> :&ensp;<code>float</code></dt>
<dd>The value of the stagnation criterion.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def stagnation_criterion(self, f, f0):
    &#39;&#39;&#39;
    Computation of the stagnation criterion.

    Parameters
    ----------
    f : tensap.FunctionalTensor
        The current approximation.
    f0 : tensap.FunctionalTensor
        The previous approximation.

    Returns
    -------
    stagnation : float
        The value of the stagnation criterion.

    &#39;&#39;&#39;</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning" href="index.html">tensap.approximation.tensor_approximation.tensor_learning</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning" href="#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning">TensorLearning</a></code></h4>
<ul class="">
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.adapt_tree" href="#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.adapt_tree">adapt_tree</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.adaptation_display" href="#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.adaptation_display">adaptation_display</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.final_display" href="#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.final_display">final_display</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.initial_guess_new_rank" href="#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.initial_guess_new_rank">initial_guess_new_rank</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.initialize" href="#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.initialize">initialize</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.local_solver" href="#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.local_solver">local_solver</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.new_rank_selection" href="#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.new_rank_selection">new_rank_selection</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.pre_processing" href="#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.pre_processing">pre_processing</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.prepare_alternating_minimization_system" href="#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.prepare_alternating_minimization_system">prepare_alternating_minimization_system</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.randomize_exploration_strategy" href="#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.randomize_exploration_strategy">randomize_exploration_strategy</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.set_parameter" href="#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.set_parameter">set_parameter</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.solve" href="#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.solve">solve</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.stagnation_criterion" href="#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.stagnation_criterion">stagnation_criterion</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>
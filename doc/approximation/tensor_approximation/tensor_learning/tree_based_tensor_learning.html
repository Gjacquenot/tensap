<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning API documentation</title>
<meta name="description" content="Module tree_based_tensor_learning â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning</code></h1>
</header>
<section id="section-intro">
<p>Module tree_based_tensor_learning.</p>
<p>Copyright (c) 2020, Anthony Nouy, Erwan Grelier
This file is part of tensap (tensor approximation package).</p>
<p>tensap is free software: you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.</p>
<p>tensap is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
See the
GNU Lesser General Public License for more details.</p>
<p>You should have received a copy of the GNU Lesser General Public License
along with tensap.
If not, see <a href="https://www.gnu.org/licenses/">https://www.gnu.org/licenses/</a>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
Module tree_based_tensor_learning.

Copyright (c) 2020, Anthony Nouy, Erwan Grelier
This file is part of tensap (tensor approximation package).

tensap is free software: you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

tensap is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Lesser General Public License for more details.

You should have received a copy of the GNU Lesser General Public License
along with tensap.  If not, see &lt;https://www.gnu.org/licenses/&gt;.

&#39;&#39;&#39;

from copy import deepcopy
from itertools import combinations
from random import shuffle
import numpy as np
import tensap


class TreeBasedTensorLearning(tensap.TensorLearning):
    &#39;&#39;&#39;
    Class TreeBasedTensorLearning.

    See also tensap.TensorLearning.

    Attributes
    ----------
    tree : tensap.DimensionTree
        The dimension tree of the tree-based tensor.
    is_active_node : list or numpy.ndarray
        Booleans indicating if each node is active.

    &#39;&#39;&#39;

    def __init__(self, tree, is_active_node, *args):
        &#39;&#39;&#39;
        Constructor for the class TreeBasedTensorLearning.

        See also tensap.TensorLearning.

        Parameters
        ----------
        tree : tensap.DimensionTree
            The dimension tree of the tree-based tensor.
        is_active_node : list or numpy.ndarray
            Booleans indicating if each node is active.
        *args : tuple
            Additional parameters.

        Returns
        -------
        None.

        &#39;&#39;&#39;
        super().__init__(*args)

        self.tree = tree
        self.is_active_node = list(map(bool, is_active_node))
        self.order = tree.dim2ind.size
        self._number_of_parameters = np.count_nonzero(is_active_node)

        self.initialization_type = &#39;canonical&#39;
        self.rank_adaptation_options[&#39;rank_one_correction&#39;] = True
        self.rank_adaptation_options[&#39;theta&#39;] = 0.8
        self.linear_model_learning_parameters[
            &#39;basis_adaptation_internal_nodes&#39;] = False

# %% Standard solver methods
    def initialize(self):
        assert self.tree is not None, \
            &#39;Must provide a DimensionTree object in property tree.&#39;

        if np.isscalar(self.rank) or len(self.rank) == self.order:
            rank = np.zeros(self.tree.nb_nodes, dtype=int)
            rank[self.is_active_node] = self.rank
            self.rank = rank
        self.rank[self.tree.root-1] = self.output_dimension

        shape = [x.shape[1] for x in self.bases_eval]
        if self.initialization_type == &#39;random&#39;:
            f = tensap.TreeBasedTensor.randn(self.tree, self.rank, shape,
                                             self.is_active_node)
        elif self.initialization_type == &#39;ones&#39;:
            f = tensap.TreeBasedTensor.ones(self.tree, self.rank, shape,
                                            self.is_active_node)
        elif self.initialization_type == &#39;initial_guess&#39;:
            f = self.initial_guess
            if not np.all(f.ranks == self.rank):
                tr = tensap.Truncator(tolerance=np.finfo(float).eps,
                                      max_rank=self.rank)
                f = tr.truncate(f)
        elif self.initialization_type == &#39;mean&#39; or \
                self.initialization_type == &#39;mean_randomized&#39;:
            if not np.all(self.rank == 1):
                raise NotImplementedError(&#39;Initialization only implemented &#39; +
                                          &#39;if np.all(self.rank == 1).&#39;)
            if not isinstance(self.training_data, list) or \
                    (isinstance(self.training_data, list) and
                     len(self.training_data) == 1):
                raise NotImplementedError(&#39;Initialization type not &#39; +
                                          &#39;implemented in unsupervised &#39; +
                                          &#39;learning.&#39;)
            if isinstance(self.bases, tensap.FunctionalBases):
                means = self.bases.mean()
            else:
                means = [np.mean(x) for x in self.bases_eval]
            if self.initialization_type == &#39;mean_randomized&#39;:
                means = [x + 0.01*np.random.randn(*x.shape) for x in means]
            means = [tensap.FullTensor(x, 2, [x.shape[0], 1]) for x in means]

            f = tensap.TreeBasedTensor.ones(self.tree, self.rank, shape)
            f.tensors[self.tree.dim2ind-1] = means
            nb_child = len(self.tree.children(self.tree.root))
            if np.ndim(self.training_data[1]) == 2:
                shape = np.concatenate((np.full(nb_child, 1),
                                       [self.training_data[1].shape[1]]))
            else:
                shape = np.full(nb_child, 1)
            f.tensors[self.tree.root-1] = \
                tensap.FullTensor(np.mean(self.training_data[1]), shape=shape)
            f.update_attributes()

            f = f.inactivate_nodes(
                np.nonzero(np.logical_not(self.is_active_node))[0]+1)
        elif self.initialization_type == &#39;canonical&#39;:
            if self.output_dimension != 1:
                print(&#39;Canonical initialization not implemented for &#39; +
                      &#39;outputDimension &gt; 1, performing a random &#39; +
                      &#39;initialization.&#39;)
                f = tensap.TreeBasedTensor.randn(self.tree, self.rank, shape,
                                                 self.is_active_node)
            else:
                f = self.canonical_initialization(np.max(self.rank))
                if not np.all(f.ranks == self.rank):
                    tr = tensap.Truncator(tolerance=np.finfo(float).eps,
                                          max_rank=self.rank)
                    f = tr.truncate(f)
        else:
            raise ValueError(&#39;Wrong initialization type.&#39;)

        if not np.all(f.ranks == self.rank):
            f = TreeBasedTensorLearning.\
                enriched_edges_to_ranks_random(f, self.rank)

        # Exploration strategy of the tree by increasing level
        tree = f.tree
        exploration_strategy = np.zeros(self._number_of_parameters, dtype=int)
        active_nodes = f.active_nodes
        rep = 0
        for level in range(np.max(tree.level)+1):
            nodes = np.intersect1d(tree.nodes_with_level(level), active_nodes)
            exploration_strategy[rep:rep+len(nodes)] = nodes
            rep += len(nodes)
        self._exploration_strategy = exploration_strategy
        return self, f

    def pre_processing(self, f):
        if isinstance(self.linear_model_learning, (list, np.ndarray)) and \
                len(self.linear_model_learning) != f.tensor.tree.nb_nodes:
            tmp = np.empty(f.tensor.tree.nb_nodes, dtype=object)
            tmp[f.tensor.is_active_node] = self.linear_model_learning
            self.linear_model_learning = tmp
        return self, f

    def randomize_exploration_strategy(self):
        strategy = np.zeros(self._number_of_parameters)
        for level in np.arange(np.max(self.tree.level), -1, -1):
            active_nodes = np.intersect1d(self.tree.nodes_with_level(level),
                                          np.nonzero(self.is_active_node)[0])
            _, ind = np.intersect1d(self._exploration_strategy,
                                    active_nodes,
                                    return_indices=True)
            strategy[ind] = self._exploration_strategy[
                np.random.permutation(ind)]
        return strategy

    def prepare_alternating_minimization_system(self, f, mu):
        tree = f.tensor.tree
        if self.linear_model_learning[mu-1].basis_adaptation:
            if np.isin(mu, tree.internal_nodes):
                if self.linear_model_learning_parameters[
                        &#39;basis_adaptation_internal_nodes&#39;]:
                    tr = tensap.Truncator(tolerance=np.finfo(float).eps,
                                          max_rank=np.max(f.tensor.ranks))
                    f.tensor = tr.hsvd(f.tensor)
                elif np.all(f.tensor.is_active_node[tree.children(mu)-1]):
                    self.linear_model_learning[mu-1].basis_adaptation = False
            f.tensor = f.tensor.orth_at_node(mu)
            self.tree = tree
            self.is_active_node = f.tensor.is_active_node
            self.linear_model_learning[mu-1].basis_adaptation_path = \
                self.create_basis_adaptation_path(f.tensor.ranks, mu)
        else:
            f.tensor = f.tensor.orth_at_node(mu)

        grad = f.parameter_gradient_eval(mu)
        if mu == tree.root:
            A = np.reshape(grad.data, [grad.shape[0], -1], order=&#39;F&#39;)
            self.linear_model_learning[mu-1].initial_guess = np.reshape(
                    f.tensor.tensors[mu-1].data,
                    [-1, f.tensor.ranks[tree.root-1]], order=&#39;F&#39;)
        else:
            A = np.reshape(grad.data, [grad.shape[0], -1,
                                       f.tensor.ranks[tree.root-1]], order=&#39;F&#39;)
            self.linear_model_learning[mu-1].initial_guess = np.reshape(
                    f.tensor.tensors[mu-1].data, -1, order=&#39;F&#39;)

        if isinstance(self.loss_function, tensap.DensityL2LossFunction):
            if isinstance(self.training_data, list) and \
                    len(self.training_data) == 2:
                y = self.training_data[1]
                if isinstance(y, tensap.FunctionalTensor):
                    y = y.tensor
                y = y.orth()
                if tree.is_leaf[mu-1]:
                    a = deepcopy(y)
                    for nod in tree.internal_nodes:
                        a.tensors[nod-1] = tensap.FullTensor(
                            y.tensors[nod-1].data *
                            f.tensor.tensors[nod-1].data,
                            shape=y.tensors[nod-1].shape)
                    ind = np.setdiff1d(range(self.order),
                                       np.nonzero(tree.dim2ind == mu)[0])
                    C = [x.data for x in f.tensor.tensors[tree.dim2ind-1]]
                    b = a.tensor_vector_product([C[x] for x in ind], ind)
                    b = b.tensors[0].data
                else:
                    b = f.tensor.dot(y) / f.tensor.tensors[mu-1].data
            else:
                b = []
        elif isinstance(self.training_data, list) and \
                len(self.training_data) == 2:
            b = self.training_data[1]
            self.linear_model_learning[mu-1].shared_coefficients = \
                mu != tree.root

        return self, A, b, f

    def set_parameter(self, f, mu, coef):
        f.tensor.tensors[mu-1] = \
            tensap.FullTensor(coef, shape=f.tensor.tensors[mu-1].shape)
        f.tensor.tensors[mu-1].is_orth = False
        return f

    def stagnation_criterion(self, f, f0):
        return (f.tensor - f0.tensor).norm() / f0.tensor.norm()

    def final_display(self, f):
        print(&#39;Ranks = [%s]&#39; % &#39;, &#39;.join(map(str, f.tensor.ranks)), end=&#39;&#39;)

    def canonical_initialization(self, rank):
        &#39;&#39;&#39;
        Rank-r canonical initialization.

        Parameters
        ----------
        rank : int
            The rank of the canonical initialization.

        Returns
        -------
        tensap.TreeBasedTensor
            The rank-r canonical initialization..

        &#39;&#39;&#39;
        solver = tensap.CanonicalTensorLearning(self.order, self.loss_function)
        if isinstance(self.linear_model_learning, list):
            solver.linear_model_learning = self.linear_model_learning[0]
        else:
            solver.linear_model_learning = self.linear_model_learning
        solver.alternating_minimization_parameters = \
            deepcopy(self.alternating_minimization_parameters)
        solver.tolerance[&#39;on_stagnation&#39;] = np.finfo(float).eps
        solver.tolerance[&#39;on_error&#39;] = np.finfo(float).eps
        solver.bases = self.bases
        solver.bases_eval = self.bases_eval
        solver.bases_eval_test = self.bases_eval_test
        solver.display = False
        solver.alternating_minimization_parameters[&#39;display&#39;] = False
        solver.initialization_type = &#39;mean&#39;
        solver.rank_adaptation = True
        solver.rank_adaptation_options[&#39;max_iterations&#39;] = rank
        solver.bases_adaptation_path = self.bases_adaptation_path
        solver.test_error = self.test_error
        solver.training_data = self.training_data
        solver.test_data = self.test_data
        solver._warnings = self._warnings

        f = solver.solve()[0]
        return f.tensor.tree_based_tensor(self.tree, self.is_active_node)

    def canonical_correction(self, f, rank):
        &#39;&#39;&#39;
        Rank-r canonical correction.

        Parameters
        ----------
        f : tensap.FunctionalTensor or None
            The current approximation.
        rank : int
            The rank of the canonical correction.

        Raises
        ------
        NotImplementedError
            If the method is not implemented.

        Returns
        -------
        f : tensap.FunctionalTensor
            The corrected approximation.

        &#39;&#39;&#39;
        if isinstance(f, tensap.FunctionalTensor):
            fx = f.tensor.tensor_matrix_product_eval_diag(self.bases_eval).data
        elif f is None:
            fx = 0
        else:
            raise NotImplementedError(&#39;Not implemented.&#39;)

        solver = deepcopy(self)
        if isinstance(solver.training_data, list) and \
                len(solver.training_data) == 2:
            solver.training_data[1] -= fx
        elif isinstance(solver.loss_function, tensap.DensityL2LossFunction):
            solver.training_data = [solver.training_data, f]

        f_add = solver.canonical_initialization(rank)
        if isinstance(f_add, tensap.FunctionalTensor):
            f_add = f_add.tensor
        if f is not None:
            f = f.tensor + f_add
        else:
            f = f_add
        return f

    def rank_one_correction(self, f):
        &#39;&#39;&#39;
        Rank one correction.

        Parameters
        ----------
        f : tensap.FunctionalTensor or None
            The current approximation.

        Raises
        ------
        NotImplementedError
            If the method is not implemented.

        Returns
        -------
        f : tensap.FunctionalTensor
            The corrected approximation.

        &#39;&#39;&#39;
        if isinstance(f, tensap.FunctionalTensor):
            fx = f.tensor.tensor_matrix_product_eval_diag(self.bases_eval).data
        elif f is None:
            fx = 0
        else:
            raise NotImplementedError(&#39;Not implemented.&#39;)

        solver = deepcopy(self)
        solver.model_selection = False
        if isinstance(solver.training_data, list) and \
                len(solver.training_data) == 2:
            solver.training_data[1] -= fx
        elif isinstance(solver.loss_function, tensap.DensityL2LossFunction):
            solver.training_data = [solver.training_data, f]
        solver.rank_adaptation = False
        solver.tree_adaptation = False
        solver.rank = 1
        solver.display = False
        solver.alternating_minimization_parameters[&#39;display&#39;] = False
        solver.initialization_type = &#39;ones&#39;
        solver.alternating_minimization_parameters[&#39;max_iterations&#39;] = 1

        f_add = solver.solve()[0]
        if isinstance(f_add, tensap.FunctionalTensor):
            f_add = f_add.tensor
        if f is not None:
            f = f.tensor + f_add
        else:
            f = f_add
        return f

    def create_basis_adaptation_path(self, ranks, alpha):
        &#39;&#39;&#39;
        Creation of the basis adaptation path.

        Parameters
        ----------
        rank : list or numpy.ndarray
            The alpha-ranks of the current approximation.
        alpha : int
            The current node.

        Returns
        -------
        path : numpy.ndarray
            The basis adaptation path.

        &#39;&#39;&#39;
        tree = self.tree
        ranks = np.array(ranks)
        ranks[tree.root-1] = 1
        if tree.is_leaf[alpha-1]:
            path_alpha = self.bases_adaptation_path[
                np.nonzero(tree.dim2ind == alpha)[0][0]].astype(bool)
            ranks = ranks[alpha-1]
            path = path_alpha[np.newaxis, :, np.newaxis, :]
            path = np.tile(path, [1, 1, ranks, 2])
            path = np.reshape(path, [path.shape[1]*ranks, -1], order=&#39;F&#39;)
        else:
            assert not self.linear_model_learning_parameters[
                &#39;basis_adaptation_internal_nodes&#39;], \
                &#39;Basis adaptation for internal nodes is not implemented.&#39;
            ch = tree.children(alpha)
            if np.all(self.is_active_node[ch-1]):
                ch_a = ch[self.is_active_node[ch-1]]
                path = np.full(np.prod(ranks[ch_a-1]) * ranks[alpha-1], True)
            else:
                path_alpha = []
                for nod in ch:
                    if self.is_active_node[nod-1]:
                        path_alpha.append(np.full((ranks[nod-1], 1), True))
                    else:
                        ind = np.nonzero(nod == tree.dim2ind)[0][0]
                        path_alpha.append(
                            self.bases_adaptation_path[ind].astype(bool))
                path = path_alpha[-1]
                for ind in np.arange(len(path_alpha)-2, -1, -1):
                    path = np.kron(path, path_alpha[ind])
                path = np.tile(path, [ranks[alpha-1], 1]).astype(bool)
        return path

# %% Rank adaptation solver methods
    def local_solver(self):
        s_local = deepcopy(self)
        s_local.display = False
        s_local.rank_adaptation = False
        s_local.store_iterates = False
        s_local.test_error = False
        s_local.model_selection = False
        return s_local

    def new_rank_selection(self, f):
        if self.rank_adaptation_options[&#39;rank_one_correction&#39;]:
            s_local = deepcopy(self)
            ranks_add = np.ones(f.tensor.tree.nb_nodes, dtype=int)
            ranks_add[f.tensor.tree.root-1] = 0
            ranks_add[f.tensor.non_active_nodes-1] = 0
            s_local.rank = TreeBasedTensorLearning.make_ranks_admissible(
                f.tensor, f.tensor.ranks + ranks_add)[0]
            s_local.initialization_type = &#39;initial_guess&#39;
            tr = tensap.Truncator(tolerance=0, max_rank=s_local.rank)
            s_local.initial_guess = tr.truncate(self.rank_one_correction(f))
            s_local.alternating_minimization_parameters[&#39;max_iterations&#39;] = 10
            s_local.model_selection = False
            s_local.rank_adaptation = False
            s_local.display = False
            s_local.alternating_minimization_parameters[&#39;display&#39;] = False
            tensor_for_selection = s_local.solve()[0].tensor
        else:
            tensor_for_selection = f.tensor

        sin_val = tensor_for_selection.singular_values()

        # Remove from the rank adaptation candidates: the inactive nodes, the
        # root, the leaf nodes with a rank equal to the dimension of the basis
        # associated to it, and the nodes for  which the smallest singular
        # value is almost zero.
        sin_val = np.array([np.nan if x is None else x for x in sin_val])
        sin_val[f.tensor.tree.root-1] = np.nan
        dim2ind = np.intersect1d(f.tensor.tree.dim2ind, f.tensor.active_nodes)
        ind = [len(set(x.size)) == 1 for x in f.tensor.tensors[dim2ind-1]]
        sin_val[dim2ind[ind]-1] = np.nan
        sin_val[s_local.rank != tensor_for_selection.ranks] = np.nan

        sin_val_min = np.array([np.min(x) for x in sin_val])
        sin_val_min[[x / tensor_for_selection.norm() &lt; np.finfo(float).eps if
                     not np.isnan(x) else False for x in sin_val_min]] = np.nan

        # Remove nodes that cannot be enriched because their rank is equal to
        # the product of the ranks of their children, and their children cannot
        # be enriched themselves.
        tree = tensor_for_selection.tree
        rank = f.tensor.ranks
        desc = np.setdiff1d(np.arange(1, tree.nb_nodes+1),
                            np.nonzero(tree.is_leaf)[0]+1)
        cannot_be_increased = np.full(tree.nb_nodes, False)
        cannot_be_increased[tree.root-1] = True
        cannot_be_increased[tree.is_leaf] = np.isnan(sin_val_min[tree.is_leaf])
        for level in np.arange(np.max(tree.level), 0, -1):
            nod_lvl = np.intersect1d(tree.nodes_with_level(level), desc)
            for nod in nod_lvl:
                ch = tree.children(nod)
                if np.all(cannot_be_increased[ch-1]) and \
                        rank[nod-1] == np.prod(rank[ch-1]):
                    cannot_be_increased[nod-1] = True
        cannot_be_increased_nodes = tree.nodes_indices[cannot_be_increased]
        for level in np.arange(1, np.max(tree.level)):
            nod_lvl = np.intersect1d(tree.nodes_with_level(level),
                                     cannot_be_increased_nodes)
            for nod in nod_lvl:
                pa = tree.parent(nod)
                ind = np.setdiff1d(tree.children(pa), nod)
                ind = np.concatenate(([pa], ind))
                if np.all(cannot_be_increased[ind-1]) and \
                        rank[nod-1] == np.prod(rank[ind-1]):
                    cannot_be_increased[nod-1] = True
        sin_val_min[cannot_be_increased] = np.nan

        if np.all(np.isnan(sin_val_min)):
            enriched_nodes = np.array([], dtype=int)
        else:
            theta = self.rank_adaptation_options[&#39;theta&#39;] * \
                np.nanmax(sin_val_min)
            enriched_nodes = np.nonzero([not np.isnan(x) and x &gt;= theta for
                                         x in sin_val_min])[0] + 1

        new_rank = np.array(f.tensor.ranks)
        new_rank[enriched_nodes-1] += 1

        if not f.tensor.is_admissible_rank(new_rank):
            # Add to the already enriched nodes nodes one by one in decreasing
            # order of singular value until the rank is admissible.
            enriched_nodes_theta = np.array(enriched_nodes)
            rank_theta = np.array(new_rank)
            sin_val_min[enriched_nodes_theta-1] = np.nan
            sin_val_min_sorted = TreeBasedTensorLearning.unique_tol(
                sin_val_min, 1e-2)
            sin_val_min_sorted = np.flip(sin_val_min_sorted)
            sin_val_min_sorted = sin_val_min_sorted[
                np.isfinite(sin_val_min_sorted)]

            for sv in sin_val_min_sorted:
                new_rank = np.array(rank_theta)
                ind = [x &gt;= sv if not np.isnan(x) else False for
                       x in sin_val_min]
                new_rank[ind] += 1
                if f.tensor.is_admissible_rank(new_rank):
                    enriched_nodes = np.concatenate((enriched_nodes_theta,
                                                     np.nonzero(ind)[0]+1))
                    break
            if not f.tensor.is_admissible_rank(new_rank):
                new_rank = f.tensor.ranks
                enriched_nodes = np.array([])

        return new_rank, enriched_nodes, tensor_for_selection

    def initial_guess_new_rank(self, s_local, f, new_rank):
        s_local.initialization_type = &#39;initial_guess&#39;
        if not np.all(f.ranks == new_rank):
            tr = tensap.Truncator(tolerance=0, max_rank=new_rank)
            s_local.initial_guess = tr.truncate(f)
        else:
            s_local.initial_guess = f
        return s_local

    def adaptation_display(self, f, enriched_nodes):
        print(&#39;\tEnriched nodes: [%s]\n\tRanks = [%s]&#39; %
              (&#39;, &#39;.join(map(str, enriched_nodes)),
               &#39;, &#39;.join(map(str, f.tensor.ranks))))

    def adapt_tree(self, f, cv_error, test_error, output, *args):
        if not self.tree_adaptation:
            return self, f, output

        output[&#39;adapted_tree&#39;] = False

        if np.any(f.tensor.ranks[f.tensor.active_nodes-1] == 0):
            print(&#39;Some ranks equal to 0, disabling tree adaptation for &#39; +
                  &#39;this step.&#39;)
            return self, f, output
        if self.tree_adaptation_options[&#39;tolerance&#39;] is not None:
            adapt_tree_error = self.tree_adaptation_options[&#39;tolerance&#39;]
        elif self.loss_function.error_type == &#39;relative&#39;:
            if test_error is None or test_error == 0:
                adapt_tree_error = cv_error
            elif cv_error is None or test_error != 0:
                adapt_tree_error = test_error
        else:
            print(&#39;Must provide a tolerance for the tree adaptation in &#39; +
                  &#39;the treeAdaptationOptions property. Disabling tree &#39; +
                  &#39;adaptation.&#39;)
            self.tree_adaptation = False
            return self, f, output

        f_perm = f.tensor.optimize_dimension_tree(
            adapt_tree_error, self.tree_adaptation_options[&#39;max_iterations&#39;])
        if f_perm.storage() &lt; f.tensor.storage():
            f.tensor = f_perm
            self.tree = f.tensor.tree
            self.is_active_node = f.tensor.is_active_node
            output[&#39;adapted_tree&#39;] = True
            if self.display:
                print(&#39;\tTree adaptation:\n\t\tRanks after permutation &#39; +
                      &#39;= [%s]&#39; % &#39;, &#39;.join(map(str, f.tensor.ranks)))
        return self, f, output

# %% Inner rank adaptation solver
    def _solve_dmrg_rank_adaptation(self):
        if &#39;max_rank&#39; not in self.rank_adaptation_options:
            self.rank_adaptation_options[&#39;max_rank&#39;] = 100
        if &#39;post_alternating_minimization&#39; not in self.rank_adaptation_options:
            self.rank_adaptation_options[&#39;post_alternating_minimization&#39;] = \
                False
        if self.rank_adaptation_options[&#39;type&#39;] == &#39;dmrg_low_rank&#39; and \
                &#39;model_selection_type&#39; not in self.rank_adaptation_options:
            self.rank_adaptation_options[&#39;model_selection_type&#39;] = &#39;cv_error&#39;

        if self.display:
            self.alternating_minimization_parameters[&#39;display&#39;] = True

        output = {&#39;flag&#39;: 0}

        self, f = self.initialize()
        f = tensap.FunctionalTensor(f, self.bases_eval)

        # Exploration strategy of the tree by decreasing level
        tree = f.tensor.tree
        exploration_strategy = np.zeros(self._number_of_parameters, dtype=int)
        active_nodes = f.tensor.active_nodes
        rep = 0
        for level in np.arange(np.max(tree.level), -1, -1):
            nodes = np.intersect1d(tree.nodes_with_level(level), active_nodes)
            exploration_strategy[rep:rep+len(nodes)] = nodes
            rep += len(nodes)
        self._exploration_strategy = np.setdiff1d(exploration_strategy,
                                                  tree.root,
                                                  assume_unique=True)

        # Replication of the LinearModelLearning objects
        if self.linear_model_learning_parameters[
                &#39;identical_for_all_parameters&#39;] and \
                not isinstance(self.linear_model_learning, (list, np.ndarray)):
            self.linear_model_learning = list(map(deepcopy, [
                self.linear_model_learning] * self._number_of_parameters))
        elif isinstance(self.linear_model_learning, (list, np.ndarray)) and \
                len(self.linear_model_learning) != self._number_of_parameters:
            raise ValueError(&#39;Must provide self._numberOfParameters &#39; +
                             &#39;LinearModelLearning objects.&#39;)

        if self.error_estimation:
            for x in self.linear_model_learning:
                setattr(x, &#39;error_estimation&#39;, True)

        # Working set paths
        if isinstance(self.linear_model_learning, (list, np.ndarray)) and \
            np.any([x.basis_adaptation for
                    x in self.linear_model_learning]) and \
                self.bases_adaptation_path is None:
            self.bases_adaptation_path = self.bases.adaptation_path()

        if self.alternating_minimization_parameters[&#39;max_iterations&#39;] == 0:
            return f, output

        output[&#39;stagnation_criterion&#39;] = []
        output[&#39;iterates&#39;] = []
        output[&#39;error_iterations&#39;] = []
        output[&#39;test_error_iterations&#39;] = []

        # Alternating minimization loop
        for iteration in range(self.alternating_minimization_parameters
                               [&#39;max_iterations&#39;]):
            self, f = self.pre_processing(f)
            f0 = deepcopy(f)

            if self.alternating_minimization_parameters[&#39;random&#39;]:
                # Randomize the exploration strategy
                alpha_list = self.randomize_exploration_strategy()
            else:
                alpha_list = self._exploration_strategy

            tree = f.tensor.tree
            for alpha in alpha_list:
                if self.linear_model_learning[alpha-1].basis_adaptation:
                    if np.isin(alpha, tree.internal_nodes):
                        if self.linear_model_learning_parameters[
                                &#39;basis_adaptation_internal_nodes&#39;]:
                            tr = tensap.Truncator(
                                tolerance=np.finfo(float).eps,
                                max_rank=np.max(f.tensor.ranks))
                            f.tensor = tr.hsvd(f.tensor)
                        elif np.all(f.tensor.is_active_node[
                                tree.children(alpha)-1]):
                            self.linear_model_learning[alpha-1].\
                                basis_adaptation = False
                    f.tensor = f.tensor.orth_at_node(tree.parent(alpha))
                    self.tree = tree
                    self.is_active_node = f.tensor.is_active_node

                    if self.rank_adaptation_options[&#39;type&#39;] == &#39;dmrg&#39;:
                        self.linear_model_learning[alpha-1].\
                            basis_adaptation_path = \
                            self.create_basis_adaptation_path_dmrg(
                                f.tensor.ranks, alpha)
                    elif self.rank_adaptation_options[&#39;type&#39;] == \
                            &#39;dmrg_low_rank&#39;:
                        self.linear_model_learning[alpha-1].\
                            basis_adaptation_path = self.\
                            create_basis_adaptation_path_dmrg_low_rank(
                                f.tensor.ranks, alpha)
                    else:
                        raise ValueError(&#39;Wrong rank adaptation type.&#39;)
                else:
                    f.tensor = f.tensor.orth_at_node(tree.parent(alpha))

                grad = f.parameter_gradient_eval_dmrg(
                    alpha, dmrg_type=self.rank_adaptation_options[&#39;type&#39;])

                if isinstance(self.loss_function,
                              tensap.DensityL2LossFunction):
                    if isinstance(self.training_data, list) and \
                            len(self.training_data) == 2:
                        y = self.training_data[1]
                        if isinstance(y, tensap.FunctionalTensor):
                            y = y.tensor
                        y = y.orth()
                        if tree.is_leaf[alpha-1]:
                            a = deepcopy(y)
                            for nod in tree.internal_nodes:
                                a.tensors[nod-1] = tensap.FullTensor(
                                    y.tensors[nod-1].data *
                                    f.tensor.tensors[nod-1].data,
                                    shape=y.tensors[nod-1].shape)
                            ind = np.setdiff1d(
                                range(self.order),
                                np.nonzero(tree.dim2ind == alpha)[0])
                            C = [x.data for
                                 x in f.tensor.tensors[tree.dim2ind-1]]
                            b = a.tensor_vector_product([C[x] for x in ind],
                                                        ind)
                            b = b.tensors[0].data
                        else:
                            b = f.tensor.dot(y) / \
                                f.tensor.tensors[alpha-1].data
                    else:
                        b = []
                elif isinstance(self.training_data, list) and \
                        len(self.training_data) == 2:
                    b = self.training_data[1]
                    self.linear_model_learning[alpha-1].shared_coefficients = \
                        alpha != tree.root

                gamma = tree.parent(alpha)
                ind = np.setdiff1d(
                    np.arange(1, f.tensor.tensors[gamma-1].order+1),
                    tree.child_number(alpha))

                if self.rank_adaptation_options[&#39;type&#39;] == &#39;dmrg&#39;:
                    A = np.reshape(grad.data, (grad.shape[0], -1), order=&#39;F&#39;)

                    self.linear_model_learning[alpha-1].training_data = [None,
                                                                         b]
                    self.linear_model_learning[alpha-1].basis = None
                    self.linear_model_learning[alpha-1].basis_eval = A

                    C, output_tmp = \
                        self.linear_model_learning[alpha-1].solve()

                    if C is None or np.count_nonzero(C) == 0 or \
                            not np.all(np.isfinite(C)):
                        print(&#39;Empty, zero or NaN solution, returning to &#39; +
                              &#39;the previous iteration.&#39;)
                        output[&#39;flag&#39;] = -2
                        output[&#39;error&#39;] = np.inf
                        break

                    sz_1 = np.prod(f.tensor.tensors[alpha-1].shape[:-1])
                    sz_2 = np.prod(f.tensor.tensors[gamma-1].shape[ind-1])
                    C = tensap.FullTensor(C, 2, [sz_1, sz_2])

                    tr = tensap.Truncator()
                    tr.tolerance = self.tolerance[&#39;on_error&#39;] / \
                        np.sqrt(np.count_nonzero(f.tensor.is_active_node)-1)
                    tr.max_rank = self.rank_adaptation_options[&#39;max_rank&#39;]
                    C = tr.truncate(C)
                    rank = [np.shape(C.space[0])[1]]

                    sz_1 = np.concatenate(
                        (f.tensor.tensors[alpha-1].shape[:-1], rank))
                    sz_2 = np.concatenate(
                        (f.tensor.tensors[gamma-1].shape[ind-1], rank))

                    a_alpha = np.reshape(C.space[0], sz_1, order=&#39;F&#39;)
                    a_gamma = np.reshape(C.space[1]*np.tile(
                        np.reshape(C.core.data, [1, -1]),
                        (np.shape(C.space[1])[0], 1)), sz_2, order=&#39;F&#39;)
                    perm = np.concatenate((ind, [tree.child_number(alpha)]))
                    a_gamma = np.transpose(a_gamma, np.argsort(perm-1))
                    sz_2[perm-1] = np.array(sz_2)
                elif self.rank_adaptation_options[&#39;type&#39;] == &#39;dmrg_low_rank&#39;:
                    A = [np.reshape(x.data, [x.shape[0], -1], order=&#39;F&#39;) for
                         x in grad]

                    s_local = TreeBasedTensorLearning(
                        tensap.DimensionTree.trivial(2), [True, True, False],
                        self.loss_function)
                    s_local.rank_adaptation = True
                    s_local.tolerance[&#39;on_error&#39;] = \
                        self.tolerance[&#39;on_error&#39;] / \
                        np.sqrt(np.count_nonzero(f.tensor.is_active_node)-1)
                    s_local.tolerance[&#39;on_stagnation&#39;] = \
                        self.tolerance[&#39;on_stagnation&#39;]
                    s_local.rank_adaptation_options[&#39;max_iterations&#39;] = \
                        self.rank_adaptation_options[&#39;max_rank&#39;]
                    s_local.alternating_minimization_parameters = \
                        deepcopy(self.alternating_minimization_parameters)
                    s_local.alternating_minimization_parameters[&#39;display&#39;] = \
                        False
                    s_local.store_iterates = True
                    s_local.test_error = False
                    s_local.error_estimation = True
                    s_local.display = False
                    s_local.order = 2
                    s_local.linear_model_learning = \
                        self.linear_model_learning[alpha-1]
                    s_local._warnings[&#39;orthonormality_warning_display&#39;] = False
                    s_local._warnings[&#39;empty_bases_warning_display&#39;] = False
                    s_local.bases_adaptation_path = \
                        self.linear_model_learning[alpha-1].\
                        basis_adaptation_path
                    s_local.training_data = self.training_data
                    s_local.bases_eval = A
                    s_local.model_selection = True
                    s_local.model_selection_options[&#39;type&#39;] = \
                        self.rank_adaptation_options[&#39;model_selection_type&#39;]
                    C, output_tmp = s_local.solve()

                    rank = [C.tensor.ranks[1]]
                    sz_1 = np.concatenate(
                        (f.tensor.tensors[alpha-1].shape[:-1], rank))
                    sz_2 = np.concatenate(
                        (f.tensor.tensors[gamma-1].shape[ind-1], rank))

                    a_alpha = C.tensor.tensors[1].reshape(sz_1)
                    a_gamma = C.tensor.tensors[0].transpose([1, 0]).reshape(
                        sz_2)

                    perm = np.concatenate((ind, [tree.child_number(alpha)]))
                    a_gamma = a_gamma.itranspose(perm-1)
                    sz_2[perm-1] = np.array(sz_2)

                    a_alpha = a_alpha.data
                    a_gamma = a_gamma.data

                    if (a_alpha is None or np.count_nonzero(a_alpha) == 0 or
                            not np.all(np.isfinite(a_alpha))) or \
                        (a_gamma is None or np.count_nonzero(a_gamma) == 0 or
                            not np.all(np.isfinite(a_gamma))):
                        print(&#39;Empty, zero or NaN solution, returning to &#39; +
                              &#39;the previous iteration.&#39;)
                        output[&#39;flag&#39;] = -2
                        output[&#39;error&#39;] = np.inf
                        break
                else:
                    raise ValueError(&#39;Wrong rank adaptation type.&#39;)
                f.tensor.tensors[alpha-1] = tensap.FullTensor(a_alpha,
                                                              np.size(sz_1),
                                                              sz_1)
                f.tensor.tensors[gamma-1] = tensap.FullTensor(a_gamma,
                                                              np.size(sz_2),
                                                              sz_2)

            if self.rank_adaptation_options[&#39;post_alternating_minimization&#39;]:
                print(&#39;\t\tPost alternating minimization.&#39;)
                s_local = TreeBasedTensorLearning(self.tree,
                                                  self.is_active_node,
                                                  self.loss_function)
                s_local.bases_eval = self.bases_eval
                s_local.bases_adaptation_path = self.bases_adaptation_path
                s_local.training_data = self.training_data
                s_local.test_error = self.test_error
                s_local.test_data = self.test_data
                s_local.bases_eval_test = self.bases_eval_test
                s_local.model_selection = False
                s_local.rank_adaptation = False
                s_local.store_iterates = False
                s_local.rank = f.tensor.ranks
                s_local.initialization_type = &#39;initial_guess&#39;
                s_local.initial_guess = f.tensor
                s_local.tolerance[&#39;on_error&#39;] = self.tolerance[&#39;on_error&#39;] / \
                    np.sqrt(np.count_nonzero(f.tensor.is_active_node)-1)
                s_local.tolerance[&#39;on_stagnation&#39;] = \
                    self.tolerance[&#39;on_stagnation&#39;]
                s_local.alternating_minimization_parameters = \
                    deepcopy(self.alternating_minimization_parameters)
                s_local.alternating_minimization_parameters[&#39;display&#39;] = False
                s_local.error_estimation = True
                s_local.display = False
                s_local.linear_model_learning = \
                    [x for x in self.linear_model_learning if x is not None]
                s_local._warnings[&#39;orthonormality_warning_display&#39;] = False
                s_local._warnings[&#39;empty_bases_warning_display&#39;] = False
                f, output_tmp = s_local.solve()

            stagnation = self.stagnation_criterion(f, f0)
            output[&#39;stagnation_criterion&#39;].append(stagnation)

            if self.store_iterates:
                if isinstance(self.bases, tensap.FunctionalBases):
                    output[&#39;iterates&#39;].append(tensap.FunctionalTensor(
                        f.tensor, self.bases))
                else:
                    output[&#39;iterates&#39;].append(f)

            if &#39;error&#39; in output_tmp:
                output[&#39;error&#39;] = output_tmp[&#39;error&#39;]
                output[&#39;error_iterations&#39;].append(output[&#39;error&#39;])

            if self.test_error:
                f_eval_test = tensap.FunctionalTensor(f, self.bases_eval_test)
                output[&#39;test_error&#39;] = self.loss_function.test_error(
                    f_eval_test, self.test_data)
                output[&#39;test_error_iterations&#39;].append(output[&#39;test_error&#39;])

            if self.alternating_minimization_parameters[&#39;display&#39;]:
                print(&#39;\tAlt. min. iteration %s/%i: stagnation = %2.5e&#39; %
                      (str(iteration).
                       zfill(len(str(self.alternating_minimization_parameters
                                     [&#39;max_iterations&#39;]-1))),
                       self.alternating_minimization_parameters
                       [&#39;max_iterations&#39;]-1,
                       stagnation), end=&#39;&#39;)
                if &#39;error&#39; in output:
                    print(&#39;, error = %2.5e&#39; % output[&#39;error&#39;], end=&#39;&#39;)
                if self.test_error:
                    if not np.isscalar(output[&#39;test_error&#39;]):
                        output[&#39;test_error&#39;] = output[&#39;test_error&#39;].numpy()
                    print(&#39;, test error = %2.5e&#39; % output[&#39;test_error&#39;],
                          end=&#39;&#39;)
                print(&#39;&#39;)

            if iteration &gt; 0 and stagnation &lt; \
                    self.alternating_minimization_parameters[&#39;stagnation&#39;]:
                output[&#39;flag&#39;] = 1
                break

            if self.test_error and \
                    output[&#39;test_error&#39;] &lt; self.tolerance[&#39;on_error&#39;]:
                output[&#39;flag&#39;] = 1
                break

            if self.tree_adaptation and iteration &gt; 0:
                C_old = f.tensor.storage()
                self, f, output = self.adapt_tree(f, output[&#39;error&#39;],
                                                  None, output, iteration)
                adapted_tree = output[&#39;adapted_tree&#39;]
                if adapted_tree:
                    if self.display:
                        print(&#39;\t\tStorage complexity before permutation &#39; +
                              &#39;= %i&#39; % C_old)
                        print(&#39;\t\tStorage complexity after permutation &#39; +
                              &#39;= %i&#39; % f.tensor.storage())
                    if self.test_error:
                        f_eval_test = tensap.FunctionalTensor(
                            f, self.bases_eval_test)
                        if self.display:
                            print(&#39;\t\tTest error after permutation &#39; +
                                  &#39;= %2.5e&#39; % self.loss_function.test_error(
                                      f_eval_test, self.test_data))

                    if self.alternating_minimization_parameters[&#39;display&#39;]:
                        print(&#39;&#39;)

        if isinstance(self.bases, tensap.FunctionalBases):
            f = tensap.FunctionalTensor(f.tensor, self.bases)
        output[&#39;iter&#39;] = iteration

        if &#39;adapted_tree&#39; in output:
            del output[&#39;adapted_tree&#39;]

        if self.display and not self.model_selection:
            if self.alternating_minimization_parameters[&#39;display&#39;]:
                print(&#39;&#39;)
            self.final_display(f)
            if &#39;error&#39; in output:
                print(&#39;, error = %2.5e&#39; % output[&#39;error&#39;], end=&#39;&#39;)
            if &#39;test_error&#39; in output:
                print(&#39;, test error = %2.5e&#39; % output[&#39;test_error&#39;], end=&#39;&#39;)
            print(&#39;&#39;)

        return f, output

    def create_basis_adaptation_path_dmrg(self, ranks, alpha):
        tree = self.tree
        ranks = np.array(ranks)
        ranks[tree.root-1] = 1
        gamma = tree.parent(alpha)

        p_alpha = self.create_basis_adaptation_path(ranks, alpha)
        if p_alpha.ndim == 1:
            p_alpha = np.reshape(p_alpha, [-1, 1])
        p_alpha = p_alpha[:int(p_alpha.shape[0]/ranks[alpha-1]), :]

        ch = tree.children(gamma)
        p_gamma = [None]*len(ch)
        for nod in np.setdiff1d(ch, alpha):
            ind = tree.child_number(nod) &lt; tree.child_number(alpha)
            if self.is_active_node[nod-1]:
                p_gamma[tree.child_number(nod)-1+ind] = \
                    np.full((ranks[nod-1], 1), True)
            else:
                rep = np.nonzero(nod == tree.dim2ind)[0][0]
                p_gamma[tree.child_number(nod)-1+ind] = \
                    self.bases_adaptation_path[rep].astype(bool)
        p_gamma[0] = p_alpha

        r_gamma = ranks[gamma-1]
        p = p_gamma[-1]
        for i in np.arange(len(p_gamma)-2, -1, -1):
            p = np.kron(p, p_gamma[i])
        return np.tile(p, (r_gamma, 1)).astype(bool)

    def create_basis_adaptation_path_dmrg_low_rank(self, ranks, alpha):
        tree = self.tree
        ranks = np.array(ranks)
        ranks[tree.root-1] = 1
        gamma = tree.parent(alpha)

        p_alpha = self.create_basis_adaptation_path(ranks, alpha)
        if p_alpha.ndim == 1:
            p_alpha = np.reshape(p_alpha, [-1, 1])
        p_alpha = p_alpha[:int(p_alpha.shape[0]/ranks[alpha-1]), :]

        ch = np.setdiff1d(tree.children(gamma), alpha)
        p_gamma = [None]*len(ch)
        for nod in ch:
            ind = tree.child_number(nod) &gt; tree.child_number(alpha)
            if self.is_active_node[nod-1]:
                p_gamma[tree.child_number(nod)-1-ind] = \
                    np.full((ranks[nod-1], 1), True)
            else:
                rep = np.nonzero(nod == tree.dim2ind)[0][0]
                p_gamma[tree.child_number(nod)-1-ind] = \
                    self.bases_adaptation_path[rep].astype(bool)

        r_gamma = ranks[gamma-1]
        p = p_gamma[-1]
        for i in np.arange(len(p_gamma)-2, -1, -1):
            p = np.kron(p, p_gamma[i])
        p = np.tile(p, (r_gamma, 1)).astype(bool)

        return [p_alpha, p]

# %% Static methods
    @staticmethod
    def tensor_train(order, *args):
        &#39;&#39;&#39;
        Call of the constructor of the class TreeBasedTensorLearning, with a
        tree and active nodes corresponding to the Tensor-Train format in
        dimension order.

        See also TreeBasedTensorLearning.

        Parameters
        ----------
        order : int
            The order of the tensor.
        *args : tuple
            Additional parameters (see the constructor of
            TreeBasedTensorLearning).

        Returns
        -------
        TreeBasedTensorLearning
            The solver with a tree and active nodes associated with the
            Tensor-Train format.

        &#39;&#39;&#39;
        tree = tensap.DimensionTree.linear(order)
        is_active_node = np.full(tree.nb_nodes, True)
        is_active_node[tree.dim2ind[1:]-1] = False
        return TreeBasedTensorLearning(tree, is_active_node, *args)

    @staticmethod
    def tensor_train_tucker(order, *args):
        &#39;&#39;&#39;
        Call of the constructor of the class TreeBasedTensorLearning, with a
        tree and active nodes corresponding to the Tensor-Train Tucker format
        in dimension order.

        See also TreeBasedTensorLearning.

        Parameters
        ----------
        order : int
            The order of the tensor.
        *args : tuple
            Additional parameters (see the constructor of
            TreeBasedTensorLearning).

        Returns
        -------
        TreeBasedTensorLearning
            The solver with a tree and active nodes associated with the
            Tensor-Train Tucker format.

        &#39;&#39;&#39;
        tree = tensap.DimensionTree.linear(order)
        is_active_node = np.full(tree.nb_nodes, True)
        return TreeBasedTensorLearning(tree, is_active_node, *args)

    @staticmethod
    def enriched_edges_to_ranks_random(f, new_rank):
        &#39;&#39;&#39;
        Enrichment of the ranks of specified edges of the tensor f using random
        additions for each child / parent couple of the enriched edges.

        Parameters
        ----------
        f : tensap.TreeBasedTensor
            The tree-based tensor to enrich.
        new_rank : list or numpy.ndarray
            The new tree-based rank.

        Returns
        -------
        f : tensap.TreeBasedTensor
            The enriched tree-based tensor.

        &#39;&#39;&#39;
        f.is_orth = False
        tree = f.tree
        enriched_dims = np.nonzero(new_rank &gt; f.ranks)[0]

        for level in np.arange(1, np.max(tree.level)+1):
            nod_lvl = np.intersect1d(tree.nodes_with_level(level),
                                     enriched_dims)
            for alpha in nod_lvl:
                gamma = tree.parent(alpha)
                rank = new_rank[alpha-1] - f.ranks[alpha-1]

                A = np.reshape(f.tensors[alpha-1].data,
                               [-1, f.ranks[alpha-1]], order=&#39;F&#39;)
                A = np.hstack((A, np.tile(np.reshape(A[:, -1], [-1, 1]),
                                          [1, rank]) *
                               (1+np.random.randn(A.shape[0], rank))))
                A[:, -1-rank-1:] /= np.sqrt(np.sum(A[:, -1-rank-1:]**2, 0))
                shape = np.array(f.tensors[alpha-1].shape)
                shape[-1] += rank
                f.tensors[alpha-1].data = np.reshape(A, shape, order=&#39;F&#39;)

                ch = f.tree.child_number(alpha)-1
                ind = np.setdiff1d(range(f.tensors[gamma-1].order), ch)
                ind = np.concatenate((ind, [ch]))
                A = np.transpose(f.tensors[gamma-1].matricize(ch).data)
                A = np.hstack((A, np.tile(np.reshape(A[:, -1], [-1, 1]),
                                          [1, rank]) *
                               (1+np.random.randn(A.shape[0], rank))))
                A[:, -1-rank-1:] /= np.sqrt(np.sum(A[:, -1-rank-1:]**2, 0))
                shape = np.array(f.tensors[gamma-1].shape)
                shape[ch] += rank
                A = np.reshape(A, shape[ind], order=&#39;F&#39;)
                f.tensors[gamma-1].data = np.transpose(A, np.argsort(ind))

                f = f.update_attributes()
        return f

    @staticmethod
    def make_ranks_admissible(f, rank):
        &#39;&#39;&#39;
        Adjustment of the ranks to make the associated tree-based tensor f
        rank-admissible, by enriching new edges associated with nodes of the
        tree until all the rank admissibility conditions are met.

        Parameters
        ----------
        f : tensap.TreeBasedTensor
            The current approximation in tree-based tensor format.
        rank : numpy.ndarray
            The proposed tree-based rank.

        Returns
        -------
        rank : numpy.ndarray
            The admissible tree-based rank (if possible).
        d : numpy.ndarray
            The enriched nodes.

        &#39;&#39;&#39;
        # Do not increase the ranks of leaf nodes with rank equal to the
        # dimension of the approximation space.
        nodes = f.active_nodes
        ind = [f.tree.is_leaf[x] and rank[x] &gt; f.tensors[x].shape[0] for
               x in nodes-1]
        rank[nodes[ind]-1] = [x.shape[0] for x in f.tensors[nodes[ind]-1]]
        rank[np.logical_not(f.is_active_node)] = 0

        delta = rank - f.ranks
        if f.is_admissible_rank(f.ranks + delta):
            rank = f.ranks + delta
            d = np.nonzero(delta)[0] + 1
            return rank, d

        ind = np.nonzero(delta)[0]
        for i in np.arange(1, np.count_nonzero(delta)+1):
            pos = list(combinations(np.arange(np.count_nonzero(delta)), i))
            shuffle(pos)
            for pos_loc in pos:
                delta_loc = np.array(delta)
                delta_loc[ind[list(pos_loc)]] = 0
                if f.is_admissible_rank(f.ranks + delta_loc):
                    rank = f.ranks + delta_loc
                    d = np.nonzero(delta_loc)[0] + 1
                    return rank, d
        return rank, []

    @staticmethod
    def unique_tol(inp, tol):
        &#39;&#39;&#39;
        Unique values within tolerance, with sorted output.

        Parameters
        ----------
        inp : list of numpy.ndarray
            The values to be checked.
        tol : float
            The tolerance defining the uniqueness.

        Returns
        -------
        bout : numpy.ndarray
            The unique values of inp, within the tolerance tol.

        &#39;&#39;&#39;
        inp = np.sort(inp)
        out = [inp[0]]

        for ind in np.arange(1, inp.size):
            if np.abs(out[-1] - inp[ind]) / out[-1] &gt; tol:
                out = np.concatenate((out, [inp[ind]]))
        return out</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning"><code class="flex name class">
<span>class <span class="ident">TreeBasedTensorLearning</span></span>
<span>(</span><span>tree, is_active_node, *args)</span>
</code></dt>
<dd>
<div class="desc"><p>Class TreeBasedTensorLearning.</p>
<p>See also tensap.TensorLearning.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>tree</code></strong> :&ensp;<code>tensap.DimensionTree</code></dt>
<dd>The dimension tree of the tree-based tensor.</dd>
<dt><strong><code>is_active_node</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>Booleans indicating if each node is active.</dd>
</dl>
<p>Constructor for the class TreeBasedTensorLearning.</p>
<p>See also tensap.TensorLearning.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>tree</code></strong> :&ensp;<code>tensap.DimensionTree</code></dt>
<dd>The dimension tree of the tree-based tensor.</dd>
<dt><strong><code>is_active_node</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>Booleans indicating if each node is active.</dd>
<dt><strong><code>*args</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Additional parameters.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TreeBasedTensorLearning(tensap.TensorLearning):
    &#39;&#39;&#39;
    Class TreeBasedTensorLearning.

    See also tensap.TensorLearning.

    Attributes
    ----------
    tree : tensap.DimensionTree
        The dimension tree of the tree-based tensor.
    is_active_node : list or numpy.ndarray
        Booleans indicating if each node is active.

    &#39;&#39;&#39;

    def __init__(self, tree, is_active_node, *args):
        &#39;&#39;&#39;
        Constructor for the class TreeBasedTensorLearning.

        See also tensap.TensorLearning.

        Parameters
        ----------
        tree : tensap.DimensionTree
            The dimension tree of the tree-based tensor.
        is_active_node : list or numpy.ndarray
            Booleans indicating if each node is active.
        *args : tuple
            Additional parameters.

        Returns
        -------
        None.

        &#39;&#39;&#39;
        super().__init__(*args)

        self.tree = tree
        self.is_active_node = list(map(bool, is_active_node))
        self.order = tree.dim2ind.size
        self._number_of_parameters = np.count_nonzero(is_active_node)

        self.initialization_type = &#39;canonical&#39;
        self.rank_adaptation_options[&#39;rank_one_correction&#39;] = True
        self.rank_adaptation_options[&#39;theta&#39;] = 0.8
        self.linear_model_learning_parameters[
            &#39;basis_adaptation_internal_nodes&#39;] = False

# %% Standard solver methods
    def initialize(self):
        assert self.tree is not None, \
            &#39;Must provide a DimensionTree object in property tree.&#39;

        if np.isscalar(self.rank) or len(self.rank) == self.order:
            rank = np.zeros(self.tree.nb_nodes, dtype=int)
            rank[self.is_active_node] = self.rank
            self.rank = rank
        self.rank[self.tree.root-1] = self.output_dimension

        shape = [x.shape[1] for x in self.bases_eval]
        if self.initialization_type == &#39;random&#39;:
            f = tensap.TreeBasedTensor.randn(self.tree, self.rank, shape,
                                             self.is_active_node)
        elif self.initialization_type == &#39;ones&#39;:
            f = tensap.TreeBasedTensor.ones(self.tree, self.rank, shape,
                                            self.is_active_node)
        elif self.initialization_type == &#39;initial_guess&#39;:
            f = self.initial_guess
            if not np.all(f.ranks == self.rank):
                tr = tensap.Truncator(tolerance=np.finfo(float).eps,
                                      max_rank=self.rank)
                f = tr.truncate(f)
        elif self.initialization_type == &#39;mean&#39; or \
                self.initialization_type == &#39;mean_randomized&#39;:
            if not np.all(self.rank == 1):
                raise NotImplementedError(&#39;Initialization only implemented &#39; +
                                          &#39;if np.all(self.rank == 1).&#39;)
            if not isinstance(self.training_data, list) or \
                    (isinstance(self.training_data, list) and
                     len(self.training_data) == 1):
                raise NotImplementedError(&#39;Initialization type not &#39; +
                                          &#39;implemented in unsupervised &#39; +
                                          &#39;learning.&#39;)
            if isinstance(self.bases, tensap.FunctionalBases):
                means = self.bases.mean()
            else:
                means = [np.mean(x) for x in self.bases_eval]
            if self.initialization_type == &#39;mean_randomized&#39;:
                means = [x + 0.01*np.random.randn(*x.shape) for x in means]
            means = [tensap.FullTensor(x, 2, [x.shape[0], 1]) for x in means]

            f = tensap.TreeBasedTensor.ones(self.tree, self.rank, shape)
            f.tensors[self.tree.dim2ind-1] = means
            nb_child = len(self.tree.children(self.tree.root))
            if np.ndim(self.training_data[1]) == 2:
                shape = np.concatenate((np.full(nb_child, 1),
                                       [self.training_data[1].shape[1]]))
            else:
                shape = np.full(nb_child, 1)
            f.tensors[self.tree.root-1] = \
                tensap.FullTensor(np.mean(self.training_data[1]), shape=shape)
            f.update_attributes()

            f = f.inactivate_nodes(
                np.nonzero(np.logical_not(self.is_active_node))[0]+1)
        elif self.initialization_type == &#39;canonical&#39;:
            if self.output_dimension != 1:
                print(&#39;Canonical initialization not implemented for &#39; +
                      &#39;outputDimension &gt; 1, performing a random &#39; +
                      &#39;initialization.&#39;)
                f = tensap.TreeBasedTensor.randn(self.tree, self.rank, shape,
                                                 self.is_active_node)
            else:
                f = self.canonical_initialization(np.max(self.rank))
                if not np.all(f.ranks == self.rank):
                    tr = tensap.Truncator(tolerance=np.finfo(float).eps,
                                          max_rank=self.rank)
                    f = tr.truncate(f)
        else:
            raise ValueError(&#39;Wrong initialization type.&#39;)

        if not np.all(f.ranks == self.rank):
            f = TreeBasedTensorLearning.\
                enriched_edges_to_ranks_random(f, self.rank)

        # Exploration strategy of the tree by increasing level
        tree = f.tree
        exploration_strategy = np.zeros(self._number_of_parameters, dtype=int)
        active_nodes = f.active_nodes
        rep = 0
        for level in range(np.max(tree.level)+1):
            nodes = np.intersect1d(tree.nodes_with_level(level), active_nodes)
            exploration_strategy[rep:rep+len(nodes)] = nodes
            rep += len(nodes)
        self._exploration_strategy = exploration_strategy
        return self, f

    def pre_processing(self, f):
        if isinstance(self.linear_model_learning, (list, np.ndarray)) and \
                len(self.linear_model_learning) != f.tensor.tree.nb_nodes:
            tmp = np.empty(f.tensor.tree.nb_nodes, dtype=object)
            tmp[f.tensor.is_active_node] = self.linear_model_learning
            self.linear_model_learning = tmp
        return self, f

    def randomize_exploration_strategy(self):
        strategy = np.zeros(self._number_of_parameters)
        for level in np.arange(np.max(self.tree.level), -1, -1):
            active_nodes = np.intersect1d(self.tree.nodes_with_level(level),
                                          np.nonzero(self.is_active_node)[0])
            _, ind = np.intersect1d(self._exploration_strategy,
                                    active_nodes,
                                    return_indices=True)
            strategy[ind] = self._exploration_strategy[
                np.random.permutation(ind)]
        return strategy

    def prepare_alternating_minimization_system(self, f, mu):
        tree = f.tensor.tree
        if self.linear_model_learning[mu-1].basis_adaptation:
            if np.isin(mu, tree.internal_nodes):
                if self.linear_model_learning_parameters[
                        &#39;basis_adaptation_internal_nodes&#39;]:
                    tr = tensap.Truncator(tolerance=np.finfo(float).eps,
                                          max_rank=np.max(f.tensor.ranks))
                    f.tensor = tr.hsvd(f.tensor)
                elif np.all(f.tensor.is_active_node[tree.children(mu)-1]):
                    self.linear_model_learning[mu-1].basis_adaptation = False
            f.tensor = f.tensor.orth_at_node(mu)
            self.tree = tree
            self.is_active_node = f.tensor.is_active_node
            self.linear_model_learning[mu-1].basis_adaptation_path = \
                self.create_basis_adaptation_path(f.tensor.ranks, mu)
        else:
            f.tensor = f.tensor.orth_at_node(mu)

        grad = f.parameter_gradient_eval(mu)
        if mu == tree.root:
            A = np.reshape(grad.data, [grad.shape[0], -1], order=&#39;F&#39;)
            self.linear_model_learning[mu-1].initial_guess = np.reshape(
                    f.tensor.tensors[mu-1].data,
                    [-1, f.tensor.ranks[tree.root-1]], order=&#39;F&#39;)
        else:
            A = np.reshape(grad.data, [grad.shape[0], -1,
                                       f.tensor.ranks[tree.root-1]], order=&#39;F&#39;)
            self.linear_model_learning[mu-1].initial_guess = np.reshape(
                    f.tensor.tensors[mu-1].data, -1, order=&#39;F&#39;)

        if isinstance(self.loss_function, tensap.DensityL2LossFunction):
            if isinstance(self.training_data, list) and \
                    len(self.training_data) == 2:
                y = self.training_data[1]
                if isinstance(y, tensap.FunctionalTensor):
                    y = y.tensor
                y = y.orth()
                if tree.is_leaf[mu-1]:
                    a = deepcopy(y)
                    for nod in tree.internal_nodes:
                        a.tensors[nod-1] = tensap.FullTensor(
                            y.tensors[nod-1].data *
                            f.tensor.tensors[nod-1].data,
                            shape=y.tensors[nod-1].shape)
                    ind = np.setdiff1d(range(self.order),
                                       np.nonzero(tree.dim2ind == mu)[0])
                    C = [x.data for x in f.tensor.tensors[tree.dim2ind-1]]
                    b = a.tensor_vector_product([C[x] for x in ind], ind)
                    b = b.tensors[0].data
                else:
                    b = f.tensor.dot(y) / f.tensor.tensors[mu-1].data
            else:
                b = []
        elif isinstance(self.training_data, list) and \
                len(self.training_data) == 2:
            b = self.training_data[1]
            self.linear_model_learning[mu-1].shared_coefficients = \
                mu != tree.root

        return self, A, b, f

    def set_parameter(self, f, mu, coef):
        f.tensor.tensors[mu-1] = \
            tensap.FullTensor(coef, shape=f.tensor.tensors[mu-1].shape)
        f.tensor.tensors[mu-1].is_orth = False
        return f

    def stagnation_criterion(self, f, f0):
        return (f.tensor - f0.tensor).norm() / f0.tensor.norm()

    def final_display(self, f):
        print(&#39;Ranks = [%s]&#39; % &#39;, &#39;.join(map(str, f.tensor.ranks)), end=&#39;&#39;)

    def canonical_initialization(self, rank):
        &#39;&#39;&#39;
        Rank-r canonical initialization.

        Parameters
        ----------
        rank : int
            The rank of the canonical initialization.

        Returns
        -------
        tensap.TreeBasedTensor
            The rank-r canonical initialization..

        &#39;&#39;&#39;
        solver = tensap.CanonicalTensorLearning(self.order, self.loss_function)
        if isinstance(self.linear_model_learning, list):
            solver.linear_model_learning = self.linear_model_learning[0]
        else:
            solver.linear_model_learning = self.linear_model_learning
        solver.alternating_minimization_parameters = \
            deepcopy(self.alternating_minimization_parameters)
        solver.tolerance[&#39;on_stagnation&#39;] = np.finfo(float).eps
        solver.tolerance[&#39;on_error&#39;] = np.finfo(float).eps
        solver.bases = self.bases
        solver.bases_eval = self.bases_eval
        solver.bases_eval_test = self.bases_eval_test
        solver.display = False
        solver.alternating_minimization_parameters[&#39;display&#39;] = False
        solver.initialization_type = &#39;mean&#39;
        solver.rank_adaptation = True
        solver.rank_adaptation_options[&#39;max_iterations&#39;] = rank
        solver.bases_adaptation_path = self.bases_adaptation_path
        solver.test_error = self.test_error
        solver.training_data = self.training_data
        solver.test_data = self.test_data
        solver._warnings = self._warnings

        f = solver.solve()[0]
        return f.tensor.tree_based_tensor(self.tree, self.is_active_node)

    def canonical_correction(self, f, rank):
        &#39;&#39;&#39;
        Rank-r canonical correction.

        Parameters
        ----------
        f : tensap.FunctionalTensor or None
            The current approximation.
        rank : int
            The rank of the canonical correction.

        Raises
        ------
        NotImplementedError
            If the method is not implemented.

        Returns
        -------
        f : tensap.FunctionalTensor
            The corrected approximation.

        &#39;&#39;&#39;
        if isinstance(f, tensap.FunctionalTensor):
            fx = f.tensor.tensor_matrix_product_eval_diag(self.bases_eval).data
        elif f is None:
            fx = 0
        else:
            raise NotImplementedError(&#39;Not implemented.&#39;)

        solver = deepcopy(self)
        if isinstance(solver.training_data, list) and \
                len(solver.training_data) == 2:
            solver.training_data[1] -= fx
        elif isinstance(solver.loss_function, tensap.DensityL2LossFunction):
            solver.training_data = [solver.training_data, f]

        f_add = solver.canonical_initialization(rank)
        if isinstance(f_add, tensap.FunctionalTensor):
            f_add = f_add.tensor
        if f is not None:
            f = f.tensor + f_add
        else:
            f = f_add
        return f

    def rank_one_correction(self, f):
        &#39;&#39;&#39;
        Rank one correction.

        Parameters
        ----------
        f : tensap.FunctionalTensor or None
            The current approximation.

        Raises
        ------
        NotImplementedError
            If the method is not implemented.

        Returns
        -------
        f : tensap.FunctionalTensor
            The corrected approximation.

        &#39;&#39;&#39;
        if isinstance(f, tensap.FunctionalTensor):
            fx = f.tensor.tensor_matrix_product_eval_diag(self.bases_eval).data
        elif f is None:
            fx = 0
        else:
            raise NotImplementedError(&#39;Not implemented.&#39;)

        solver = deepcopy(self)
        solver.model_selection = False
        if isinstance(solver.training_data, list) and \
                len(solver.training_data) == 2:
            solver.training_data[1] -= fx
        elif isinstance(solver.loss_function, tensap.DensityL2LossFunction):
            solver.training_data = [solver.training_data, f]
        solver.rank_adaptation = False
        solver.tree_adaptation = False
        solver.rank = 1
        solver.display = False
        solver.alternating_minimization_parameters[&#39;display&#39;] = False
        solver.initialization_type = &#39;ones&#39;
        solver.alternating_minimization_parameters[&#39;max_iterations&#39;] = 1

        f_add = solver.solve()[0]
        if isinstance(f_add, tensap.FunctionalTensor):
            f_add = f_add.tensor
        if f is not None:
            f = f.tensor + f_add
        else:
            f = f_add
        return f

    def create_basis_adaptation_path(self, ranks, alpha):
        &#39;&#39;&#39;
        Creation of the basis adaptation path.

        Parameters
        ----------
        rank : list or numpy.ndarray
            The alpha-ranks of the current approximation.
        alpha : int
            The current node.

        Returns
        -------
        path : numpy.ndarray
            The basis adaptation path.

        &#39;&#39;&#39;
        tree = self.tree
        ranks = np.array(ranks)
        ranks[tree.root-1] = 1
        if tree.is_leaf[alpha-1]:
            path_alpha = self.bases_adaptation_path[
                np.nonzero(tree.dim2ind == alpha)[0][0]].astype(bool)
            ranks = ranks[alpha-1]
            path = path_alpha[np.newaxis, :, np.newaxis, :]
            path = np.tile(path, [1, 1, ranks, 2])
            path = np.reshape(path, [path.shape[1]*ranks, -1], order=&#39;F&#39;)
        else:
            assert not self.linear_model_learning_parameters[
                &#39;basis_adaptation_internal_nodes&#39;], \
                &#39;Basis adaptation for internal nodes is not implemented.&#39;
            ch = tree.children(alpha)
            if np.all(self.is_active_node[ch-1]):
                ch_a = ch[self.is_active_node[ch-1]]
                path = np.full(np.prod(ranks[ch_a-1]) * ranks[alpha-1], True)
            else:
                path_alpha = []
                for nod in ch:
                    if self.is_active_node[nod-1]:
                        path_alpha.append(np.full((ranks[nod-1], 1), True))
                    else:
                        ind = np.nonzero(nod == tree.dim2ind)[0][0]
                        path_alpha.append(
                            self.bases_adaptation_path[ind].astype(bool))
                path = path_alpha[-1]
                for ind in np.arange(len(path_alpha)-2, -1, -1):
                    path = np.kron(path, path_alpha[ind])
                path = np.tile(path, [ranks[alpha-1], 1]).astype(bool)
        return path

# %% Rank adaptation solver methods
    def local_solver(self):
        s_local = deepcopy(self)
        s_local.display = False
        s_local.rank_adaptation = False
        s_local.store_iterates = False
        s_local.test_error = False
        s_local.model_selection = False
        return s_local

    def new_rank_selection(self, f):
        if self.rank_adaptation_options[&#39;rank_one_correction&#39;]:
            s_local = deepcopy(self)
            ranks_add = np.ones(f.tensor.tree.nb_nodes, dtype=int)
            ranks_add[f.tensor.tree.root-1] = 0
            ranks_add[f.tensor.non_active_nodes-1] = 0
            s_local.rank = TreeBasedTensorLearning.make_ranks_admissible(
                f.tensor, f.tensor.ranks + ranks_add)[0]
            s_local.initialization_type = &#39;initial_guess&#39;
            tr = tensap.Truncator(tolerance=0, max_rank=s_local.rank)
            s_local.initial_guess = tr.truncate(self.rank_one_correction(f))
            s_local.alternating_minimization_parameters[&#39;max_iterations&#39;] = 10
            s_local.model_selection = False
            s_local.rank_adaptation = False
            s_local.display = False
            s_local.alternating_minimization_parameters[&#39;display&#39;] = False
            tensor_for_selection = s_local.solve()[0].tensor
        else:
            tensor_for_selection = f.tensor

        sin_val = tensor_for_selection.singular_values()

        # Remove from the rank adaptation candidates: the inactive nodes, the
        # root, the leaf nodes with a rank equal to the dimension of the basis
        # associated to it, and the nodes for  which the smallest singular
        # value is almost zero.
        sin_val = np.array([np.nan if x is None else x for x in sin_val])
        sin_val[f.tensor.tree.root-1] = np.nan
        dim2ind = np.intersect1d(f.tensor.tree.dim2ind, f.tensor.active_nodes)
        ind = [len(set(x.size)) == 1 for x in f.tensor.tensors[dim2ind-1]]
        sin_val[dim2ind[ind]-1] = np.nan
        sin_val[s_local.rank != tensor_for_selection.ranks] = np.nan

        sin_val_min = np.array([np.min(x) for x in sin_val])
        sin_val_min[[x / tensor_for_selection.norm() &lt; np.finfo(float).eps if
                     not np.isnan(x) else False for x in sin_val_min]] = np.nan

        # Remove nodes that cannot be enriched because their rank is equal to
        # the product of the ranks of their children, and their children cannot
        # be enriched themselves.
        tree = tensor_for_selection.tree
        rank = f.tensor.ranks
        desc = np.setdiff1d(np.arange(1, tree.nb_nodes+1),
                            np.nonzero(tree.is_leaf)[0]+1)
        cannot_be_increased = np.full(tree.nb_nodes, False)
        cannot_be_increased[tree.root-1] = True
        cannot_be_increased[tree.is_leaf] = np.isnan(sin_val_min[tree.is_leaf])
        for level in np.arange(np.max(tree.level), 0, -1):
            nod_lvl = np.intersect1d(tree.nodes_with_level(level), desc)
            for nod in nod_lvl:
                ch = tree.children(nod)
                if np.all(cannot_be_increased[ch-1]) and \
                        rank[nod-1] == np.prod(rank[ch-1]):
                    cannot_be_increased[nod-1] = True
        cannot_be_increased_nodes = tree.nodes_indices[cannot_be_increased]
        for level in np.arange(1, np.max(tree.level)):
            nod_lvl = np.intersect1d(tree.nodes_with_level(level),
                                     cannot_be_increased_nodes)
            for nod in nod_lvl:
                pa = tree.parent(nod)
                ind = np.setdiff1d(tree.children(pa), nod)
                ind = np.concatenate(([pa], ind))
                if np.all(cannot_be_increased[ind-1]) and \
                        rank[nod-1] == np.prod(rank[ind-1]):
                    cannot_be_increased[nod-1] = True
        sin_val_min[cannot_be_increased] = np.nan

        if np.all(np.isnan(sin_val_min)):
            enriched_nodes = np.array([], dtype=int)
        else:
            theta = self.rank_adaptation_options[&#39;theta&#39;] * \
                np.nanmax(sin_val_min)
            enriched_nodes = np.nonzero([not np.isnan(x) and x &gt;= theta for
                                         x in sin_val_min])[0] + 1

        new_rank = np.array(f.tensor.ranks)
        new_rank[enriched_nodes-1] += 1

        if not f.tensor.is_admissible_rank(new_rank):
            # Add to the already enriched nodes nodes one by one in decreasing
            # order of singular value until the rank is admissible.
            enriched_nodes_theta = np.array(enriched_nodes)
            rank_theta = np.array(new_rank)
            sin_val_min[enriched_nodes_theta-1] = np.nan
            sin_val_min_sorted = TreeBasedTensorLearning.unique_tol(
                sin_val_min, 1e-2)
            sin_val_min_sorted = np.flip(sin_val_min_sorted)
            sin_val_min_sorted = sin_val_min_sorted[
                np.isfinite(sin_val_min_sorted)]

            for sv in sin_val_min_sorted:
                new_rank = np.array(rank_theta)
                ind = [x &gt;= sv if not np.isnan(x) else False for
                       x in sin_val_min]
                new_rank[ind] += 1
                if f.tensor.is_admissible_rank(new_rank):
                    enriched_nodes = np.concatenate((enriched_nodes_theta,
                                                     np.nonzero(ind)[0]+1))
                    break
            if not f.tensor.is_admissible_rank(new_rank):
                new_rank = f.tensor.ranks
                enriched_nodes = np.array([])

        return new_rank, enriched_nodes, tensor_for_selection

    def initial_guess_new_rank(self, s_local, f, new_rank):
        s_local.initialization_type = &#39;initial_guess&#39;
        if not np.all(f.ranks == new_rank):
            tr = tensap.Truncator(tolerance=0, max_rank=new_rank)
            s_local.initial_guess = tr.truncate(f)
        else:
            s_local.initial_guess = f
        return s_local

    def adaptation_display(self, f, enriched_nodes):
        print(&#39;\tEnriched nodes: [%s]\n\tRanks = [%s]&#39; %
              (&#39;, &#39;.join(map(str, enriched_nodes)),
               &#39;, &#39;.join(map(str, f.tensor.ranks))))

    def adapt_tree(self, f, cv_error, test_error, output, *args):
        if not self.tree_adaptation:
            return self, f, output

        output[&#39;adapted_tree&#39;] = False

        if np.any(f.tensor.ranks[f.tensor.active_nodes-1] == 0):
            print(&#39;Some ranks equal to 0, disabling tree adaptation for &#39; +
                  &#39;this step.&#39;)
            return self, f, output
        if self.tree_adaptation_options[&#39;tolerance&#39;] is not None:
            adapt_tree_error = self.tree_adaptation_options[&#39;tolerance&#39;]
        elif self.loss_function.error_type == &#39;relative&#39;:
            if test_error is None or test_error == 0:
                adapt_tree_error = cv_error
            elif cv_error is None or test_error != 0:
                adapt_tree_error = test_error
        else:
            print(&#39;Must provide a tolerance for the tree adaptation in &#39; +
                  &#39;the treeAdaptationOptions property. Disabling tree &#39; +
                  &#39;adaptation.&#39;)
            self.tree_adaptation = False
            return self, f, output

        f_perm = f.tensor.optimize_dimension_tree(
            adapt_tree_error, self.tree_adaptation_options[&#39;max_iterations&#39;])
        if f_perm.storage() &lt; f.tensor.storage():
            f.tensor = f_perm
            self.tree = f.tensor.tree
            self.is_active_node = f.tensor.is_active_node
            output[&#39;adapted_tree&#39;] = True
            if self.display:
                print(&#39;\tTree adaptation:\n\t\tRanks after permutation &#39; +
                      &#39;= [%s]&#39; % &#39;, &#39;.join(map(str, f.tensor.ranks)))
        return self, f, output

# %% Inner rank adaptation solver
    def _solve_dmrg_rank_adaptation(self):
        if &#39;max_rank&#39; not in self.rank_adaptation_options:
            self.rank_adaptation_options[&#39;max_rank&#39;] = 100
        if &#39;post_alternating_minimization&#39; not in self.rank_adaptation_options:
            self.rank_adaptation_options[&#39;post_alternating_minimization&#39;] = \
                False
        if self.rank_adaptation_options[&#39;type&#39;] == &#39;dmrg_low_rank&#39; and \
                &#39;model_selection_type&#39; not in self.rank_adaptation_options:
            self.rank_adaptation_options[&#39;model_selection_type&#39;] = &#39;cv_error&#39;

        if self.display:
            self.alternating_minimization_parameters[&#39;display&#39;] = True

        output = {&#39;flag&#39;: 0}

        self, f = self.initialize()
        f = tensap.FunctionalTensor(f, self.bases_eval)

        # Exploration strategy of the tree by decreasing level
        tree = f.tensor.tree
        exploration_strategy = np.zeros(self._number_of_parameters, dtype=int)
        active_nodes = f.tensor.active_nodes
        rep = 0
        for level in np.arange(np.max(tree.level), -1, -1):
            nodes = np.intersect1d(tree.nodes_with_level(level), active_nodes)
            exploration_strategy[rep:rep+len(nodes)] = nodes
            rep += len(nodes)
        self._exploration_strategy = np.setdiff1d(exploration_strategy,
                                                  tree.root,
                                                  assume_unique=True)

        # Replication of the LinearModelLearning objects
        if self.linear_model_learning_parameters[
                &#39;identical_for_all_parameters&#39;] and \
                not isinstance(self.linear_model_learning, (list, np.ndarray)):
            self.linear_model_learning = list(map(deepcopy, [
                self.linear_model_learning] * self._number_of_parameters))
        elif isinstance(self.linear_model_learning, (list, np.ndarray)) and \
                len(self.linear_model_learning) != self._number_of_parameters:
            raise ValueError(&#39;Must provide self._numberOfParameters &#39; +
                             &#39;LinearModelLearning objects.&#39;)

        if self.error_estimation:
            for x in self.linear_model_learning:
                setattr(x, &#39;error_estimation&#39;, True)

        # Working set paths
        if isinstance(self.linear_model_learning, (list, np.ndarray)) and \
            np.any([x.basis_adaptation for
                    x in self.linear_model_learning]) and \
                self.bases_adaptation_path is None:
            self.bases_adaptation_path = self.bases.adaptation_path()

        if self.alternating_minimization_parameters[&#39;max_iterations&#39;] == 0:
            return f, output

        output[&#39;stagnation_criterion&#39;] = []
        output[&#39;iterates&#39;] = []
        output[&#39;error_iterations&#39;] = []
        output[&#39;test_error_iterations&#39;] = []

        # Alternating minimization loop
        for iteration in range(self.alternating_minimization_parameters
                               [&#39;max_iterations&#39;]):
            self, f = self.pre_processing(f)
            f0 = deepcopy(f)

            if self.alternating_minimization_parameters[&#39;random&#39;]:
                # Randomize the exploration strategy
                alpha_list = self.randomize_exploration_strategy()
            else:
                alpha_list = self._exploration_strategy

            tree = f.tensor.tree
            for alpha in alpha_list:
                if self.linear_model_learning[alpha-1].basis_adaptation:
                    if np.isin(alpha, tree.internal_nodes):
                        if self.linear_model_learning_parameters[
                                &#39;basis_adaptation_internal_nodes&#39;]:
                            tr = tensap.Truncator(
                                tolerance=np.finfo(float).eps,
                                max_rank=np.max(f.tensor.ranks))
                            f.tensor = tr.hsvd(f.tensor)
                        elif np.all(f.tensor.is_active_node[
                                tree.children(alpha)-1]):
                            self.linear_model_learning[alpha-1].\
                                basis_adaptation = False
                    f.tensor = f.tensor.orth_at_node(tree.parent(alpha))
                    self.tree = tree
                    self.is_active_node = f.tensor.is_active_node

                    if self.rank_adaptation_options[&#39;type&#39;] == &#39;dmrg&#39;:
                        self.linear_model_learning[alpha-1].\
                            basis_adaptation_path = \
                            self.create_basis_adaptation_path_dmrg(
                                f.tensor.ranks, alpha)
                    elif self.rank_adaptation_options[&#39;type&#39;] == \
                            &#39;dmrg_low_rank&#39;:
                        self.linear_model_learning[alpha-1].\
                            basis_adaptation_path = self.\
                            create_basis_adaptation_path_dmrg_low_rank(
                                f.tensor.ranks, alpha)
                    else:
                        raise ValueError(&#39;Wrong rank adaptation type.&#39;)
                else:
                    f.tensor = f.tensor.orth_at_node(tree.parent(alpha))

                grad = f.parameter_gradient_eval_dmrg(
                    alpha, dmrg_type=self.rank_adaptation_options[&#39;type&#39;])

                if isinstance(self.loss_function,
                              tensap.DensityL2LossFunction):
                    if isinstance(self.training_data, list) and \
                            len(self.training_data) == 2:
                        y = self.training_data[1]
                        if isinstance(y, tensap.FunctionalTensor):
                            y = y.tensor
                        y = y.orth()
                        if tree.is_leaf[alpha-1]:
                            a = deepcopy(y)
                            for nod in tree.internal_nodes:
                                a.tensors[nod-1] = tensap.FullTensor(
                                    y.tensors[nod-1].data *
                                    f.tensor.tensors[nod-1].data,
                                    shape=y.tensors[nod-1].shape)
                            ind = np.setdiff1d(
                                range(self.order),
                                np.nonzero(tree.dim2ind == alpha)[0])
                            C = [x.data for
                                 x in f.tensor.tensors[tree.dim2ind-1]]
                            b = a.tensor_vector_product([C[x] for x in ind],
                                                        ind)
                            b = b.tensors[0].data
                        else:
                            b = f.tensor.dot(y) / \
                                f.tensor.tensors[alpha-1].data
                    else:
                        b = []
                elif isinstance(self.training_data, list) and \
                        len(self.training_data) == 2:
                    b = self.training_data[1]
                    self.linear_model_learning[alpha-1].shared_coefficients = \
                        alpha != tree.root

                gamma = tree.parent(alpha)
                ind = np.setdiff1d(
                    np.arange(1, f.tensor.tensors[gamma-1].order+1),
                    tree.child_number(alpha))

                if self.rank_adaptation_options[&#39;type&#39;] == &#39;dmrg&#39;:
                    A = np.reshape(grad.data, (grad.shape[0], -1), order=&#39;F&#39;)

                    self.linear_model_learning[alpha-1].training_data = [None,
                                                                         b]
                    self.linear_model_learning[alpha-1].basis = None
                    self.linear_model_learning[alpha-1].basis_eval = A

                    C, output_tmp = \
                        self.linear_model_learning[alpha-1].solve()

                    if C is None or np.count_nonzero(C) == 0 or \
                            not np.all(np.isfinite(C)):
                        print(&#39;Empty, zero or NaN solution, returning to &#39; +
                              &#39;the previous iteration.&#39;)
                        output[&#39;flag&#39;] = -2
                        output[&#39;error&#39;] = np.inf
                        break

                    sz_1 = np.prod(f.tensor.tensors[alpha-1].shape[:-1])
                    sz_2 = np.prod(f.tensor.tensors[gamma-1].shape[ind-1])
                    C = tensap.FullTensor(C, 2, [sz_1, sz_2])

                    tr = tensap.Truncator()
                    tr.tolerance = self.tolerance[&#39;on_error&#39;] / \
                        np.sqrt(np.count_nonzero(f.tensor.is_active_node)-1)
                    tr.max_rank = self.rank_adaptation_options[&#39;max_rank&#39;]
                    C = tr.truncate(C)
                    rank = [np.shape(C.space[0])[1]]

                    sz_1 = np.concatenate(
                        (f.tensor.tensors[alpha-1].shape[:-1], rank))
                    sz_2 = np.concatenate(
                        (f.tensor.tensors[gamma-1].shape[ind-1], rank))

                    a_alpha = np.reshape(C.space[0], sz_1, order=&#39;F&#39;)
                    a_gamma = np.reshape(C.space[1]*np.tile(
                        np.reshape(C.core.data, [1, -1]),
                        (np.shape(C.space[1])[0], 1)), sz_2, order=&#39;F&#39;)
                    perm = np.concatenate((ind, [tree.child_number(alpha)]))
                    a_gamma = np.transpose(a_gamma, np.argsort(perm-1))
                    sz_2[perm-1] = np.array(sz_2)
                elif self.rank_adaptation_options[&#39;type&#39;] == &#39;dmrg_low_rank&#39;:
                    A = [np.reshape(x.data, [x.shape[0], -1], order=&#39;F&#39;) for
                         x in grad]

                    s_local = TreeBasedTensorLearning(
                        tensap.DimensionTree.trivial(2), [True, True, False],
                        self.loss_function)
                    s_local.rank_adaptation = True
                    s_local.tolerance[&#39;on_error&#39;] = \
                        self.tolerance[&#39;on_error&#39;] / \
                        np.sqrt(np.count_nonzero(f.tensor.is_active_node)-1)
                    s_local.tolerance[&#39;on_stagnation&#39;] = \
                        self.tolerance[&#39;on_stagnation&#39;]
                    s_local.rank_adaptation_options[&#39;max_iterations&#39;] = \
                        self.rank_adaptation_options[&#39;max_rank&#39;]
                    s_local.alternating_minimization_parameters = \
                        deepcopy(self.alternating_minimization_parameters)
                    s_local.alternating_minimization_parameters[&#39;display&#39;] = \
                        False
                    s_local.store_iterates = True
                    s_local.test_error = False
                    s_local.error_estimation = True
                    s_local.display = False
                    s_local.order = 2
                    s_local.linear_model_learning = \
                        self.linear_model_learning[alpha-1]
                    s_local._warnings[&#39;orthonormality_warning_display&#39;] = False
                    s_local._warnings[&#39;empty_bases_warning_display&#39;] = False
                    s_local.bases_adaptation_path = \
                        self.linear_model_learning[alpha-1].\
                        basis_adaptation_path
                    s_local.training_data = self.training_data
                    s_local.bases_eval = A
                    s_local.model_selection = True
                    s_local.model_selection_options[&#39;type&#39;] = \
                        self.rank_adaptation_options[&#39;model_selection_type&#39;]
                    C, output_tmp = s_local.solve()

                    rank = [C.tensor.ranks[1]]
                    sz_1 = np.concatenate(
                        (f.tensor.tensors[alpha-1].shape[:-1], rank))
                    sz_2 = np.concatenate(
                        (f.tensor.tensors[gamma-1].shape[ind-1], rank))

                    a_alpha = C.tensor.tensors[1].reshape(sz_1)
                    a_gamma = C.tensor.tensors[0].transpose([1, 0]).reshape(
                        sz_2)

                    perm = np.concatenate((ind, [tree.child_number(alpha)]))
                    a_gamma = a_gamma.itranspose(perm-1)
                    sz_2[perm-1] = np.array(sz_2)

                    a_alpha = a_alpha.data
                    a_gamma = a_gamma.data

                    if (a_alpha is None or np.count_nonzero(a_alpha) == 0 or
                            not np.all(np.isfinite(a_alpha))) or \
                        (a_gamma is None or np.count_nonzero(a_gamma) == 0 or
                            not np.all(np.isfinite(a_gamma))):
                        print(&#39;Empty, zero or NaN solution, returning to &#39; +
                              &#39;the previous iteration.&#39;)
                        output[&#39;flag&#39;] = -2
                        output[&#39;error&#39;] = np.inf
                        break
                else:
                    raise ValueError(&#39;Wrong rank adaptation type.&#39;)
                f.tensor.tensors[alpha-1] = tensap.FullTensor(a_alpha,
                                                              np.size(sz_1),
                                                              sz_1)
                f.tensor.tensors[gamma-1] = tensap.FullTensor(a_gamma,
                                                              np.size(sz_2),
                                                              sz_2)

            if self.rank_adaptation_options[&#39;post_alternating_minimization&#39;]:
                print(&#39;\t\tPost alternating minimization.&#39;)
                s_local = TreeBasedTensorLearning(self.tree,
                                                  self.is_active_node,
                                                  self.loss_function)
                s_local.bases_eval = self.bases_eval
                s_local.bases_adaptation_path = self.bases_adaptation_path
                s_local.training_data = self.training_data
                s_local.test_error = self.test_error
                s_local.test_data = self.test_data
                s_local.bases_eval_test = self.bases_eval_test
                s_local.model_selection = False
                s_local.rank_adaptation = False
                s_local.store_iterates = False
                s_local.rank = f.tensor.ranks
                s_local.initialization_type = &#39;initial_guess&#39;
                s_local.initial_guess = f.tensor
                s_local.tolerance[&#39;on_error&#39;] = self.tolerance[&#39;on_error&#39;] / \
                    np.sqrt(np.count_nonzero(f.tensor.is_active_node)-1)
                s_local.tolerance[&#39;on_stagnation&#39;] = \
                    self.tolerance[&#39;on_stagnation&#39;]
                s_local.alternating_minimization_parameters = \
                    deepcopy(self.alternating_minimization_parameters)
                s_local.alternating_minimization_parameters[&#39;display&#39;] = False
                s_local.error_estimation = True
                s_local.display = False
                s_local.linear_model_learning = \
                    [x for x in self.linear_model_learning if x is not None]
                s_local._warnings[&#39;orthonormality_warning_display&#39;] = False
                s_local._warnings[&#39;empty_bases_warning_display&#39;] = False
                f, output_tmp = s_local.solve()

            stagnation = self.stagnation_criterion(f, f0)
            output[&#39;stagnation_criterion&#39;].append(stagnation)

            if self.store_iterates:
                if isinstance(self.bases, tensap.FunctionalBases):
                    output[&#39;iterates&#39;].append(tensap.FunctionalTensor(
                        f.tensor, self.bases))
                else:
                    output[&#39;iterates&#39;].append(f)

            if &#39;error&#39; in output_tmp:
                output[&#39;error&#39;] = output_tmp[&#39;error&#39;]
                output[&#39;error_iterations&#39;].append(output[&#39;error&#39;])

            if self.test_error:
                f_eval_test = tensap.FunctionalTensor(f, self.bases_eval_test)
                output[&#39;test_error&#39;] = self.loss_function.test_error(
                    f_eval_test, self.test_data)
                output[&#39;test_error_iterations&#39;].append(output[&#39;test_error&#39;])

            if self.alternating_minimization_parameters[&#39;display&#39;]:
                print(&#39;\tAlt. min. iteration %s/%i: stagnation = %2.5e&#39; %
                      (str(iteration).
                       zfill(len(str(self.alternating_minimization_parameters
                                     [&#39;max_iterations&#39;]-1))),
                       self.alternating_minimization_parameters
                       [&#39;max_iterations&#39;]-1,
                       stagnation), end=&#39;&#39;)
                if &#39;error&#39; in output:
                    print(&#39;, error = %2.5e&#39; % output[&#39;error&#39;], end=&#39;&#39;)
                if self.test_error:
                    if not np.isscalar(output[&#39;test_error&#39;]):
                        output[&#39;test_error&#39;] = output[&#39;test_error&#39;].numpy()
                    print(&#39;, test error = %2.5e&#39; % output[&#39;test_error&#39;],
                          end=&#39;&#39;)
                print(&#39;&#39;)

            if iteration &gt; 0 and stagnation &lt; \
                    self.alternating_minimization_parameters[&#39;stagnation&#39;]:
                output[&#39;flag&#39;] = 1
                break

            if self.test_error and \
                    output[&#39;test_error&#39;] &lt; self.tolerance[&#39;on_error&#39;]:
                output[&#39;flag&#39;] = 1
                break

            if self.tree_adaptation and iteration &gt; 0:
                C_old = f.tensor.storage()
                self, f, output = self.adapt_tree(f, output[&#39;error&#39;],
                                                  None, output, iteration)
                adapted_tree = output[&#39;adapted_tree&#39;]
                if adapted_tree:
                    if self.display:
                        print(&#39;\t\tStorage complexity before permutation &#39; +
                              &#39;= %i&#39; % C_old)
                        print(&#39;\t\tStorage complexity after permutation &#39; +
                              &#39;= %i&#39; % f.tensor.storage())
                    if self.test_error:
                        f_eval_test = tensap.FunctionalTensor(
                            f, self.bases_eval_test)
                        if self.display:
                            print(&#39;\t\tTest error after permutation &#39; +
                                  &#39;= %2.5e&#39; % self.loss_function.test_error(
                                      f_eval_test, self.test_data))

                    if self.alternating_minimization_parameters[&#39;display&#39;]:
                        print(&#39;&#39;)

        if isinstance(self.bases, tensap.FunctionalBases):
            f = tensap.FunctionalTensor(f.tensor, self.bases)
        output[&#39;iter&#39;] = iteration

        if &#39;adapted_tree&#39; in output:
            del output[&#39;adapted_tree&#39;]

        if self.display and not self.model_selection:
            if self.alternating_minimization_parameters[&#39;display&#39;]:
                print(&#39;&#39;)
            self.final_display(f)
            if &#39;error&#39; in output:
                print(&#39;, error = %2.5e&#39; % output[&#39;error&#39;], end=&#39;&#39;)
            if &#39;test_error&#39; in output:
                print(&#39;, test error = %2.5e&#39; % output[&#39;test_error&#39;], end=&#39;&#39;)
            print(&#39;&#39;)

        return f, output

    def create_basis_adaptation_path_dmrg(self, ranks, alpha):
        tree = self.tree
        ranks = np.array(ranks)
        ranks[tree.root-1] = 1
        gamma = tree.parent(alpha)

        p_alpha = self.create_basis_adaptation_path(ranks, alpha)
        if p_alpha.ndim == 1:
            p_alpha = np.reshape(p_alpha, [-1, 1])
        p_alpha = p_alpha[:int(p_alpha.shape[0]/ranks[alpha-1]), :]

        ch = tree.children(gamma)
        p_gamma = [None]*len(ch)
        for nod in np.setdiff1d(ch, alpha):
            ind = tree.child_number(nod) &lt; tree.child_number(alpha)
            if self.is_active_node[nod-1]:
                p_gamma[tree.child_number(nod)-1+ind] = \
                    np.full((ranks[nod-1], 1), True)
            else:
                rep = np.nonzero(nod == tree.dim2ind)[0][0]
                p_gamma[tree.child_number(nod)-1+ind] = \
                    self.bases_adaptation_path[rep].astype(bool)
        p_gamma[0] = p_alpha

        r_gamma = ranks[gamma-1]
        p = p_gamma[-1]
        for i in np.arange(len(p_gamma)-2, -1, -1):
            p = np.kron(p, p_gamma[i])
        return np.tile(p, (r_gamma, 1)).astype(bool)

    def create_basis_adaptation_path_dmrg_low_rank(self, ranks, alpha):
        tree = self.tree
        ranks = np.array(ranks)
        ranks[tree.root-1] = 1
        gamma = tree.parent(alpha)

        p_alpha = self.create_basis_adaptation_path(ranks, alpha)
        if p_alpha.ndim == 1:
            p_alpha = np.reshape(p_alpha, [-1, 1])
        p_alpha = p_alpha[:int(p_alpha.shape[0]/ranks[alpha-1]), :]

        ch = np.setdiff1d(tree.children(gamma), alpha)
        p_gamma = [None]*len(ch)
        for nod in ch:
            ind = tree.child_number(nod) &gt; tree.child_number(alpha)
            if self.is_active_node[nod-1]:
                p_gamma[tree.child_number(nod)-1-ind] = \
                    np.full((ranks[nod-1], 1), True)
            else:
                rep = np.nonzero(nod == tree.dim2ind)[0][0]
                p_gamma[tree.child_number(nod)-1-ind] = \
                    self.bases_adaptation_path[rep].astype(bool)

        r_gamma = ranks[gamma-1]
        p = p_gamma[-1]
        for i in np.arange(len(p_gamma)-2, -1, -1):
            p = np.kron(p, p_gamma[i])
        p = np.tile(p, (r_gamma, 1)).astype(bool)

        return [p_alpha, p]

# %% Static methods
    @staticmethod
    def tensor_train(order, *args):
        &#39;&#39;&#39;
        Call of the constructor of the class TreeBasedTensorLearning, with a
        tree and active nodes corresponding to the Tensor-Train format in
        dimension order.

        See also TreeBasedTensorLearning.

        Parameters
        ----------
        order : int
            The order of the tensor.
        *args : tuple
            Additional parameters (see the constructor of
            TreeBasedTensorLearning).

        Returns
        -------
        TreeBasedTensorLearning
            The solver with a tree and active nodes associated with the
            Tensor-Train format.

        &#39;&#39;&#39;
        tree = tensap.DimensionTree.linear(order)
        is_active_node = np.full(tree.nb_nodes, True)
        is_active_node[tree.dim2ind[1:]-1] = False
        return TreeBasedTensorLearning(tree, is_active_node, *args)

    @staticmethod
    def tensor_train_tucker(order, *args):
        &#39;&#39;&#39;
        Call of the constructor of the class TreeBasedTensorLearning, with a
        tree and active nodes corresponding to the Tensor-Train Tucker format
        in dimension order.

        See also TreeBasedTensorLearning.

        Parameters
        ----------
        order : int
            The order of the tensor.
        *args : tuple
            Additional parameters (see the constructor of
            TreeBasedTensorLearning).

        Returns
        -------
        TreeBasedTensorLearning
            The solver with a tree and active nodes associated with the
            Tensor-Train Tucker format.

        &#39;&#39;&#39;
        tree = tensap.DimensionTree.linear(order)
        is_active_node = np.full(tree.nb_nodes, True)
        return TreeBasedTensorLearning(tree, is_active_node, *args)

    @staticmethod
    def enriched_edges_to_ranks_random(f, new_rank):
        &#39;&#39;&#39;
        Enrichment of the ranks of specified edges of the tensor f using random
        additions for each child / parent couple of the enriched edges.

        Parameters
        ----------
        f : tensap.TreeBasedTensor
            The tree-based tensor to enrich.
        new_rank : list or numpy.ndarray
            The new tree-based rank.

        Returns
        -------
        f : tensap.TreeBasedTensor
            The enriched tree-based tensor.

        &#39;&#39;&#39;
        f.is_orth = False
        tree = f.tree
        enriched_dims = np.nonzero(new_rank &gt; f.ranks)[0]

        for level in np.arange(1, np.max(tree.level)+1):
            nod_lvl = np.intersect1d(tree.nodes_with_level(level),
                                     enriched_dims)
            for alpha in nod_lvl:
                gamma = tree.parent(alpha)
                rank = new_rank[alpha-1] - f.ranks[alpha-1]

                A = np.reshape(f.tensors[alpha-1].data,
                               [-1, f.ranks[alpha-1]], order=&#39;F&#39;)
                A = np.hstack((A, np.tile(np.reshape(A[:, -1], [-1, 1]),
                                          [1, rank]) *
                               (1+np.random.randn(A.shape[0], rank))))
                A[:, -1-rank-1:] /= np.sqrt(np.sum(A[:, -1-rank-1:]**2, 0))
                shape = np.array(f.tensors[alpha-1].shape)
                shape[-1] += rank
                f.tensors[alpha-1].data = np.reshape(A, shape, order=&#39;F&#39;)

                ch = f.tree.child_number(alpha)-1
                ind = np.setdiff1d(range(f.tensors[gamma-1].order), ch)
                ind = np.concatenate((ind, [ch]))
                A = np.transpose(f.tensors[gamma-1].matricize(ch).data)
                A = np.hstack((A, np.tile(np.reshape(A[:, -1], [-1, 1]),
                                          [1, rank]) *
                               (1+np.random.randn(A.shape[0], rank))))
                A[:, -1-rank-1:] /= np.sqrt(np.sum(A[:, -1-rank-1:]**2, 0))
                shape = np.array(f.tensors[gamma-1].shape)
                shape[ch] += rank
                A = np.reshape(A, shape[ind], order=&#39;F&#39;)
                f.tensors[gamma-1].data = np.transpose(A, np.argsort(ind))

                f = f.update_attributes()
        return f

    @staticmethod
    def make_ranks_admissible(f, rank):
        &#39;&#39;&#39;
        Adjustment of the ranks to make the associated tree-based tensor f
        rank-admissible, by enriching new edges associated with nodes of the
        tree until all the rank admissibility conditions are met.

        Parameters
        ----------
        f : tensap.TreeBasedTensor
            The current approximation in tree-based tensor format.
        rank : numpy.ndarray
            The proposed tree-based rank.

        Returns
        -------
        rank : numpy.ndarray
            The admissible tree-based rank (if possible).
        d : numpy.ndarray
            The enriched nodes.

        &#39;&#39;&#39;
        # Do not increase the ranks of leaf nodes with rank equal to the
        # dimension of the approximation space.
        nodes = f.active_nodes
        ind = [f.tree.is_leaf[x] and rank[x] &gt; f.tensors[x].shape[0] for
               x in nodes-1]
        rank[nodes[ind]-1] = [x.shape[0] for x in f.tensors[nodes[ind]-1]]
        rank[np.logical_not(f.is_active_node)] = 0

        delta = rank - f.ranks
        if f.is_admissible_rank(f.ranks + delta):
            rank = f.ranks + delta
            d = np.nonzero(delta)[0] + 1
            return rank, d

        ind = np.nonzero(delta)[0]
        for i in np.arange(1, np.count_nonzero(delta)+1):
            pos = list(combinations(np.arange(np.count_nonzero(delta)), i))
            shuffle(pos)
            for pos_loc in pos:
                delta_loc = np.array(delta)
                delta_loc[ind[list(pos_loc)]] = 0
                if f.is_admissible_rank(f.ranks + delta_loc):
                    rank = f.ranks + delta_loc
                    d = np.nonzero(delta_loc)[0] + 1
                    return rank, d
        return rank, []

    @staticmethod
    def unique_tol(inp, tol):
        &#39;&#39;&#39;
        Unique values within tolerance, with sorted output.

        Parameters
        ----------
        inp : list of numpy.ndarray
            The values to be checked.
        tol : float
            The tolerance defining the uniqueness.

        Returns
        -------
        bout : numpy.ndarray
            The unique values of inp, within the tolerance tol.

        &#39;&#39;&#39;
        inp = np.sort(inp)
        out = [inp[0]]

        for ind in np.arange(1, inp.size):
            if np.abs(out[-1] - inp[ind]) / out[-1] &gt; tol:
                out = np.concatenate((out, [inp[ind]]))
        return out</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning" href="tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning">TensorLearning</a></li>
<li><a title="tensap.approximation.learning.learning.Learning" href="../../learning/learning.html#tensap.approximation.learning.learning.Learning">Learning</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.enriched_edges_to_ranks_random"><code class="name flex">
<span>def <span class="ident">enriched_edges_to_ranks_random</span></span>(<span>f, new_rank)</span>
</code></dt>
<dd>
<div class="desc"><p>Enrichment of the ranks of specified edges of the tensor f using random
additions for each child / parent couple of the enriched edges.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.TreeBasedTensor</code></dt>
<dd>The tree-based tensor to enrich.</dd>
<dt><strong><code>new_rank</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>The new tree-based rank.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.TreeBasedTensor</code></dt>
<dd>The enriched tree-based tensor.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def enriched_edges_to_ranks_random(f, new_rank):
    &#39;&#39;&#39;
    Enrichment of the ranks of specified edges of the tensor f using random
    additions for each child / parent couple of the enriched edges.

    Parameters
    ----------
    f : tensap.TreeBasedTensor
        The tree-based tensor to enrich.
    new_rank : list or numpy.ndarray
        The new tree-based rank.

    Returns
    -------
    f : tensap.TreeBasedTensor
        The enriched tree-based tensor.

    &#39;&#39;&#39;
    f.is_orth = False
    tree = f.tree
    enriched_dims = np.nonzero(new_rank &gt; f.ranks)[0]

    for level in np.arange(1, np.max(tree.level)+1):
        nod_lvl = np.intersect1d(tree.nodes_with_level(level),
                                 enriched_dims)
        for alpha in nod_lvl:
            gamma = tree.parent(alpha)
            rank = new_rank[alpha-1] - f.ranks[alpha-1]

            A = np.reshape(f.tensors[alpha-1].data,
                           [-1, f.ranks[alpha-1]], order=&#39;F&#39;)
            A = np.hstack((A, np.tile(np.reshape(A[:, -1], [-1, 1]),
                                      [1, rank]) *
                           (1+np.random.randn(A.shape[0], rank))))
            A[:, -1-rank-1:] /= np.sqrt(np.sum(A[:, -1-rank-1:]**2, 0))
            shape = np.array(f.tensors[alpha-1].shape)
            shape[-1] += rank
            f.tensors[alpha-1].data = np.reshape(A, shape, order=&#39;F&#39;)

            ch = f.tree.child_number(alpha)-1
            ind = np.setdiff1d(range(f.tensors[gamma-1].order), ch)
            ind = np.concatenate((ind, [ch]))
            A = np.transpose(f.tensors[gamma-1].matricize(ch).data)
            A = np.hstack((A, np.tile(np.reshape(A[:, -1], [-1, 1]),
                                      [1, rank]) *
                           (1+np.random.randn(A.shape[0], rank))))
            A[:, -1-rank-1:] /= np.sqrt(np.sum(A[:, -1-rank-1:]**2, 0))
            shape = np.array(f.tensors[gamma-1].shape)
            shape[ch] += rank
            A = np.reshape(A, shape[ind], order=&#39;F&#39;)
            f.tensors[gamma-1].data = np.transpose(A, np.argsort(ind))

            f = f.update_attributes()
    return f</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.make_ranks_admissible"><code class="name flex">
<span>def <span class="ident">make_ranks_admissible</span></span>(<span>f, rank)</span>
</code></dt>
<dd>
<div class="desc"><p>Adjustment of the ranks to make the associated tree-based tensor f
rank-admissible, by enriching new edges associated with nodes of the
tree until all the rank admissibility conditions are met.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.TreeBasedTensor</code></dt>
<dd>The current approximation in tree-based tensor format.</dd>
<dt><strong><code>rank</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The proposed tree-based rank.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>rank</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The admissible tree-based rank (if possible).</dd>
<dt><strong><code>d</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The enriched nodes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def make_ranks_admissible(f, rank):
    &#39;&#39;&#39;
    Adjustment of the ranks to make the associated tree-based tensor f
    rank-admissible, by enriching new edges associated with nodes of the
    tree until all the rank admissibility conditions are met.

    Parameters
    ----------
    f : tensap.TreeBasedTensor
        The current approximation in tree-based tensor format.
    rank : numpy.ndarray
        The proposed tree-based rank.

    Returns
    -------
    rank : numpy.ndarray
        The admissible tree-based rank (if possible).
    d : numpy.ndarray
        The enriched nodes.

    &#39;&#39;&#39;
    # Do not increase the ranks of leaf nodes with rank equal to the
    # dimension of the approximation space.
    nodes = f.active_nodes
    ind = [f.tree.is_leaf[x] and rank[x] &gt; f.tensors[x].shape[0] for
           x in nodes-1]
    rank[nodes[ind]-1] = [x.shape[0] for x in f.tensors[nodes[ind]-1]]
    rank[np.logical_not(f.is_active_node)] = 0

    delta = rank - f.ranks
    if f.is_admissible_rank(f.ranks + delta):
        rank = f.ranks + delta
        d = np.nonzero(delta)[0] + 1
        return rank, d

    ind = np.nonzero(delta)[0]
    for i in np.arange(1, np.count_nonzero(delta)+1):
        pos = list(combinations(np.arange(np.count_nonzero(delta)), i))
        shuffle(pos)
        for pos_loc in pos:
            delta_loc = np.array(delta)
            delta_loc[ind[list(pos_loc)]] = 0
            if f.is_admissible_rank(f.ranks + delta_loc):
                rank = f.ranks + delta_loc
                d = np.nonzero(delta_loc)[0] + 1
                return rank, d
    return rank, []</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.tensor_train"><code class="name flex">
<span>def <span class="ident">tensor_train</span></span>(<span>order, *args)</span>
</code></dt>
<dd>
<div class="desc"><p>Call of the constructor of the class TreeBasedTensorLearning, with a
tree and active nodes corresponding to the Tensor-Train format in
dimension order.</p>
<p>See also TreeBasedTensorLearning.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>order</code></strong> :&ensp;<code>int</code></dt>
<dd>The order of the tensor.</dd>
<dt><strong><code>*args</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Additional parameters (see the constructor of
TreeBasedTensorLearning).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning" href="#tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning">TreeBasedTensorLearning</a></code></dt>
<dd>The solver with a tree and active nodes associated with the
Tensor-Train format.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def tensor_train(order, *args):
    &#39;&#39;&#39;
    Call of the constructor of the class TreeBasedTensorLearning, with a
    tree and active nodes corresponding to the Tensor-Train format in
    dimension order.

    See also TreeBasedTensorLearning.

    Parameters
    ----------
    order : int
        The order of the tensor.
    *args : tuple
        Additional parameters (see the constructor of
        TreeBasedTensorLearning).

    Returns
    -------
    TreeBasedTensorLearning
        The solver with a tree and active nodes associated with the
        Tensor-Train format.

    &#39;&#39;&#39;
    tree = tensap.DimensionTree.linear(order)
    is_active_node = np.full(tree.nb_nodes, True)
    is_active_node[tree.dim2ind[1:]-1] = False
    return TreeBasedTensorLearning(tree, is_active_node, *args)</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.tensor_train_tucker"><code class="name flex">
<span>def <span class="ident">tensor_train_tucker</span></span>(<span>order, *args)</span>
</code></dt>
<dd>
<div class="desc"><p>Call of the constructor of the class TreeBasedTensorLearning, with a
tree and active nodes corresponding to the Tensor-Train Tucker format
in dimension order.</p>
<p>See also TreeBasedTensorLearning.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>order</code></strong> :&ensp;<code>int</code></dt>
<dd>The order of the tensor.</dd>
<dt><strong><code>*args</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Additional parameters (see the constructor of
TreeBasedTensorLearning).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning" href="#tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning">TreeBasedTensorLearning</a></code></dt>
<dd>The solver with a tree and active nodes associated with the
Tensor-Train Tucker format.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def tensor_train_tucker(order, *args):
    &#39;&#39;&#39;
    Call of the constructor of the class TreeBasedTensorLearning, with a
    tree and active nodes corresponding to the Tensor-Train Tucker format
    in dimension order.

    See also TreeBasedTensorLearning.

    Parameters
    ----------
    order : int
        The order of the tensor.
    *args : tuple
        Additional parameters (see the constructor of
        TreeBasedTensorLearning).

    Returns
    -------
    TreeBasedTensorLearning
        The solver with a tree and active nodes associated with the
        Tensor-Train Tucker format.

    &#39;&#39;&#39;
    tree = tensap.DimensionTree.linear(order)
    is_active_node = np.full(tree.nb_nodes, True)
    return TreeBasedTensorLearning(tree, is_active_node, *args)</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.unique_tol"><code class="name flex">
<span>def <span class="ident">unique_tol</span></span>(<span>inp, tol)</span>
</code></dt>
<dd>
<div class="desc"><p>Unique values within tolerance, with sorted output.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>inp</code></strong> :&ensp;<code>list</code> of <code>numpy.ndarray</code></dt>
<dd>The values to be checked.</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>float</code></dt>
<dd>The tolerance defining the uniqueness.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>bout</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The unique values of inp, within the tolerance tol.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def unique_tol(inp, tol):
    &#39;&#39;&#39;
    Unique values within tolerance, with sorted output.

    Parameters
    ----------
    inp : list of numpy.ndarray
        The values to be checked.
    tol : float
        The tolerance defining the uniqueness.

    Returns
    -------
    bout : numpy.ndarray
        The unique values of inp, within the tolerance tol.

    &#39;&#39;&#39;
    inp = np.sort(inp)
    out = [inp[0]]

    for ind in np.arange(1, inp.size):
        if np.abs(out[-1] - inp[ind]) / out[-1] &gt; tol:
            out = np.concatenate((out, [inp[ind]]))
    return out</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.canonical_correction"><code class="name flex">
<span>def <span class="ident">canonical_correction</span></span>(<span>self, f, rank)</span>
</code></dt>
<dd>
<div class="desc"><p>Rank-r canonical correction.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.FunctionalTensor</code> or <code>None</code></dt>
<dd>The current approximation.</dd>
<dt><strong><code>rank</code></strong> :&ensp;<code>int</code></dt>
<dd>The rank of the canonical correction.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>NotImplementedError</code></dt>
<dd>If the method is not implemented.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.FunctionalTensor</code></dt>
<dd>The corrected approximation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def canonical_correction(self, f, rank):
    &#39;&#39;&#39;
    Rank-r canonical correction.

    Parameters
    ----------
    f : tensap.FunctionalTensor or None
        The current approximation.
    rank : int
        The rank of the canonical correction.

    Raises
    ------
    NotImplementedError
        If the method is not implemented.

    Returns
    -------
    f : tensap.FunctionalTensor
        The corrected approximation.

    &#39;&#39;&#39;
    if isinstance(f, tensap.FunctionalTensor):
        fx = f.tensor.tensor_matrix_product_eval_diag(self.bases_eval).data
    elif f is None:
        fx = 0
    else:
        raise NotImplementedError(&#39;Not implemented.&#39;)

    solver = deepcopy(self)
    if isinstance(solver.training_data, list) and \
            len(solver.training_data) == 2:
        solver.training_data[1] -= fx
    elif isinstance(solver.loss_function, tensap.DensityL2LossFunction):
        solver.training_data = [solver.training_data, f]

    f_add = solver.canonical_initialization(rank)
    if isinstance(f_add, tensap.FunctionalTensor):
        f_add = f_add.tensor
    if f is not None:
        f = f.tensor + f_add
    else:
        f = f_add
    return f</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.canonical_initialization"><code class="name flex">
<span>def <span class="ident">canonical_initialization</span></span>(<span>self, rank)</span>
</code></dt>
<dd>
<div class="desc"><p>Rank-r canonical initialization.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>rank</code></strong> :&ensp;<code>int</code></dt>
<dd>The rank of the canonical initialization.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tensap.TreeBasedTensor</code></dt>
<dd>The rank-r canonical initialization..</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def canonical_initialization(self, rank):
    &#39;&#39;&#39;
    Rank-r canonical initialization.

    Parameters
    ----------
    rank : int
        The rank of the canonical initialization.

    Returns
    -------
    tensap.TreeBasedTensor
        The rank-r canonical initialization..

    &#39;&#39;&#39;
    solver = tensap.CanonicalTensorLearning(self.order, self.loss_function)
    if isinstance(self.linear_model_learning, list):
        solver.linear_model_learning = self.linear_model_learning[0]
    else:
        solver.linear_model_learning = self.linear_model_learning
    solver.alternating_minimization_parameters = \
        deepcopy(self.alternating_minimization_parameters)
    solver.tolerance[&#39;on_stagnation&#39;] = np.finfo(float).eps
    solver.tolerance[&#39;on_error&#39;] = np.finfo(float).eps
    solver.bases = self.bases
    solver.bases_eval = self.bases_eval
    solver.bases_eval_test = self.bases_eval_test
    solver.display = False
    solver.alternating_minimization_parameters[&#39;display&#39;] = False
    solver.initialization_type = &#39;mean&#39;
    solver.rank_adaptation = True
    solver.rank_adaptation_options[&#39;max_iterations&#39;] = rank
    solver.bases_adaptation_path = self.bases_adaptation_path
    solver.test_error = self.test_error
    solver.training_data = self.training_data
    solver.test_data = self.test_data
    solver._warnings = self._warnings

    f = solver.solve()[0]
    return f.tensor.tree_based_tensor(self.tree, self.is_active_node)</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.create_basis_adaptation_path"><code class="name flex">
<span>def <span class="ident">create_basis_adaptation_path</span></span>(<span>self, ranks, alpha)</span>
</code></dt>
<dd>
<div class="desc"><p>Creation of the basis adaptation path.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>rank</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>The alpha-ranks of the current approximation.</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>int</code></dt>
<dd>The current node.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The basis adaptation path.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_basis_adaptation_path(self, ranks, alpha):
    &#39;&#39;&#39;
    Creation of the basis adaptation path.

    Parameters
    ----------
    rank : list or numpy.ndarray
        The alpha-ranks of the current approximation.
    alpha : int
        The current node.

    Returns
    -------
    path : numpy.ndarray
        The basis adaptation path.

    &#39;&#39;&#39;
    tree = self.tree
    ranks = np.array(ranks)
    ranks[tree.root-1] = 1
    if tree.is_leaf[alpha-1]:
        path_alpha = self.bases_adaptation_path[
            np.nonzero(tree.dim2ind == alpha)[0][0]].astype(bool)
        ranks = ranks[alpha-1]
        path = path_alpha[np.newaxis, :, np.newaxis, :]
        path = np.tile(path, [1, 1, ranks, 2])
        path = np.reshape(path, [path.shape[1]*ranks, -1], order=&#39;F&#39;)
    else:
        assert not self.linear_model_learning_parameters[
            &#39;basis_adaptation_internal_nodes&#39;], \
            &#39;Basis adaptation for internal nodes is not implemented.&#39;
        ch = tree.children(alpha)
        if np.all(self.is_active_node[ch-1]):
            ch_a = ch[self.is_active_node[ch-1]]
            path = np.full(np.prod(ranks[ch_a-1]) * ranks[alpha-1], True)
        else:
            path_alpha = []
            for nod in ch:
                if self.is_active_node[nod-1]:
                    path_alpha.append(np.full((ranks[nod-1], 1), True))
                else:
                    ind = np.nonzero(nod == tree.dim2ind)[0][0]
                    path_alpha.append(
                        self.bases_adaptation_path[ind].astype(bool))
            path = path_alpha[-1]
            for ind in np.arange(len(path_alpha)-2, -1, -1):
                path = np.kron(path, path_alpha[ind])
            path = np.tile(path, [ranks[alpha-1], 1]).astype(bool)
    return path</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.create_basis_adaptation_path_dmrg"><code class="name flex">
<span>def <span class="ident">create_basis_adaptation_path_dmrg</span></span>(<span>self, ranks, alpha)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_basis_adaptation_path_dmrg(self, ranks, alpha):
    tree = self.tree
    ranks = np.array(ranks)
    ranks[tree.root-1] = 1
    gamma = tree.parent(alpha)

    p_alpha = self.create_basis_adaptation_path(ranks, alpha)
    if p_alpha.ndim == 1:
        p_alpha = np.reshape(p_alpha, [-1, 1])
    p_alpha = p_alpha[:int(p_alpha.shape[0]/ranks[alpha-1]), :]

    ch = tree.children(gamma)
    p_gamma = [None]*len(ch)
    for nod in np.setdiff1d(ch, alpha):
        ind = tree.child_number(nod) &lt; tree.child_number(alpha)
        if self.is_active_node[nod-1]:
            p_gamma[tree.child_number(nod)-1+ind] = \
                np.full((ranks[nod-1], 1), True)
        else:
            rep = np.nonzero(nod == tree.dim2ind)[0][0]
            p_gamma[tree.child_number(nod)-1+ind] = \
                self.bases_adaptation_path[rep].astype(bool)
    p_gamma[0] = p_alpha

    r_gamma = ranks[gamma-1]
    p = p_gamma[-1]
    for i in np.arange(len(p_gamma)-2, -1, -1):
        p = np.kron(p, p_gamma[i])
    return np.tile(p, (r_gamma, 1)).astype(bool)</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.create_basis_adaptation_path_dmrg_low_rank"><code class="name flex">
<span>def <span class="ident">create_basis_adaptation_path_dmrg_low_rank</span></span>(<span>self, ranks, alpha)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_basis_adaptation_path_dmrg_low_rank(self, ranks, alpha):
    tree = self.tree
    ranks = np.array(ranks)
    ranks[tree.root-1] = 1
    gamma = tree.parent(alpha)

    p_alpha = self.create_basis_adaptation_path(ranks, alpha)
    if p_alpha.ndim == 1:
        p_alpha = np.reshape(p_alpha, [-1, 1])
    p_alpha = p_alpha[:int(p_alpha.shape[0]/ranks[alpha-1]), :]

    ch = np.setdiff1d(tree.children(gamma), alpha)
    p_gamma = [None]*len(ch)
    for nod in ch:
        ind = tree.child_number(nod) &gt; tree.child_number(alpha)
        if self.is_active_node[nod-1]:
            p_gamma[tree.child_number(nod)-1-ind] = \
                np.full((ranks[nod-1], 1), True)
        else:
            rep = np.nonzero(nod == tree.dim2ind)[0][0]
            p_gamma[tree.child_number(nod)-1-ind] = \
                self.bases_adaptation_path[rep].astype(bool)

    r_gamma = ranks[gamma-1]
    p = p_gamma[-1]
    for i in np.arange(len(p_gamma)-2, -1, -1):
        p = np.kron(p, p_gamma[i])
    p = np.tile(p, (r_gamma, 1)).astype(bool)

    return [p_alpha, p]</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.rank_one_correction"><code class="name flex">
<span>def <span class="ident">rank_one_correction</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"><p>Rank one correction.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.FunctionalTensor</code> or <code>None</code></dt>
<dd>The current approximation.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>NotImplementedError</code></dt>
<dd>If the method is not implemented.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>f</code></strong> :&ensp;<code>tensap.FunctionalTensor</code></dt>
<dd>The corrected approximation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rank_one_correction(self, f):
    &#39;&#39;&#39;
    Rank one correction.

    Parameters
    ----------
    f : tensap.FunctionalTensor or None
        The current approximation.

    Raises
    ------
    NotImplementedError
        If the method is not implemented.

    Returns
    -------
    f : tensap.FunctionalTensor
        The corrected approximation.

    &#39;&#39;&#39;
    if isinstance(f, tensap.FunctionalTensor):
        fx = f.tensor.tensor_matrix_product_eval_diag(self.bases_eval).data
    elif f is None:
        fx = 0
    else:
        raise NotImplementedError(&#39;Not implemented.&#39;)

    solver = deepcopy(self)
    solver.model_selection = False
    if isinstance(solver.training_data, list) and \
            len(solver.training_data) == 2:
        solver.training_data[1] -= fx
    elif isinstance(solver.loss_function, tensap.DensityL2LossFunction):
        solver.training_data = [solver.training_data, f]
    solver.rank_adaptation = False
    solver.tree_adaptation = False
    solver.rank = 1
    solver.display = False
    solver.alternating_minimization_parameters[&#39;display&#39;] = False
    solver.initialization_type = &#39;ones&#39;
    solver.alternating_minimization_parameters[&#39;max_iterations&#39;] = 1

    f_add = solver.solve()[0]
    if isinstance(f_add, tensap.FunctionalTensor):
        f_add = f_add.tensor
    if f is not None:
        f = f.tensor + f_add
    else:
        f = f_add
    return f</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning" href="tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning">TensorLearning</a></b></code>:
<ul class="hlist">
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.adapt_tree" href="tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.adapt_tree">adapt_tree</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.adaptation_display" href="tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.adaptation_display">adaptation_display</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.final_display" href="tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.final_display">final_display</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.initial_guess_new_rank" href="tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.initial_guess_new_rank">initial_guess_new_rank</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.initialize" href="tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.initialize">initialize</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.local_solver" href="tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.local_solver">local_solver</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.new_rank_selection" href="tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.new_rank_selection">new_rank_selection</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.pre_processing" href="tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.pre_processing">pre_processing</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.prepare_alternating_minimization_system" href="tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.prepare_alternating_minimization_system">prepare_alternating_minimization_system</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.randomize_exploration_strategy" href="tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.randomize_exploration_strategy">randomize_exploration_strategy</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.set_parameter" href="tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.set_parameter">set_parameter</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.solve" href="tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.solve">solve</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.stagnation_criterion" href="tensor_learning.html#tensap.approximation.tensor_approximation.tensor_learning.tensor_learning.TensorLearning.stagnation_criterion">stagnation_criterion</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning" href="index.html">tensap.approximation.tensor_approximation.tensor_learning</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning" href="#tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning">TreeBasedTensorLearning</a></code></h4>
<ul class="">
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.canonical_correction" href="#tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.canonical_correction">canonical_correction</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.canonical_initialization" href="#tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.canonical_initialization">canonical_initialization</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.create_basis_adaptation_path" href="#tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.create_basis_adaptation_path">create_basis_adaptation_path</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.create_basis_adaptation_path_dmrg" href="#tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.create_basis_adaptation_path_dmrg">create_basis_adaptation_path_dmrg</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.create_basis_adaptation_path_dmrg_low_rank" href="#tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.create_basis_adaptation_path_dmrg_low_rank">create_basis_adaptation_path_dmrg_low_rank</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.enriched_edges_to_ranks_random" href="#tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.enriched_edges_to_ranks_random">enriched_edges_to_ranks_random</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.make_ranks_admissible" href="#tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.make_ranks_admissible">make_ranks_admissible</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.rank_one_correction" href="#tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.rank_one_correction">rank_one_correction</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.tensor_train" href="#tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.tensor_train">tensor_train</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.tensor_train_tucker" href="#tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.tensor_train_tucker">tensor_train_tucker</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.unique_tol" href="#tensap.approximation.tensor_approximation.tensor_learning.tree_based_tensor_learning.TreeBasedTensorLearning.unique_tol">unique_tol</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>
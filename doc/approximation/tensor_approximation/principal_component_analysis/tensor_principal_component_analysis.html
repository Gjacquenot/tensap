<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis API documentation</title>
<meta name="description" content="Module tensor_principal_component_analysis â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis</code></h1>
</header>
<section id="section-intro">
<p>Module tensor_principal_component_analysis.</p>
<p>Copyright (c) 2020, Anthony Nouy, Erwan Grelier
This file is part of tensap (tensor approximation package).</p>
<p>tensap is free software: you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.</p>
<p>tensap is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
See the
GNU Lesser General Public License for more details.</p>
<p>You should have received a copy of the GNU Lesser General Public License
along with tensap.
If not, see <a href="https://www.gnu.org/licenses/">https://www.gnu.org/licenses/</a>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
Module tensor_principal_component_analysis.

Copyright (c) 2020, Anthony Nouy, Erwan Grelier
This file is part of tensap (tensor approximation package).

tensap is free software: you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

tensap is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Lesser General Public License for more details.

You should have received a copy of the GNU Lesser General Public License
along with tensap.  If not, see &lt;https://www.gnu.org/licenses/&gt;.

&#39;&#39;&#39;

from copy import deepcopy
import numpy as np
import tensap


class TensorPrincipalComponentAnalysis:
    &#39;&#39;&#39;
    Class TensorPrincipalComponentAnalysis: principal component analysis of an
    algebraic tensor.

    Attributes
    ----------
    display : bool
        Boolean specifying the verbosity of the methods.
    pca_sampling_factor : int
        A factor to determine the number of samples N for the estimation of
        the principal components (1 by default):
            - if prescribed precision, N = pca_sampling_factor*N_alpha,
            - if prescribed rank, N = pca_sampling_factor*t.
    pca_adaptive_sampling : bool
        Adaptive sampling to determine the principal components with prescribed
        precision.
    tol : int or float or list or numpy.ndarray
        An array containing the prescribed relative precisions (the length
        depends on the format).
        If len(tol)==1, use the same value for all alpha.
        Set tol = inf to prescribe the rank.
    max_rank : int or list or numpy.ndarray
        An array containing the maximum alpha-ranks (the length depends on the
        format).
        If len(max_rank)==1, use the same value for all alpha.
        Set max_rank = np.inf to prescribe the precision.

    &#39;&#39;&#39;

    def __init__(self):
        &#39;&#39;&#39;
        Constructor for the class TensorPrincipalComponentAnalysis.

        Returns
        -------
        None.

        &#39;&#39;&#39;
        self.display = True
        self.pca_sampling_factor = 1
        self.pca_adaptive_sampling = False
        self.tol = 1e-8
        self.max_rank = np.inf

    def alpha_principal_components(self, fun, shape, alpha, tol, B_alpha,
                                   I_alpha):
        &#39;&#39;&#39;
        Evaluate the alpha-principal components of a tensor f.

        For alpha in {0,...,d-1}, it evaluates the alpha-principal components
        of a tensor f, meaning the principal components of the matricisations
        f_alpha(i_alpha,i_notalpha), where i_alpha and i_notalpha are groups of
        indices.

        It evaluates f_alpha on the product of a set of indices in dimension
        alpha (of size Nalpha) and a set of random indices (N samples) in the
        complementary dimensions.
        Then, it computes approximations of the alpha-principal components
        in a given basis phi_1(i_alpha) ... phi_Nalpha(i_alpha).

        If t is an integer, t is the rank (number of principal components).
        If t&lt;1, the rank (number of principal components) is determined such
        that the relative error after truncation is t.

        Parameters
        ----------
        fun : fun or tensap.Function
            Function of d variables i_1, ..., i_d which returns the entries of
            the tensor.
        shape : list or numpy.ndarray
            The shape of the tensor.
        alpha : int
            An array containing a tuple in {0,...,d-1}.
        tol : int or float
            The number of principal components or a positive number smaller
            than 1 (tolerance).
        B_alpha : numpy.ndarray
            Array of shape (N_\alpha, N_\alpha) whose i-th column is the
            evaluation of phi_i at the set of indices i_alpha in Ialpha.
        I_alpha : numpy.ndarray
            Array of shape (N_alpha, #alpha) containing N_alpha tuples i_alpha.

        Returns
        -------
        pc : numpy.ndarray
            The principal components of the tensor.
        output : dict
            A dictionnary of outputs, containing the singular values
            corresponding to the principal components, as well as the set of
            indices at which the tensor has been evaluated.

        &#39;&#39;&#39;
        alpha = np.atleast_1d(alpha)
        B_alpha = np.atleast_2d(B_alpha)
        I_alpha = np.array(I_alpha)

        X = tensap.random_multi_indices(shape)
        d = len(shape)
        not_alpha = np.setdiff1d(range(d), alpha)

        if tol &lt; 1:
            N = self.pca_sampling_factor * B_alpha.shape[1]
        else:
            N = self.pca_sampling_factor * tol
        N = int(np.ceil(N))

        X_not_alpha = tensap.RandomVector(X.random_variables[not_alpha])
        I_not_alpha = X_not_alpha.random(N)

        alpha_not_alpha = np.concatenate((alpha, not_alpha))
        ind = [np.nonzero(alpha_not_alpha == x)[0][0] for x in range(d)]

        output = {}
        if tol &lt; 1 and self.pca_adaptive_sampling:
            A = tensap.FullTensor(np.zeros((I_alpha.shape[0], 0)), 2,
                                  [I_alpha.shape[0], 0])
            for k in range(N):
                product_grid = tensap.FullTensorGrid(
                    [I_alpha, I_not_alpha[k, :]]).array()
                A_k = np.linalg.solve(B_alpha, fun(product_grid[:, ind]))
                A.data = np.column_stack((A.data, A_k))
                pc, sin_val = A.principal_components(tol)
                if sin_val[-1, -1] &lt; 1e-15 or pc.shape[1] &lt; \
                        np.ceil((k+1) / self.pca_sampling_factor):
                    break
            output[&#39;number_of_evaluations&#39;] = I_alpha.shape[0] * (k+1)
        else:
            product_grid = tensap.FullTensorGrid([I_alpha,
                                                  I_not_alpha]).array()
            A = fun(product_grid[:, ind])
            A = np.reshape(A, [B_alpha.shape[0], N], &#39;F&#39;)
            A = np.linalg.solve(B_alpha, A)
            A = tensap.FullTensor(A, 2, [B_alpha.shape[0], N])
            pc, sin_val = A.principal_components(tol)
            output[&#39;number_of_evaluations&#39;] = I_alpha.shape[0] * N

        output[&#39;singular_values&#39;] = sin_val
        output[&#39;samples&#39;] = product_grid
        return pc, output

    def hopca(self, fun, shape):
        &#39;&#39;&#39;
        Return the set of alpha-principal components of an algebraic tensor,
        for all alpha in {0,1,...,d-1}.

        For prescribed precision, set TPCA.max_rank = np.inf and TPCA.tol to
        the desired precision (possibly an array of length d).

        For prescribed rank, set TPCA.tol = np.inf and TPCA.max_rank to the
        desired rank (possibly an array of length d).

        See also the documentation of the class
        TensorPrincipalComponentAnalysis.

        Parameters
        ----------
        fun : fun or tensap.Function
            Function of d variables i_1, ..., i_d which returns the entries of
            the tensor.
        shape : list or numpy.ndarray
            The shape of the tensor.

        Raises
        ------
        ValueError
            If the provided tolerance and max ranks are not correct.

        Returns
        -------
        f_pc : list
            List of the alpha-principal components of the tensor.
        output : list
            List containing the outputs of the method
            alpha_principal_components.

        &#39;&#39;&#39;
        solver = deepcopy(self)
        d = len(shape)

        if np.ndim(self.tol) == 0 or len(self.tol) == 1:
            solver.tol = np.full(d, self.tol)
        elif len(self.tol) != d:
            raise ValueError(&#39;tol should be a scalar or an array of length d.&#39;)

        if np.ndim(self.max_rank) == 0 or len(self.max_rank) == 1:
            solver.max_rank = np.full(d, self.max_rank)
        elif len(self.max_rank) != d:
            raise ValueError(&#39;max_rank should be a scalar or an array of &#39; +
                             &#39;length d.&#39;)

        f_pc = []
        output = []
        for alpha in range(d):
            I_alpha = np.reshape(np.arange(shape[alpha]), (-1, 1))
            B_alpha = np.eye(shape[alpha])
            tol_alpha = np.min((solver.tol[alpha], solver.max_rank[alpha]))
            f_pc_alpha, output_alpha = \
                solver.alpha_principal_components(fun, shape, alpha, tol_alpha,
                                                  B_alpha, I_alpha)
            f_pc.append(f_pc_alpha)
            output.append(output_alpha)
        return f_pc, output

    def tucker_approximation(self, fun, shape):
        &#39;&#39;&#39;
        Approximation of a tensor of order d in Tucker format based on a
        Principal Component Analysis.

        For a prescribed precision, set TPCA.max_rank = np.inf and TPCA.tol to
        the desired precision (possibly an array of length d).

        For a prescribed rank, set TPCA.tol = np.inf and TPCA.max_rank to the
        desired rank (possibly an array of length d).

        See also the documentation of the class
        TensorPrincipalComponentAnalysis.

        Parameters
        ----------
        fun : fun or tensap.Function
            Function of d variables i_1, ..., i_d which returns the entries of
            the tensor.
        shape : list or numpy.ndarray
            The shape of the tensor.

        Raises
        ------
        ValueError
            If the provided tolerance and max ranks are not correct.

        Returns
        -------
        tensap.TreeBasedTensor
            A tensor in tree based format with a trivial tree.
        dict
            Dictionnary containing the outputs of the method.

        &#39;&#39;&#39;
        solver = deepcopy(self)
        d = len(shape)
        tree = tensap.DimensionTree.trivial(d)

        if np.ndim(self.tol) == 1 and len(self.tol) == d:
            tol = solver.tol
            solver.tol = np.zeros(d+1)
            solver.tol[tree.dim2ind-1] = tol
        elif np.ndim(self.tol) == 1 and len(self.tol) &gt; 1:
            raise ValueError(&#39;tol should be a scalar or an array of length d.&#39;)

        if np.ndim(self.max_rank) == 1 and len(self.max_rank) == d:
            rank = solver.max_rank
            solver.max_rank = np.zeros(d+1)
            solver.max_rank[tree.dim2ind-1] = rank
        elif np.ndim(self.max_rank) == 1 and len(self.max_rank) &gt; 1:
            raise ValueError(&#39;max_rank should be a scalar or an array of &#39; +
                             &#39;length d.&#39;)

        return solver.tree_based_approximation(fun, shape, tree)

    def tt_approximation(self, fun, shape):
        &#39;&#39;&#39;
        Approximation of a tensor of order d in tensor train format based on a
        Principal Component Analysis.

        For a prescribed precision, set TPCA.max_rank = np.inf and TPCA.tol to
        the desired precision (possibly an array of length d-1).

        For a prescribed rank, set TPCA.tol = np.inf and TPCA.max_rank to the
        desired rank (possibly an array of length d-1).

        See also the documentation of the class
        TensorPrincipalComponentAnalysis.

        Parameters
        ----------
        fun : fun or tensap.Function
            Function of d variables i_1, ..., i_d which returns the entries of
            the tensor.
        shape : list or numpy.ndarray
            The shape of the tensor.

        Raises
        ------
        ValueError
            If the provided tolerance and max ranks are not correct.

        Returns
        -------
        tensap.TreeBasedTensor
            A tensor in tree based format with a linear tree.
        dict
            Dictionnary containing the outputs of the method.

        &#39;&#39;&#39;
        solver = deepcopy(self)
        d = len(shape)
        tree = tensap.DimensionTree.linear(d)
        is_active_node = np.full(tree.nb_nodes, True)
        is_active_node[tree.dim2ind[1:]-1] = False
        rep_tt = np.nonzero(is_active_node)[0]
        rep_tt = np.flip(rep_tt[1:])

        if np.ndim(self.tol) == 1 and len(self.tol) == d-1:
            tol = solver.tol
            solver.tol = np.zeros(tree.nb_nodes)
            solver.tol[rep_tt] = tol
        elif np.ndim(self.tol) == 1 and len(self.tol) &gt; 1:
            raise ValueError(&#39;tol should be a scalar or an array of length &#39; +
                             &#39;d-1.&#39;)

        if np.ndim(self.max_rank) == 1 and len(self.max_rank) == d-1:
            rank = solver.max_rank
            solver.max_rank = np.zeros(tree.nb_nodes)
            solver.max_rank[rep_tt] = rank
        elif np.ndim(self.max_rank) == 1 and len(self.max_rank) &gt; 1:
            raise ValueError(&#39;max_rank should be a scalar or an array of &#39; +
                             &#39;length d-1.&#39;)

        return solver.tree_based_approximation(fun, shape, tree,
                                               is_active_node)

    def tree_based_approximation(self, fun, shape, tree, is_active_node=None):
        &#39;&#39;&#39;
        Approximation of a tensor of order d in tree based tensor format based
        on a Principal Component Analysis.

        For a prescribed precision, set TPCA.max_rank = np.inf and TPCA.tol to
        the desired precision (possibly an array of length d-1).

        For a prescribed rank, set TPCA.tol = np.inf and TPCA.max_rank to the
        desired rank (possibly an array of length d-1).

        See also the documentation of the class
        TensorPrincipalComponentAnalysis.

        Parameters
        ----------
        fun : fun or tensap.Function
            Function of d variables i_1, ..., i_d which returns the entries of
            the tensor.
        shape : list or numpy.ndarray
            The shape of the tensor.
        tree : tensap.DimensionTree
            The required dimension tree.
        is_active_node : list or numpy.ndarray, optional
            An array of booleans indicating which nodes of the tree are active.
            The default is None, settings all the nodes active.

        Raises
        ------
        ValueError
            If the provided tolerance and max ranks are not correct.

        Returns
        -------
        tensap.TreeBasedTensor
            A tensor in tree based format.
        dict
            Dictionnary containing the outputs of the method.

        &#39;&#39;&#39;
        solver = deepcopy(self)
        d = len(shape)

        if is_active_node is None:
            is_active_node = np.full(tree.nb_nodes, True)

        if (np.ndim(self.tol) == 0 or len(self.tol) == 1) and self.tol &lt; 1:
            solver.tol /= np.sqrt(np.count_nonzero(is_active_node)-1)

        if np.ndim(self.tol) == 0 or len(self.tol) == 1:
            solver.tol = np.full(tree.nb_nodes, solver.tol)
        elif len(self.tol) != tree.nb_nodes:
            raise ValueError(&#39;tol should be a scalar or an array of length &#39; +
                             &#39;tree.nb_nodes.&#39;)

        if np.ndim(self.max_rank) == 0 or len(self.max_rank) == 1:
            solver.max_rank = np.full(tree.nb_nodes, self.max_rank)
        elif len(self.max_rank) != tree.nb_nodes:
            raise ValueError(&#39;max_rank should be a scalar or an array of &#39; +
                             &#39;length tree.nb_nodes.&#39;)

        grids = [np.reshape(np.arange(x), (-1, 1)) for x in shape]
        alpha_basis = np.empty(tree.nb_nodes, dtype=object)
        alpha_grids = np.empty(tree.nb_nodes, dtype=object)
        outputs = np.empty(tree.nb_nodes, dtype=object)
        samples = np.empty(tree.nb_nodes, dtype=object)
        tensors = [[]]*tree.nb_nodes
        number_of_evaluations = 0
        for nu in range(d):
            alpha = tree.dim2ind[nu]
            B_alpha = np.eye(shape[nu])
            if is_active_node[alpha-1]:
                tol_alpha = np.min((solver.tol[alpha-1],
                                    solver.max_rank[alpha-1]))
                pc_alpha, outputs[alpha-1] = \
                    solver.alpha_principal_components(fun, shape, nu,
                                                      tol_alpha, B_alpha,
                                                      grids[nu])
                samples[alpha-1] = outputs[alpha-1][&#39;samples&#39;]
                shape_alpha = [shape[nu], pc_alpha.shape[1]]
                tensors[alpha-1] = tensap.FullTensor(pc_alpha, 2, shape_alpha)

                B_alpha = np.matmul(B_alpha, pc_alpha)
                I_alpha = tensap.magic_indices(B_alpha)[0]
                alpha_grids[alpha-1] = grids[nu][I_alpha, :]
                alpha_basis[alpha-1] = B_alpha[I_alpha, :]

                number_of_evaluations += outputs[alpha-1][
                    &#39;number_of_evaluations&#39;]
                if solver.display:
                    print(&#39;alpha = %i : rank = %i, nb_eval = %i&#39; %
                          (alpha, shape_alpha[-1],
                           outputs[alpha-1][&#39;number_of_evaluations&#39;]))
            else:
                alpha_grids[alpha-1] = grids[nu]
                alpha_basis[alpha-1] = B_alpha

        for level in np.arange(np.max(tree.level), 0, -1):
            for alpha in np.intersect1d(tree.nodes_with_level(level),
                                        tree.internal_nodes):
                S_alpha = tree.children(alpha)
                B_alpha = TensorPrincipalComponentAnalysis.\
                    _tensor_product_b_alpha(alpha_basis[S_alpha-1])
                alpha_grids[alpha-1] = \
                    tensap.FullTensorGrid(alpha_grids[S_alpha-1]).array()

                tol_alpha = np.min((solver.tol[alpha-1],
                                    solver.max_rank[alpha-1]))
                pc_alpha, outputs[alpha-1] = \
                    solver.alpha_principal_components(fun, shape,
                                                      tree.dims[alpha-1],
                                                      tol_alpha,
                                                      B_alpha,
                                                      alpha_grids[alpha-1])
                samples[alpha-1] = outputs[alpha-1][&#39;samples&#39;]
                shape_alpha = np.concatenate(([x.shape[1] for
                                              x in alpha_basis[S_alpha-1]],
                                             [pc_alpha.shape[1]]))
                tensors[alpha-1] = tensap.FullTensor(pc_alpha, len(S_alpha)+1,
                                                     shape_alpha)

                B_alpha = np.matmul(B_alpha, pc_alpha)
                I_alpha = tensap.magic_indices(B_alpha)[0]
                alpha_grids[alpha-1] = alpha_grids[alpha-1][I_alpha, :]
                alpha_basis[alpha-1] = B_alpha[I_alpha, :]
                number_of_evaluations += outputs[alpha-1][
                    &#39;number_of_evaluations&#39;]
                if solver.display:
                    print(&#39;alpha = %i : rank = %i, nb_eval = %i&#39; %
                          (alpha, shape_alpha[-1],
                           outputs[alpha-1][&#39;number_of_evaluations&#39;]))

        alpha = tree.root
        S_alpha = tree.children(alpha)
        B_alpha = TensorPrincipalComponentAnalysis.\
            _tensor_product_b_alpha(alpha_basis[S_alpha-1])
        I_alpha = tensap.FullTensorGrid(alpha_grids[S_alpha-1]).array()
        shape_alpha = [x.shape[1] for x in alpha_basis[S_alpha-1]]
        ind = [np.nonzero(tree.dims[alpha-1] == x)[0][0] for x in range(d)]
        tensors[alpha-1] = tensap.FullTensor(
            np.linalg.solve(B_alpha, fun(I_alpha[:, ind])), len(S_alpha),
            shape_alpha)
        alpha_grids[alpha-1] = I_alpha
        number_of_evaluations += I_alpha.shape[0]
        samples[alpha-1] = I_alpha
        if solver.display:
            print(&#39;Interpolation - nb_eval = %i&#39; % I_alpha.shape[0])

        f = tensap.TreeBasedTensor(tensors, tree)

        output = {&#39;number_of_evaluations&#39;: number_of_evaluations,
                  &#39;samples&#39;: samples,
                  &#39;alpha_basis&#39;: alpha_basis,
                  &#39;alpha_grids&#39;: alpha_grids,
                  &#39;outputs&#39;: outputs}

        return f, output

    @staticmethod
    def _tensor_product_b_alpha(Bs):
        &#39;&#39;&#39;
        Function used in the method tree_based_approximation.

        Parameters
        ----------
        Bs : list
            List containing s matricices B1 , ..., Bs where Bi is a
            n[i]-by-r[i] array.

        Returns
        -------
        numpy.ndarray
            An array of shape prod(n)-by-prod(r) whose entries are
            B[I , J] = B1[i_1, j_1] ... Bs[i_s, j_s]
            with I = (i_1, ..., is) and J = (j1, ..., js).

        &#39;&#39;&#39;
        Bs = [tensap.FullTensor(x, 2, x.shape) for x in Bs]
        B = Bs[0]
        for k in np.arange(1, len(Bs)):
            B = B.tensordot(Bs[k], 0)
        return B.matricize(np.arange(0, B.order-1, 2)).data</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis"><code class="flex name class">
<span>class <span class="ident">TensorPrincipalComponentAnalysis</span></span>
</code></dt>
<dd>
<div class="desc"><p>Class TensorPrincipalComponentAnalysis: principal component analysis of an
algebraic tensor.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>display</code></strong> :&ensp;<code>bool</code></dt>
<dd>Boolean specifying the verbosity of the methods.</dd>
<dt><strong><code>pca_sampling_factor</code></strong> :&ensp;<code>int</code></dt>
<dd>A factor to determine the number of samples N for the estimation of
the principal components (1 by default):
- if prescribed precision, N = pca_sampling_factor<em>N_alpha,
- if prescribed rank, N = pca_sampling_factor</em>t.</dd>
<dt><strong><code>pca_adaptive_sampling</code></strong> :&ensp;<code>bool</code></dt>
<dd>Adaptive sampling to determine the principal components with prescribed
precision.</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>int</code> or <code>float</code> or <code>list</code> or <code>numpy.ndarray</code></dt>
<dd>An array containing the prescribed relative precisions (the length
depends on the format).
If len(tol)==1, use the same value for all alpha.
Set tol = inf to prescribe the rank.</dd>
<dt><strong><code>max_rank</code></strong> :&ensp;<code>int</code> or <code>list</code> or <code>numpy.ndarray</code></dt>
<dd>An array containing the maximum alpha-ranks (the length depends on the
format).
If len(max_rank)==1, use the same value for all alpha.
Set max_rank = np.inf to prescribe the precision.</dd>
</dl>
<p>Constructor for the class TensorPrincipalComponentAnalysis.</p>
<h2 id="returns">Returns</h2>
<p>None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TensorPrincipalComponentAnalysis:
    &#39;&#39;&#39;
    Class TensorPrincipalComponentAnalysis: principal component analysis of an
    algebraic tensor.

    Attributes
    ----------
    display : bool
        Boolean specifying the verbosity of the methods.
    pca_sampling_factor : int
        A factor to determine the number of samples N for the estimation of
        the principal components (1 by default):
            - if prescribed precision, N = pca_sampling_factor*N_alpha,
            - if prescribed rank, N = pca_sampling_factor*t.
    pca_adaptive_sampling : bool
        Adaptive sampling to determine the principal components with prescribed
        precision.
    tol : int or float or list or numpy.ndarray
        An array containing the prescribed relative precisions (the length
        depends on the format).
        If len(tol)==1, use the same value for all alpha.
        Set tol = inf to prescribe the rank.
    max_rank : int or list or numpy.ndarray
        An array containing the maximum alpha-ranks (the length depends on the
        format).
        If len(max_rank)==1, use the same value for all alpha.
        Set max_rank = np.inf to prescribe the precision.

    &#39;&#39;&#39;

    def __init__(self):
        &#39;&#39;&#39;
        Constructor for the class TensorPrincipalComponentAnalysis.

        Returns
        -------
        None.

        &#39;&#39;&#39;
        self.display = True
        self.pca_sampling_factor = 1
        self.pca_adaptive_sampling = False
        self.tol = 1e-8
        self.max_rank = np.inf

    def alpha_principal_components(self, fun, shape, alpha, tol, B_alpha,
                                   I_alpha):
        &#39;&#39;&#39;
        Evaluate the alpha-principal components of a tensor f.

        For alpha in {0,...,d-1}, it evaluates the alpha-principal components
        of a tensor f, meaning the principal components of the matricisations
        f_alpha(i_alpha,i_notalpha), where i_alpha and i_notalpha are groups of
        indices.

        It evaluates f_alpha on the product of a set of indices in dimension
        alpha (of size Nalpha) and a set of random indices (N samples) in the
        complementary dimensions.
        Then, it computes approximations of the alpha-principal components
        in a given basis phi_1(i_alpha) ... phi_Nalpha(i_alpha).

        If t is an integer, t is the rank (number of principal components).
        If t&lt;1, the rank (number of principal components) is determined such
        that the relative error after truncation is t.

        Parameters
        ----------
        fun : fun or tensap.Function
            Function of d variables i_1, ..., i_d which returns the entries of
            the tensor.
        shape : list or numpy.ndarray
            The shape of the tensor.
        alpha : int
            An array containing a tuple in {0,...,d-1}.
        tol : int or float
            The number of principal components or a positive number smaller
            than 1 (tolerance).
        B_alpha : numpy.ndarray
            Array of shape (N_\alpha, N_\alpha) whose i-th column is the
            evaluation of phi_i at the set of indices i_alpha in Ialpha.
        I_alpha : numpy.ndarray
            Array of shape (N_alpha, #alpha) containing N_alpha tuples i_alpha.

        Returns
        -------
        pc : numpy.ndarray
            The principal components of the tensor.
        output : dict
            A dictionnary of outputs, containing the singular values
            corresponding to the principal components, as well as the set of
            indices at which the tensor has been evaluated.

        &#39;&#39;&#39;
        alpha = np.atleast_1d(alpha)
        B_alpha = np.atleast_2d(B_alpha)
        I_alpha = np.array(I_alpha)

        X = tensap.random_multi_indices(shape)
        d = len(shape)
        not_alpha = np.setdiff1d(range(d), alpha)

        if tol &lt; 1:
            N = self.pca_sampling_factor * B_alpha.shape[1]
        else:
            N = self.pca_sampling_factor * tol
        N = int(np.ceil(N))

        X_not_alpha = tensap.RandomVector(X.random_variables[not_alpha])
        I_not_alpha = X_not_alpha.random(N)

        alpha_not_alpha = np.concatenate((alpha, not_alpha))
        ind = [np.nonzero(alpha_not_alpha == x)[0][0] for x in range(d)]

        output = {}
        if tol &lt; 1 and self.pca_adaptive_sampling:
            A = tensap.FullTensor(np.zeros((I_alpha.shape[0], 0)), 2,
                                  [I_alpha.shape[0], 0])
            for k in range(N):
                product_grid = tensap.FullTensorGrid(
                    [I_alpha, I_not_alpha[k, :]]).array()
                A_k = np.linalg.solve(B_alpha, fun(product_grid[:, ind]))
                A.data = np.column_stack((A.data, A_k))
                pc, sin_val = A.principal_components(tol)
                if sin_val[-1, -1] &lt; 1e-15 or pc.shape[1] &lt; \
                        np.ceil((k+1) / self.pca_sampling_factor):
                    break
            output[&#39;number_of_evaluations&#39;] = I_alpha.shape[0] * (k+1)
        else:
            product_grid = tensap.FullTensorGrid([I_alpha,
                                                  I_not_alpha]).array()
            A = fun(product_grid[:, ind])
            A = np.reshape(A, [B_alpha.shape[0], N], &#39;F&#39;)
            A = np.linalg.solve(B_alpha, A)
            A = tensap.FullTensor(A, 2, [B_alpha.shape[0], N])
            pc, sin_val = A.principal_components(tol)
            output[&#39;number_of_evaluations&#39;] = I_alpha.shape[0] * N

        output[&#39;singular_values&#39;] = sin_val
        output[&#39;samples&#39;] = product_grid
        return pc, output

    def hopca(self, fun, shape):
        &#39;&#39;&#39;
        Return the set of alpha-principal components of an algebraic tensor,
        for all alpha in {0,1,...,d-1}.

        For prescribed precision, set TPCA.max_rank = np.inf and TPCA.tol to
        the desired precision (possibly an array of length d).

        For prescribed rank, set TPCA.tol = np.inf and TPCA.max_rank to the
        desired rank (possibly an array of length d).

        See also the documentation of the class
        TensorPrincipalComponentAnalysis.

        Parameters
        ----------
        fun : fun or tensap.Function
            Function of d variables i_1, ..., i_d which returns the entries of
            the tensor.
        shape : list or numpy.ndarray
            The shape of the tensor.

        Raises
        ------
        ValueError
            If the provided tolerance and max ranks are not correct.

        Returns
        -------
        f_pc : list
            List of the alpha-principal components of the tensor.
        output : list
            List containing the outputs of the method
            alpha_principal_components.

        &#39;&#39;&#39;
        solver = deepcopy(self)
        d = len(shape)

        if np.ndim(self.tol) == 0 or len(self.tol) == 1:
            solver.tol = np.full(d, self.tol)
        elif len(self.tol) != d:
            raise ValueError(&#39;tol should be a scalar or an array of length d.&#39;)

        if np.ndim(self.max_rank) == 0 or len(self.max_rank) == 1:
            solver.max_rank = np.full(d, self.max_rank)
        elif len(self.max_rank) != d:
            raise ValueError(&#39;max_rank should be a scalar or an array of &#39; +
                             &#39;length d.&#39;)

        f_pc = []
        output = []
        for alpha in range(d):
            I_alpha = np.reshape(np.arange(shape[alpha]), (-1, 1))
            B_alpha = np.eye(shape[alpha])
            tol_alpha = np.min((solver.tol[alpha], solver.max_rank[alpha]))
            f_pc_alpha, output_alpha = \
                solver.alpha_principal_components(fun, shape, alpha, tol_alpha,
                                                  B_alpha, I_alpha)
            f_pc.append(f_pc_alpha)
            output.append(output_alpha)
        return f_pc, output

    def tucker_approximation(self, fun, shape):
        &#39;&#39;&#39;
        Approximation of a tensor of order d in Tucker format based on a
        Principal Component Analysis.

        For a prescribed precision, set TPCA.max_rank = np.inf and TPCA.tol to
        the desired precision (possibly an array of length d).

        For a prescribed rank, set TPCA.tol = np.inf and TPCA.max_rank to the
        desired rank (possibly an array of length d).

        See also the documentation of the class
        TensorPrincipalComponentAnalysis.

        Parameters
        ----------
        fun : fun or tensap.Function
            Function of d variables i_1, ..., i_d which returns the entries of
            the tensor.
        shape : list or numpy.ndarray
            The shape of the tensor.

        Raises
        ------
        ValueError
            If the provided tolerance and max ranks are not correct.

        Returns
        -------
        tensap.TreeBasedTensor
            A tensor in tree based format with a trivial tree.
        dict
            Dictionnary containing the outputs of the method.

        &#39;&#39;&#39;
        solver = deepcopy(self)
        d = len(shape)
        tree = tensap.DimensionTree.trivial(d)

        if np.ndim(self.tol) == 1 and len(self.tol) == d:
            tol = solver.tol
            solver.tol = np.zeros(d+1)
            solver.tol[tree.dim2ind-1] = tol
        elif np.ndim(self.tol) == 1 and len(self.tol) &gt; 1:
            raise ValueError(&#39;tol should be a scalar or an array of length d.&#39;)

        if np.ndim(self.max_rank) == 1 and len(self.max_rank) == d:
            rank = solver.max_rank
            solver.max_rank = np.zeros(d+1)
            solver.max_rank[tree.dim2ind-1] = rank
        elif np.ndim(self.max_rank) == 1 and len(self.max_rank) &gt; 1:
            raise ValueError(&#39;max_rank should be a scalar or an array of &#39; +
                             &#39;length d.&#39;)

        return solver.tree_based_approximation(fun, shape, tree)

    def tt_approximation(self, fun, shape):
        &#39;&#39;&#39;
        Approximation of a tensor of order d in tensor train format based on a
        Principal Component Analysis.

        For a prescribed precision, set TPCA.max_rank = np.inf and TPCA.tol to
        the desired precision (possibly an array of length d-1).

        For a prescribed rank, set TPCA.tol = np.inf and TPCA.max_rank to the
        desired rank (possibly an array of length d-1).

        See also the documentation of the class
        TensorPrincipalComponentAnalysis.

        Parameters
        ----------
        fun : fun or tensap.Function
            Function of d variables i_1, ..., i_d which returns the entries of
            the tensor.
        shape : list or numpy.ndarray
            The shape of the tensor.

        Raises
        ------
        ValueError
            If the provided tolerance and max ranks are not correct.

        Returns
        -------
        tensap.TreeBasedTensor
            A tensor in tree based format with a linear tree.
        dict
            Dictionnary containing the outputs of the method.

        &#39;&#39;&#39;
        solver = deepcopy(self)
        d = len(shape)
        tree = tensap.DimensionTree.linear(d)
        is_active_node = np.full(tree.nb_nodes, True)
        is_active_node[tree.dim2ind[1:]-1] = False
        rep_tt = np.nonzero(is_active_node)[0]
        rep_tt = np.flip(rep_tt[1:])

        if np.ndim(self.tol) == 1 and len(self.tol) == d-1:
            tol = solver.tol
            solver.tol = np.zeros(tree.nb_nodes)
            solver.tol[rep_tt] = tol
        elif np.ndim(self.tol) == 1 and len(self.tol) &gt; 1:
            raise ValueError(&#39;tol should be a scalar or an array of length &#39; +
                             &#39;d-1.&#39;)

        if np.ndim(self.max_rank) == 1 and len(self.max_rank) == d-1:
            rank = solver.max_rank
            solver.max_rank = np.zeros(tree.nb_nodes)
            solver.max_rank[rep_tt] = rank
        elif np.ndim(self.max_rank) == 1 and len(self.max_rank) &gt; 1:
            raise ValueError(&#39;max_rank should be a scalar or an array of &#39; +
                             &#39;length d-1.&#39;)

        return solver.tree_based_approximation(fun, shape, tree,
                                               is_active_node)

    def tree_based_approximation(self, fun, shape, tree, is_active_node=None):
        &#39;&#39;&#39;
        Approximation of a tensor of order d in tree based tensor format based
        on a Principal Component Analysis.

        For a prescribed precision, set TPCA.max_rank = np.inf and TPCA.tol to
        the desired precision (possibly an array of length d-1).

        For a prescribed rank, set TPCA.tol = np.inf and TPCA.max_rank to the
        desired rank (possibly an array of length d-1).

        See also the documentation of the class
        TensorPrincipalComponentAnalysis.

        Parameters
        ----------
        fun : fun or tensap.Function
            Function of d variables i_1, ..., i_d which returns the entries of
            the tensor.
        shape : list or numpy.ndarray
            The shape of the tensor.
        tree : tensap.DimensionTree
            The required dimension tree.
        is_active_node : list or numpy.ndarray, optional
            An array of booleans indicating which nodes of the tree are active.
            The default is None, settings all the nodes active.

        Raises
        ------
        ValueError
            If the provided tolerance and max ranks are not correct.

        Returns
        -------
        tensap.TreeBasedTensor
            A tensor in tree based format.
        dict
            Dictionnary containing the outputs of the method.

        &#39;&#39;&#39;
        solver = deepcopy(self)
        d = len(shape)

        if is_active_node is None:
            is_active_node = np.full(tree.nb_nodes, True)

        if (np.ndim(self.tol) == 0 or len(self.tol) == 1) and self.tol &lt; 1:
            solver.tol /= np.sqrt(np.count_nonzero(is_active_node)-1)

        if np.ndim(self.tol) == 0 or len(self.tol) == 1:
            solver.tol = np.full(tree.nb_nodes, solver.tol)
        elif len(self.tol) != tree.nb_nodes:
            raise ValueError(&#39;tol should be a scalar or an array of length &#39; +
                             &#39;tree.nb_nodes.&#39;)

        if np.ndim(self.max_rank) == 0 or len(self.max_rank) == 1:
            solver.max_rank = np.full(tree.nb_nodes, self.max_rank)
        elif len(self.max_rank) != tree.nb_nodes:
            raise ValueError(&#39;max_rank should be a scalar or an array of &#39; +
                             &#39;length tree.nb_nodes.&#39;)

        grids = [np.reshape(np.arange(x), (-1, 1)) for x in shape]
        alpha_basis = np.empty(tree.nb_nodes, dtype=object)
        alpha_grids = np.empty(tree.nb_nodes, dtype=object)
        outputs = np.empty(tree.nb_nodes, dtype=object)
        samples = np.empty(tree.nb_nodes, dtype=object)
        tensors = [[]]*tree.nb_nodes
        number_of_evaluations = 0
        for nu in range(d):
            alpha = tree.dim2ind[nu]
            B_alpha = np.eye(shape[nu])
            if is_active_node[alpha-1]:
                tol_alpha = np.min((solver.tol[alpha-1],
                                    solver.max_rank[alpha-1]))
                pc_alpha, outputs[alpha-1] = \
                    solver.alpha_principal_components(fun, shape, nu,
                                                      tol_alpha, B_alpha,
                                                      grids[nu])
                samples[alpha-1] = outputs[alpha-1][&#39;samples&#39;]
                shape_alpha = [shape[nu], pc_alpha.shape[1]]
                tensors[alpha-1] = tensap.FullTensor(pc_alpha, 2, shape_alpha)

                B_alpha = np.matmul(B_alpha, pc_alpha)
                I_alpha = tensap.magic_indices(B_alpha)[0]
                alpha_grids[alpha-1] = grids[nu][I_alpha, :]
                alpha_basis[alpha-1] = B_alpha[I_alpha, :]

                number_of_evaluations += outputs[alpha-1][
                    &#39;number_of_evaluations&#39;]
                if solver.display:
                    print(&#39;alpha = %i : rank = %i, nb_eval = %i&#39; %
                          (alpha, shape_alpha[-1],
                           outputs[alpha-1][&#39;number_of_evaluations&#39;]))
            else:
                alpha_grids[alpha-1] = grids[nu]
                alpha_basis[alpha-1] = B_alpha

        for level in np.arange(np.max(tree.level), 0, -1):
            for alpha in np.intersect1d(tree.nodes_with_level(level),
                                        tree.internal_nodes):
                S_alpha = tree.children(alpha)
                B_alpha = TensorPrincipalComponentAnalysis.\
                    _tensor_product_b_alpha(alpha_basis[S_alpha-1])
                alpha_grids[alpha-1] = \
                    tensap.FullTensorGrid(alpha_grids[S_alpha-1]).array()

                tol_alpha = np.min((solver.tol[alpha-1],
                                    solver.max_rank[alpha-1]))
                pc_alpha, outputs[alpha-1] = \
                    solver.alpha_principal_components(fun, shape,
                                                      tree.dims[alpha-1],
                                                      tol_alpha,
                                                      B_alpha,
                                                      alpha_grids[alpha-1])
                samples[alpha-1] = outputs[alpha-1][&#39;samples&#39;]
                shape_alpha = np.concatenate(([x.shape[1] for
                                              x in alpha_basis[S_alpha-1]],
                                             [pc_alpha.shape[1]]))
                tensors[alpha-1] = tensap.FullTensor(pc_alpha, len(S_alpha)+1,
                                                     shape_alpha)

                B_alpha = np.matmul(B_alpha, pc_alpha)
                I_alpha = tensap.magic_indices(B_alpha)[0]
                alpha_grids[alpha-1] = alpha_grids[alpha-1][I_alpha, :]
                alpha_basis[alpha-1] = B_alpha[I_alpha, :]
                number_of_evaluations += outputs[alpha-1][
                    &#39;number_of_evaluations&#39;]
                if solver.display:
                    print(&#39;alpha = %i : rank = %i, nb_eval = %i&#39; %
                          (alpha, shape_alpha[-1],
                           outputs[alpha-1][&#39;number_of_evaluations&#39;]))

        alpha = tree.root
        S_alpha = tree.children(alpha)
        B_alpha = TensorPrincipalComponentAnalysis.\
            _tensor_product_b_alpha(alpha_basis[S_alpha-1])
        I_alpha = tensap.FullTensorGrid(alpha_grids[S_alpha-1]).array()
        shape_alpha = [x.shape[1] for x in alpha_basis[S_alpha-1]]
        ind = [np.nonzero(tree.dims[alpha-1] == x)[0][0] for x in range(d)]
        tensors[alpha-1] = tensap.FullTensor(
            np.linalg.solve(B_alpha, fun(I_alpha[:, ind])), len(S_alpha),
            shape_alpha)
        alpha_grids[alpha-1] = I_alpha
        number_of_evaluations += I_alpha.shape[0]
        samples[alpha-1] = I_alpha
        if solver.display:
            print(&#39;Interpolation - nb_eval = %i&#39; % I_alpha.shape[0])

        f = tensap.TreeBasedTensor(tensors, tree)

        output = {&#39;number_of_evaluations&#39;: number_of_evaluations,
                  &#39;samples&#39;: samples,
                  &#39;alpha_basis&#39;: alpha_basis,
                  &#39;alpha_grids&#39;: alpha_grids,
                  &#39;outputs&#39;: outputs}

        return f, output

    @staticmethod
    def _tensor_product_b_alpha(Bs):
        &#39;&#39;&#39;
        Function used in the method tree_based_approximation.

        Parameters
        ----------
        Bs : list
            List containing s matricices B1 , ..., Bs where Bi is a
            n[i]-by-r[i] array.

        Returns
        -------
        numpy.ndarray
            An array of shape prod(n)-by-prod(r) whose entries are
            B[I , J] = B1[i_1, j_1] ... Bs[i_s, j_s]
            with I = (i_1, ..., is) and J = (j1, ..., js).

        &#39;&#39;&#39;
        Bs = [tensap.FullTensor(x, 2, x.shape) for x in Bs]
        B = Bs[0]
        for k in np.arange(1, len(Bs)):
            B = B.tensordot(Bs[k], 0)
        return B.matricize(np.arange(0, B.order-1, 2)).data</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis.alpha_principal_components"><code class="name flex">
<span>def <span class="ident">alpha_principal_components</span></span>(<span>self, fun, shape, alpha, tol, B_alpha, I_alpha)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the alpha-principal components of a tensor f.</p>
<p>For alpha in {0,&hellip;,d-1}, it evaluates the alpha-principal components
of a tensor f, meaning the principal components of the matricisations
f_alpha(i_alpha,i_notalpha), where i_alpha and i_notalpha are groups of
indices.</p>
<p>It evaluates f_alpha on the product of a set of indices in dimension
alpha (of size Nalpha) and a set of random indices (N samples) in the
complementary dimensions.
Then, it computes approximations of the alpha-principal components
in a given basis phi_1(i_alpha) &hellip; phi_Nalpha(i_alpha).</p>
<p>If t is an integer, t is the rank (number of principal components).
If t&lt;1, the rank (number of principal components) is determined such
that the relative error after truncation is t.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>fun</code></strong> :&ensp;<code>fun</code> or <code>tensap.Function</code></dt>
<dd>Function of d variables i_1, &hellip;, i_d which returns the entries of
the tensor.</dd>
<dt><strong><code>shape</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>The shape of the tensor.</dd>
<dt><strong><code>alpha</code></strong> :&ensp;<code>int</code></dt>
<dd>An array containing a tuple in {0,&hellip;,d-1}.</dd>
<dt><strong><code>tol</code></strong> :&ensp;<code>int</code> or <code>float</code></dt>
<dd>The number of principal components or a positive number smaller
than 1 (tolerance).</dd>
<dt><strong><code>B_alpha</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Array of shape (N_lpha, N_lpha) whose i-th column is the
evaluation of phi_i at the set of indices i_alpha in Ialpha.</dd>
<dt><strong><code>I_alpha</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Array of shape (N_alpha, #alpha) containing N_alpha tuples i_alpha.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>pc</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>The principal components of the tensor.</dd>
<dt><strong><code>output</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dictionnary of outputs, containing the singular values
corresponding to the principal components, as well as the set of
indices at which the tensor has been evaluated.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def alpha_principal_components(self, fun, shape, alpha, tol, B_alpha,
                               I_alpha):
    &#39;&#39;&#39;
    Evaluate the alpha-principal components of a tensor f.

    For alpha in {0,...,d-1}, it evaluates the alpha-principal components
    of a tensor f, meaning the principal components of the matricisations
    f_alpha(i_alpha,i_notalpha), where i_alpha and i_notalpha are groups of
    indices.

    It evaluates f_alpha on the product of a set of indices in dimension
    alpha (of size Nalpha) and a set of random indices (N samples) in the
    complementary dimensions.
    Then, it computes approximations of the alpha-principal components
    in a given basis phi_1(i_alpha) ... phi_Nalpha(i_alpha).

    If t is an integer, t is the rank (number of principal components).
    If t&lt;1, the rank (number of principal components) is determined such
    that the relative error after truncation is t.

    Parameters
    ----------
    fun : fun or tensap.Function
        Function of d variables i_1, ..., i_d which returns the entries of
        the tensor.
    shape : list or numpy.ndarray
        The shape of the tensor.
    alpha : int
        An array containing a tuple in {0,...,d-1}.
    tol : int or float
        The number of principal components or a positive number smaller
        than 1 (tolerance).
    B_alpha : numpy.ndarray
        Array of shape (N_\alpha, N_\alpha) whose i-th column is the
        evaluation of phi_i at the set of indices i_alpha in Ialpha.
    I_alpha : numpy.ndarray
        Array of shape (N_alpha, #alpha) containing N_alpha tuples i_alpha.

    Returns
    -------
    pc : numpy.ndarray
        The principal components of the tensor.
    output : dict
        A dictionnary of outputs, containing the singular values
        corresponding to the principal components, as well as the set of
        indices at which the tensor has been evaluated.

    &#39;&#39;&#39;
    alpha = np.atleast_1d(alpha)
    B_alpha = np.atleast_2d(B_alpha)
    I_alpha = np.array(I_alpha)

    X = tensap.random_multi_indices(shape)
    d = len(shape)
    not_alpha = np.setdiff1d(range(d), alpha)

    if tol &lt; 1:
        N = self.pca_sampling_factor * B_alpha.shape[1]
    else:
        N = self.pca_sampling_factor * tol
    N = int(np.ceil(N))

    X_not_alpha = tensap.RandomVector(X.random_variables[not_alpha])
    I_not_alpha = X_not_alpha.random(N)

    alpha_not_alpha = np.concatenate((alpha, not_alpha))
    ind = [np.nonzero(alpha_not_alpha == x)[0][0] for x in range(d)]

    output = {}
    if tol &lt; 1 and self.pca_adaptive_sampling:
        A = tensap.FullTensor(np.zeros((I_alpha.shape[0], 0)), 2,
                              [I_alpha.shape[0], 0])
        for k in range(N):
            product_grid = tensap.FullTensorGrid(
                [I_alpha, I_not_alpha[k, :]]).array()
            A_k = np.linalg.solve(B_alpha, fun(product_grid[:, ind]))
            A.data = np.column_stack((A.data, A_k))
            pc, sin_val = A.principal_components(tol)
            if sin_val[-1, -1] &lt; 1e-15 or pc.shape[1] &lt; \
                    np.ceil((k+1) / self.pca_sampling_factor):
                break
        output[&#39;number_of_evaluations&#39;] = I_alpha.shape[0] * (k+1)
    else:
        product_grid = tensap.FullTensorGrid([I_alpha,
                                              I_not_alpha]).array()
        A = fun(product_grid[:, ind])
        A = np.reshape(A, [B_alpha.shape[0], N], &#39;F&#39;)
        A = np.linalg.solve(B_alpha, A)
        A = tensap.FullTensor(A, 2, [B_alpha.shape[0], N])
        pc, sin_val = A.principal_components(tol)
        output[&#39;number_of_evaluations&#39;] = I_alpha.shape[0] * N

    output[&#39;singular_values&#39;] = sin_val
    output[&#39;samples&#39;] = product_grid
    return pc, output</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis.hopca"><code class="name flex">
<span>def <span class="ident">hopca</span></span>(<span>self, fun, shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the set of alpha-principal components of an algebraic tensor,
for all alpha in {0,1,&hellip;,d-1}.</p>
<p>For prescribed precision, set TPCA.max_rank = np.inf and TPCA.tol to
the desired precision (possibly an array of length d).</p>
<p>For prescribed rank, set TPCA.tol = np.inf and TPCA.max_rank to the
desired rank (possibly an array of length d).</p>
<p>See also the documentation of the class
TensorPrincipalComponentAnalysis.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>fun</code></strong> :&ensp;<code>fun</code> or <code>tensap.Function</code></dt>
<dd>Function of d variables i_1, &hellip;, i_d which returns the entries of
the tensor.</dd>
<dt><strong><code>shape</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>The shape of the tensor.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the provided tolerance and max ranks are not correct.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>f_pc</code></strong> :&ensp;<code>list</code></dt>
<dd>List of the alpha-principal components of the tensor.</dd>
<dt><strong><code>output</code></strong> :&ensp;<code>list</code></dt>
<dd>List containing the outputs of the method
alpha_principal_components.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hopca(self, fun, shape):
    &#39;&#39;&#39;
    Return the set of alpha-principal components of an algebraic tensor,
    for all alpha in {0,1,...,d-1}.

    For prescribed precision, set TPCA.max_rank = np.inf and TPCA.tol to
    the desired precision (possibly an array of length d).

    For prescribed rank, set TPCA.tol = np.inf and TPCA.max_rank to the
    desired rank (possibly an array of length d).

    See also the documentation of the class
    TensorPrincipalComponentAnalysis.

    Parameters
    ----------
    fun : fun or tensap.Function
        Function of d variables i_1, ..., i_d which returns the entries of
        the tensor.
    shape : list or numpy.ndarray
        The shape of the tensor.

    Raises
    ------
    ValueError
        If the provided tolerance and max ranks are not correct.

    Returns
    -------
    f_pc : list
        List of the alpha-principal components of the tensor.
    output : list
        List containing the outputs of the method
        alpha_principal_components.

    &#39;&#39;&#39;
    solver = deepcopy(self)
    d = len(shape)

    if np.ndim(self.tol) == 0 or len(self.tol) == 1:
        solver.tol = np.full(d, self.tol)
    elif len(self.tol) != d:
        raise ValueError(&#39;tol should be a scalar or an array of length d.&#39;)

    if np.ndim(self.max_rank) == 0 or len(self.max_rank) == 1:
        solver.max_rank = np.full(d, self.max_rank)
    elif len(self.max_rank) != d:
        raise ValueError(&#39;max_rank should be a scalar or an array of &#39; +
                         &#39;length d.&#39;)

    f_pc = []
    output = []
    for alpha in range(d):
        I_alpha = np.reshape(np.arange(shape[alpha]), (-1, 1))
        B_alpha = np.eye(shape[alpha])
        tol_alpha = np.min((solver.tol[alpha], solver.max_rank[alpha]))
        f_pc_alpha, output_alpha = \
            solver.alpha_principal_components(fun, shape, alpha, tol_alpha,
                                              B_alpha, I_alpha)
        f_pc.append(f_pc_alpha)
        output.append(output_alpha)
    return f_pc, output</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis.tree_based_approximation"><code class="name flex">
<span>def <span class="ident">tree_based_approximation</span></span>(<span>self, fun, shape, tree, is_active_node=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Approximation of a tensor of order d in tree based tensor format based
on a Principal Component Analysis.</p>
<p>For a prescribed precision, set TPCA.max_rank = np.inf and TPCA.tol to
the desired precision (possibly an array of length d-1).</p>
<p>For a prescribed rank, set TPCA.tol = np.inf and TPCA.max_rank to the
desired rank (possibly an array of length d-1).</p>
<p>See also the documentation of the class
TensorPrincipalComponentAnalysis.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>fun</code></strong> :&ensp;<code>fun</code> or <code>tensap.Function</code></dt>
<dd>Function of d variables i_1, &hellip;, i_d which returns the entries of
the tensor.</dd>
<dt><strong><code>shape</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>The shape of the tensor.</dd>
<dt><strong><code>tree</code></strong> :&ensp;<code>tensap.DimensionTree</code></dt>
<dd>The required dimension tree.</dd>
<dt><strong><code>is_active_node</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code>, optional</dt>
<dd>An array of booleans indicating which nodes of the tree are active.
The default is None, settings all the nodes active.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the provided tolerance and max ranks are not correct.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tensap.TreeBasedTensor</code></dt>
<dd>A tensor in tree based format.</dd>
<dt><code>dict</code></dt>
<dd>Dictionnary containing the outputs of the method.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tree_based_approximation(self, fun, shape, tree, is_active_node=None):
    &#39;&#39;&#39;
    Approximation of a tensor of order d in tree based tensor format based
    on a Principal Component Analysis.

    For a prescribed precision, set TPCA.max_rank = np.inf and TPCA.tol to
    the desired precision (possibly an array of length d-1).

    For a prescribed rank, set TPCA.tol = np.inf and TPCA.max_rank to the
    desired rank (possibly an array of length d-1).

    See also the documentation of the class
    TensorPrincipalComponentAnalysis.

    Parameters
    ----------
    fun : fun or tensap.Function
        Function of d variables i_1, ..., i_d which returns the entries of
        the tensor.
    shape : list or numpy.ndarray
        The shape of the tensor.
    tree : tensap.DimensionTree
        The required dimension tree.
    is_active_node : list or numpy.ndarray, optional
        An array of booleans indicating which nodes of the tree are active.
        The default is None, settings all the nodes active.

    Raises
    ------
    ValueError
        If the provided tolerance and max ranks are not correct.

    Returns
    -------
    tensap.TreeBasedTensor
        A tensor in tree based format.
    dict
        Dictionnary containing the outputs of the method.

    &#39;&#39;&#39;
    solver = deepcopy(self)
    d = len(shape)

    if is_active_node is None:
        is_active_node = np.full(tree.nb_nodes, True)

    if (np.ndim(self.tol) == 0 or len(self.tol) == 1) and self.tol &lt; 1:
        solver.tol /= np.sqrt(np.count_nonzero(is_active_node)-1)

    if np.ndim(self.tol) == 0 or len(self.tol) == 1:
        solver.tol = np.full(tree.nb_nodes, solver.tol)
    elif len(self.tol) != tree.nb_nodes:
        raise ValueError(&#39;tol should be a scalar or an array of length &#39; +
                         &#39;tree.nb_nodes.&#39;)

    if np.ndim(self.max_rank) == 0 or len(self.max_rank) == 1:
        solver.max_rank = np.full(tree.nb_nodes, self.max_rank)
    elif len(self.max_rank) != tree.nb_nodes:
        raise ValueError(&#39;max_rank should be a scalar or an array of &#39; +
                         &#39;length tree.nb_nodes.&#39;)

    grids = [np.reshape(np.arange(x), (-1, 1)) for x in shape]
    alpha_basis = np.empty(tree.nb_nodes, dtype=object)
    alpha_grids = np.empty(tree.nb_nodes, dtype=object)
    outputs = np.empty(tree.nb_nodes, dtype=object)
    samples = np.empty(tree.nb_nodes, dtype=object)
    tensors = [[]]*tree.nb_nodes
    number_of_evaluations = 0
    for nu in range(d):
        alpha = tree.dim2ind[nu]
        B_alpha = np.eye(shape[nu])
        if is_active_node[alpha-1]:
            tol_alpha = np.min((solver.tol[alpha-1],
                                solver.max_rank[alpha-1]))
            pc_alpha, outputs[alpha-1] = \
                solver.alpha_principal_components(fun, shape, nu,
                                                  tol_alpha, B_alpha,
                                                  grids[nu])
            samples[alpha-1] = outputs[alpha-1][&#39;samples&#39;]
            shape_alpha = [shape[nu], pc_alpha.shape[1]]
            tensors[alpha-1] = tensap.FullTensor(pc_alpha, 2, shape_alpha)

            B_alpha = np.matmul(B_alpha, pc_alpha)
            I_alpha = tensap.magic_indices(B_alpha)[0]
            alpha_grids[alpha-1] = grids[nu][I_alpha, :]
            alpha_basis[alpha-1] = B_alpha[I_alpha, :]

            number_of_evaluations += outputs[alpha-1][
                &#39;number_of_evaluations&#39;]
            if solver.display:
                print(&#39;alpha = %i : rank = %i, nb_eval = %i&#39; %
                      (alpha, shape_alpha[-1],
                       outputs[alpha-1][&#39;number_of_evaluations&#39;]))
        else:
            alpha_grids[alpha-1] = grids[nu]
            alpha_basis[alpha-1] = B_alpha

    for level in np.arange(np.max(tree.level), 0, -1):
        for alpha in np.intersect1d(tree.nodes_with_level(level),
                                    tree.internal_nodes):
            S_alpha = tree.children(alpha)
            B_alpha = TensorPrincipalComponentAnalysis.\
                _tensor_product_b_alpha(alpha_basis[S_alpha-1])
            alpha_grids[alpha-1] = \
                tensap.FullTensorGrid(alpha_grids[S_alpha-1]).array()

            tol_alpha = np.min((solver.tol[alpha-1],
                                solver.max_rank[alpha-1]))
            pc_alpha, outputs[alpha-1] = \
                solver.alpha_principal_components(fun, shape,
                                                  tree.dims[alpha-1],
                                                  tol_alpha,
                                                  B_alpha,
                                                  alpha_grids[alpha-1])
            samples[alpha-1] = outputs[alpha-1][&#39;samples&#39;]
            shape_alpha = np.concatenate(([x.shape[1] for
                                          x in alpha_basis[S_alpha-1]],
                                         [pc_alpha.shape[1]]))
            tensors[alpha-1] = tensap.FullTensor(pc_alpha, len(S_alpha)+1,
                                                 shape_alpha)

            B_alpha = np.matmul(B_alpha, pc_alpha)
            I_alpha = tensap.magic_indices(B_alpha)[0]
            alpha_grids[alpha-1] = alpha_grids[alpha-1][I_alpha, :]
            alpha_basis[alpha-1] = B_alpha[I_alpha, :]
            number_of_evaluations += outputs[alpha-1][
                &#39;number_of_evaluations&#39;]
            if solver.display:
                print(&#39;alpha = %i : rank = %i, nb_eval = %i&#39; %
                      (alpha, shape_alpha[-1],
                       outputs[alpha-1][&#39;number_of_evaluations&#39;]))

    alpha = tree.root
    S_alpha = tree.children(alpha)
    B_alpha = TensorPrincipalComponentAnalysis.\
        _tensor_product_b_alpha(alpha_basis[S_alpha-1])
    I_alpha = tensap.FullTensorGrid(alpha_grids[S_alpha-1]).array()
    shape_alpha = [x.shape[1] for x in alpha_basis[S_alpha-1]]
    ind = [np.nonzero(tree.dims[alpha-1] == x)[0][0] for x in range(d)]
    tensors[alpha-1] = tensap.FullTensor(
        np.linalg.solve(B_alpha, fun(I_alpha[:, ind])), len(S_alpha),
        shape_alpha)
    alpha_grids[alpha-1] = I_alpha
    number_of_evaluations += I_alpha.shape[0]
    samples[alpha-1] = I_alpha
    if solver.display:
        print(&#39;Interpolation - nb_eval = %i&#39; % I_alpha.shape[0])

    f = tensap.TreeBasedTensor(tensors, tree)

    output = {&#39;number_of_evaluations&#39;: number_of_evaluations,
              &#39;samples&#39;: samples,
              &#39;alpha_basis&#39;: alpha_basis,
              &#39;alpha_grids&#39;: alpha_grids,
              &#39;outputs&#39;: outputs}

    return f, output</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis.tt_approximation"><code class="name flex">
<span>def <span class="ident">tt_approximation</span></span>(<span>self, fun, shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Approximation of a tensor of order d in tensor train format based on a
Principal Component Analysis.</p>
<p>For a prescribed precision, set TPCA.max_rank = np.inf and TPCA.tol to
the desired precision (possibly an array of length d-1).</p>
<p>For a prescribed rank, set TPCA.tol = np.inf and TPCA.max_rank to the
desired rank (possibly an array of length d-1).</p>
<p>See also the documentation of the class
TensorPrincipalComponentAnalysis.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>fun</code></strong> :&ensp;<code>fun</code> or <code>tensap.Function</code></dt>
<dd>Function of d variables i_1, &hellip;, i_d which returns the entries of
the tensor.</dd>
<dt><strong><code>shape</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>The shape of the tensor.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the provided tolerance and max ranks are not correct.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tensap.TreeBasedTensor</code></dt>
<dd>A tensor in tree based format with a linear tree.</dd>
<dt><code>dict</code></dt>
<dd>Dictionnary containing the outputs of the method.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tt_approximation(self, fun, shape):
    &#39;&#39;&#39;
    Approximation of a tensor of order d in tensor train format based on a
    Principal Component Analysis.

    For a prescribed precision, set TPCA.max_rank = np.inf and TPCA.tol to
    the desired precision (possibly an array of length d-1).

    For a prescribed rank, set TPCA.tol = np.inf and TPCA.max_rank to the
    desired rank (possibly an array of length d-1).

    See also the documentation of the class
    TensorPrincipalComponentAnalysis.

    Parameters
    ----------
    fun : fun or tensap.Function
        Function of d variables i_1, ..., i_d which returns the entries of
        the tensor.
    shape : list or numpy.ndarray
        The shape of the tensor.

    Raises
    ------
    ValueError
        If the provided tolerance and max ranks are not correct.

    Returns
    -------
    tensap.TreeBasedTensor
        A tensor in tree based format with a linear tree.
    dict
        Dictionnary containing the outputs of the method.

    &#39;&#39;&#39;
    solver = deepcopy(self)
    d = len(shape)
    tree = tensap.DimensionTree.linear(d)
    is_active_node = np.full(tree.nb_nodes, True)
    is_active_node[tree.dim2ind[1:]-1] = False
    rep_tt = np.nonzero(is_active_node)[0]
    rep_tt = np.flip(rep_tt[1:])

    if np.ndim(self.tol) == 1 and len(self.tol) == d-1:
        tol = solver.tol
        solver.tol = np.zeros(tree.nb_nodes)
        solver.tol[rep_tt] = tol
    elif np.ndim(self.tol) == 1 and len(self.tol) &gt; 1:
        raise ValueError(&#39;tol should be a scalar or an array of length &#39; +
                         &#39;d-1.&#39;)

    if np.ndim(self.max_rank) == 1 and len(self.max_rank) == d-1:
        rank = solver.max_rank
        solver.max_rank = np.zeros(tree.nb_nodes)
        solver.max_rank[rep_tt] = rank
    elif np.ndim(self.max_rank) == 1 and len(self.max_rank) &gt; 1:
        raise ValueError(&#39;max_rank should be a scalar or an array of &#39; +
                         &#39;length d-1.&#39;)

    return solver.tree_based_approximation(fun, shape, tree,
                                           is_active_node)</code></pre>
</details>
</dd>
<dt id="tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis.tucker_approximation"><code class="name flex">
<span>def <span class="ident">tucker_approximation</span></span>(<span>self, fun, shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Approximation of a tensor of order d in Tucker format based on a
Principal Component Analysis.</p>
<p>For a prescribed precision, set TPCA.max_rank = np.inf and TPCA.tol to
the desired precision (possibly an array of length d).</p>
<p>For a prescribed rank, set TPCA.tol = np.inf and TPCA.max_rank to the
desired rank (possibly an array of length d).</p>
<p>See also the documentation of the class
TensorPrincipalComponentAnalysis.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>fun</code></strong> :&ensp;<code>fun</code> or <code>tensap.Function</code></dt>
<dd>Function of d variables i_1, &hellip;, i_d which returns the entries of
the tensor.</dd>
<dt><strong><code>shape</code></strong> :&ensp;<code>list</code> or <code>numpy.ndarray</code></dt>
<dd>The shape of the tensor.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the provided tolerance and max ranks are not correct.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tensap.TreeBasedTensor</code></dt>
<dd>A tensor in tree based format with a trivial tree.</dd>
<dt><code>dict</code></dt>
<dd>Dictionnary containing the outputs of the method.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tucker_approximation(self, fun, shape):
    &#39;&#39;&#39;
    Approximation of a tensor of order d in Tucker format based on a
    Principal Component Analysis.

    For a prescribed precision, set TPCA.max_rank = np.inf and TPCA.tol to
    the desired precision (possibly an array of length d).

    For a prescribed rank, set TPCA.tol = np.inf and TPCA.max_rank to the
    desired rank (possibly an array of length d).

    See also the documentation of the class
    TensorPrincipalComponentAnalysis.

    Parameters
    ----------
    fun : fun or tensap.Function
        Function of d variables i_1, ..., i_d which returns the entries of
        the tensor.
    shape : list or numpy.ndarray
        The shape of the tensor.

    Raises
    ------
    ValueError
        If the provided tolerance and max ranks are not correct.

    Returns
    -------
    tensap.TreeBasedTensor
        A tensor in tree based format with a trivial tree.
    dict
        Dictionnary containing the outputs of the method.

    &#39;&#39;&#39;
    solver = deepcopy(self)
    d = len(shape)
    tree = tensap.DimensionTree.trivial(d)

    if np.ndim(self.tol) == 1 and len(self.tol) == d:
        tol = solver.tol
        solver.tol = np.zeros(d+1)
        solver.tol[tree.dim2ind-1] = tol
    elif np.ndim(self.tol) == 1 and len(self.tol) &gt; 1:
        raise ValueError(&#39;tol should be a scalar or an array of length d.&#39;)

    if np.ndim(self.max_rank) == 1 and len(self.max_rank) == d:
        rank = solver.max_rank
        solver.max_rank = np.zeros(d+1)
        solver.max_rank[tree.dim2ind-1] = rank
    elif np.ndim(self.max_rank) == 1 and len(self.max_rank) &gt; 1:
        raise ValueError(&#39;max_rank should be a scalar or an array of &#39; +
                         &#39;length d.&#39;)

    return solver.tree_based_approximation(fun, shape, tree)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="tensap.approximation.tensor_approximation.principal_component_analysis" href="index.html">tensap.approximation.tensor_approximation.principal_component_analysis</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis" href="#tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis">TensorPrincipalComponentAnalysis</a></code></h4>
<ul class="">
<li><code><a title="tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis.alpha_principal_components" href="#tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis.alpha_principal_components">alpha_principal_components</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis.hopca" href="#tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis.hopca">hopca</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis.tree_based_approximation" href="#tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis.tree_based_approximation">tree_based_approximation</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis.tt_approximation" href="#tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis.tt_approximation">tt_approximation</a></code></li>
<li><code><a title="tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis.tucker_approximation" href="#tensap.approximation.tensor_approximation.principal_component_analysis.tensor_principal_component_analysis.TensorPrincipalComponentAnalysis.tucker_approximation">tucker_approximation</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>